{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace as bk\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import torch\n",
    "import nlp\n",
    "from tqdm import tqdm\n",
    "from transformers import ElectraTokenizerFast\n",
    "hf_fast_tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-small-generator\")\n",
    "from fastai2.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cola = nlp.load_dataset('glue', 'cola', cache_dir='~/tmp')\n",
    "\n",
    "def HF_TokenizeTfm(cols):\n",
    "  if isinstance(cols, list): cols = {c:c for c in cols}\n",
    "  assert isinstance(cols, dict)\n",
    "  def _tokenize(example):\n",
    "    for in_col, out_col in cols.items():\n",
    "      example[out_col] = hf_fast_tokenizer.convert_tokens_to_ids(hf_fast_tokenizer.tokenize(example[in_col]))\n",
    "    return example\n",
    "  return _tokenize\n",
    "\n",
    "tokenized_cola = {}\n",
    "for split, dset in cola.items():\n",
    "  tokenized_cola[split] = dset.map(HF_TokenizeTfm({'sentence':'text_idxs'}),\n",
    "                          remove_columns=['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Integration with fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delegates()\n",
    "class HF_Dataloader(TfmdDL):\n",
    "  \n",
    "  def __init__(self, dataset, pad_idx, sort=True, **kwargs):\n",
    "    if pad_idx is not None:\n",
    "      kwargs['before_batch'] = partial(pad_input_chunk, pad_idx=pad_idx, pad_first=False)\n",
    "    if sort:\n",
    "      self.lens = [ len(sample[0]) for sample in dataset ]\n",
    "    store_attr(self, 'pad_idx,sort')\n",
    "    super().__init__(dataset, **kwargs)\n",
    "  \n",
    "  def get_idxs(self):\n",
    "    idxs = super().get_idxs()\n",
    "    if not self.sort : return idxs\n",
    "    return sorted(idxs, key=lambda i: self.lens[i], reverse=True)\n",
    "\n",
    "  def new(self, dataset, **kwargs):\n",
    "    return super().new(dataset=self.dataset, pad_idx=self.pad_idx, sort=self.sort, **kwargs)\n",
    "\n",
    "class HF_Dataset(FilteredBase):\n",
    "  \n",
    "  def __init__(self, hf_dset, cols, hf_tokenizer=None, pretty_show=False, n_inp=1):\n",
    "    \n",
    "    # some default setting for tensor type used in decoding\n",
    "    if isinstance(cols, list): \n",
    "      if n_inp==1: \n",
    "        if len(cols)==1: cols = {cols[0]: TensorText}\n",
    "        elif len(cols)==2: cols = {cols[0]: TensorText, cols[1]: TensorCategory}\n",
    "      else: cols = { c: noop for c in cols }\n",
    "    assert isinstance(cols, dict)\n",
    "    \n",
    "    # make dataset output pytorch tensor\n",
    "    if hf_dset.format['type'] != 'torch': \n",
    "      hf_dset.set_format( type='torch', columns=list(cols.keys()) )\n",
    "\n",
    "    # store attributes\n",
    "    store_attr(self, \"hf_dset,cols,n_inp,hf_tokenizer,pretty_show\")\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sample = self.hf_dset[idx]\n",
    "    return tuple( tensor_cls(sample[col]) for col, tensor_cls in self.cols.items() )\n",
    "\n",
    "  def __len__(self): return len(self.hf_dset)\n",
    "\n",
    "  @property\n",
    "  def column_names(self): return list(self.cols.keys())\n",
    "\n",
    "  def decode(self, o, full=True): \n",
    "    return tuple( self._decode(o_) for o_ in o )\n",
    "\n",
    "  @typedispatch\n",
    "  def _decode(self, t:TensorText): \n",
    "    assert self.hf_tokenizer, \"You should give huggingface tokenizer if you want to show batch.\"\n",
    "    if self.pretty_show: text = self.hf_tokenizer.decode([idx for idx in t if idx != self.hf_tokenizer.pad_token_id])\n",
    "    else: text = ' '.join(self.hf_tokenizer.convert_ids_to_tokens(t))\n",
    "    return TitledStr(text)\n",
    "\n",
    "  @typedispatch\n",
    "  def _decode(self, t:LMTensorText): return self._decode[TensorText](self, t)\n",
    "\n",
    "  @typedispatch\n",
    "  def _decode(self, t:TensorCategory): return Category(t.item())\n",
    "  \n",
    "class HF_Datasets(FilteredBase):\n",
    "  _dl_type,_dbunch_type = HF_Dataloader,DataLoaders\n",
    "  def __init__(self, hs_dsets: dict, *args, **kwargs):\n",
    "    self.hs_dsets = { split: HF_Dataset(dset, *args, **kwargs) for split, dset in hs_dsets.items()}\n",
    "  def subset(self, i): return list(self.hs_dsets.values())[i]\n",
    "  def __getitem__(self, split): return self.hs_dsets[split]\n",
    "  @property\n",
    "  def n_subsets(self): return len(self.hs_dsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everybody who has ever, worked in any office which contained any typewriter which had ever been used to type any letters which had to be signed by any administrator who ever worked in any department like mine will know what i mean.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hank plays the guitar and finds arrangements for all the old folk songs which are still sung in these hills, and ernie writes down all the old folk songs which are still sung in these hills.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>playing with matches is ; lots of fun, but doing, so and emptying gasoline from one can to another at the same time is a sport best reserved for arsons.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>in january 2002, a dull star in an obscure constellation suddenly became 600, 000 times more luminous than our sun, temporarily making it the brightest star in our galaxy.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>which folks up at corporate headquarters do you think that the sooner you solve this problem, the quicker you'll be able to tell t to buzz off?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>the dumplings which sasha is gobbling down faster than i can reheat the meatballs are extremely tasty, if i do say so.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>will put a picture of bill on your desk before tomorrow, this girl in the red coat will put a picture of bill on your desk before tomorrow.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>a burlap sack of potatoes with mealy skins fell on the professor of linguistics with the terrible taste in t - shirts from the twelfth story.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>one of the jewish children is a spunky girl, who gave a black eye to the kid with the german roots before the start of the war.</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "cola_dsets = HF_Datasets(tokenized_cola, ['text_idxs', 'label'], hf_fast_tokenizer, pretty_show=True)\n",
    "cola_dls = cola_dsets.dataloaders(bs=32, pad_idx=hf_fast_tokenizer.pad_token_id)\n",
    "cola_dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Aggregate samples of HuggingFace/nlp dataset\n",
    "~ task traditional language model task for example ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregateTransform():\n",
    "  def __init__(self, hf_dset, inp_cols, out_cols):\n",
    "    for inp_col in inp_cols: assert inp_col in hf_dset.column_names, f\"{inp_col} is not in {hf_dset.column_names}\"\n",
    "    for out_col in out_cols: assert out_col not in hf_dset.column_names, f\"Because we will remove existing columns after transform, output columns can't share the same names with existing columns\"\n",
    "    # Hugginface nlp dataset\n",
    "    # batched map need dataset be in python format\n",
    "    hf_dset.set_format(type=None, columns=hf_dset.column_names)\n",
    "    self.hf_dset = hf_dset\n",
    "    self.original_cols = hf_dset.column_names\n",
    "\n",
    "  def map(self, batch_size=1000, **kwargs):\n",
    "    return self.hf_dset.map(function=self, batched=True, batch_size=batch_size, with_indices=True,\n",
    "                            remove_columns=self.original_cols, **kwargs)\n",
    "\n",
    "class LMTransform(AggregateTransform):\n",
    "  def __init__(self, hf_dset, max_len, text_col, x_text_col='x_text', y_text_col='y_text'):\n",
    "    super().__init__(hf_dset, [text_col], [x_text_col, y_text_col])\n",
    "    self.text_col, self.x_text_col, self.y_text_col = text_col, x_text_col, y_text_col\n",
    "\n",
    "    # Global scop variables for the loop\n",
    "    self._max_len = max_len + 1\n",
    "    self.last_idx = len(hf_dset) - 1\n",
    "    self.residual_len, self.new_text = self._max_len, []\n",
    "\n",
    "  def __call__(self, b, idxs):\n",
    "    self.x_texts, self.y_texts =  [], []\n",
    "    for text in tqdm(b[self.text_col], leave=False):\n",
    "      self._accumulate(text)\n",
    "    # last\n",
    "    if self.last_idx in idxs and len(self.new_text) >= 2:\n",
    "      self.x_texts.append(self.new_text[:-1])\n",
    "      self.y_texts.append(self.new_text[1:]) \n",
    "    new_b = {self.x_text_col: self.x_texts, self.y_text_col: self.y_texts}\n",
    "    # map require the returned includes original columns\n",
    "    for key in b: new_b[key] = [None]*len(self.x_texts)\n",
    "    \n",
    "    return new_b\n",
    "\n",
    "  def _accumulate(self, text):\n",
    "    \"text: a list of indices\"\n",
    "    usable_len = len(text)\n",
    "    cursor = 0\n",
    "    while usable_len != 0:\n",
    "      use_len = min(usable_len, self.residual_len)\n",
    "      self.new_text += text[cursor:cursor+use_len]\n",
    "      self.residual_len -= use_len\n",
    "      usable_len -= use_len\n",
    "      cursor += use_len\n",
    "      if self.residual_len == 0:\n",
    "        self.x_texts.append(self.new_text[:-1])\n",
    "        self.y_texts.append(self.new_text[1:])\n",
    "        self.new_text = []\n",
    "        self.residual_len = self._max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0%|          | 0/2 [00:00<?, ?it/s]Original dataset:\nnum of samples: 1043\nsecond to last sentence: John arranged for himself to get the prize.\n          last sentence: John talked to Bill about himself.\nLM dataset:\nnum of sampels: 481\nlast text (x): . john talked to bill about himself\nlast text (y): john talked to bill about himself.\n"
    }
   ],
   "source": [
    "cola_val = tokenized_cola['validation']\n",
    "lm_dataset = LMTransform(cola_val, max_len=20, text_col='text_idxs').map()\n",
    "\n",
    "print('Original dataset:')\n",
    "print('num of samples:', len(cola['validation']))\n",
    "print('second to last sentence:', cola['validation'][-2]['sentence'])\n",
    "print('          last sentence:', cola['validation'][-1]['sentence'])\n",
    "print('LM dataset:')\n",
    "print('num of sampels:', len(lm_dataset))\n",
    "print('last text (x):', hf_fast_tokenizer.decode(lm_dataset[-1]['x_text']))\n",
    "print('last text (y):', hf_fast_tokenizer.decode(lm_dataset[-1]['y_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey</td>\n      <td>sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey .</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less .</td>\n      <td>mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less . as</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>you eat the most , you want the least . the more you would want , the less you would</td>\n      <td>eat the most , you want the least . the more you would want , the less you would eat</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>. i demand that the more john eat , the more he pays . mary listen ##s to the grateful</td>\n      <td>i demand that the more john eat , the more he pays . mary listen ##s to the grateful dead</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>, she gets depressed . the ang ##rier mary got , the more she looked at pictures . the higher</td>\n      <td>she gets depressed . the ang ##rier mary got , the more she looked at pictures . the higher the</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>stakes , the lower his expectations are . the more fred is ob ##no ##xious , the less attention you</td>\n      <td>, the lower his expectations are . the more fred is ob ##no ##xious , the less attention you should</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>pay to him . john was lots more ob ##no ##xious than fred . the more people you give beer</td>\n      <td>to him . john was lots more ob ##no ##xious than fred . the more people you give beer to</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>, the more people get sick . the more does bill smoke , the more susan hates him . the</td>\n      <td>the more people get sick . the more does bill smoke , the more susan hates him . the more</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>pictures of him that appear in the news , the more embarrassed john becomes . every senator seems to become</td>\n      <td>of him that appear in the news , the more embarrassed john becomes . every senator seems to become more</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "lm_dl = HF_Dataloader(HF_Dataset(lm_dataset, {'x_text':LMTensorText, 'y_text':TensorText},hf_fast_tokenizer), sort=False, pad_idx=hf_fast_tokenizer.pad_token_id)\n",
    "lm_dl.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}