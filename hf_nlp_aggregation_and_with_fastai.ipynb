{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace as bk\n",
    "import os\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pyarrow as pa\n",
    "import nlp\n",
    "from transformers import ElectraTokenizerFast\n",
    "hf_fast_tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-small-generator\")\n",
    "from fastai2.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HF_TokenizeTfm():\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    `hf_dset` (`nlp.Dataset`)\n",
    "    `cols`: with one of the following signature:\n",
    "      - `cols`(`List[str]`): tokenize the col into col\n",
    "      - `cols`(`Dict[str]`): tokenize the col(key) and into col(value)\n",
    "    `hf_tokenizer`: tokenizer of HuggingFace/Transformers.\n",
    "    `remove_original`: after tokenization, remove all original columns to save cache size.\n",
    "  \"\"\"\n",
    "  def __init__(self, hf_dset, cols, hf_tokenizer, remove_original=False):\n",
    "    if isinstance(cols, list): cols = {c:c for c in cols}\n",
    "    assert isinstance(cols, dict)\n",
    "    self.hf_dset, self.cols, self.tokenizer = hf_dset, cols, hf_tokenizer\n",
    "    self.remove_original = remove_original\n",
    "    if remove_original:\n",
    "      for in_col,out_col in cols.items(): assert in_col !=  out_col\n",
    "    \"\"\"\n",
    "    If don't specify cache file name, it will be hashed binary of pickled function that\n",
    "    passed to `map`, so if you pass the same function, it knows to use cache.\n",
    "    But tokenizer can't be pickled, so use tokenizer config to make tfms use different \n",
    "    tokenizers unique.  \n",
    "    \"\"\"\n",
    "    self.tokenizer_config = hf_tokenizer.pretrained_init_configuration\n",
    "  \n",
    "  def __call__(self, example):\n",
    "    for in_col, out_col in self.cols.items():\n",
    "      example[out_col] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(example[in_col]))\n",
    "    return example\n",
    "\n",
    "  def __getstate__(self):\n",
    "    \"specify something you don't want pickle here, remember to use copy to not modfiy orginal instance\"\n",
    "    state = self.__dict__.copy() \n",
    "    state['tokenizer'] = None \n",
    "    return state\n",
    "\n",
    "  def map(self, **kwargs):\n",
    "    if self.remove_original:\n",
    "      assert 'remove_columns' not in kwargs, \"You have specified to remove all original columns.\"\n",
    "      return self.hf_dset.map(self, remove_columns=self.hf_dset.column_names, **kwargs)\n",
    "    else:\n",
    "      return self.hf_dset.map(self, **kwargs)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "8551it [00:01, 4834.76it/s]\n1043it [00:00, 9376.89it/s]\n1063it [00:00, 9970.14it/s]\n"
    }
   ],
   "source": [
    "cola = nlp.load_dataset('glue', 'cola')\n",
    "\n",
    "tokenized_cola = {}\n",
    "for split, dset in cola.items():\n",
    "  tokenized_cola[split] = HF_TokenizeTfm(dset, {'sentence':'text_idxs'}, hf_fast_tokenizer).map(remove_columns=['sentence', 'idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Integration with fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delegates()\n",
    "class HF_Dataloader(TfmdDL):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    `dataset`: any class that output a tuple, which has token ids in its first element, from `__getitem__`.\n",
    "    `pad_idx` (`int`): If sepcified, pad texts to the longest text in the batch.\n",
    "    `sort` (`Optional[bool]`, default: `True`): Sort the samples with their length, thus samples of similar legnth collated into a batch and we can pad less. Notice if it is True, the shuffle will be overrided and not shuffle.\n",
    "    `filterout` (`Optional[callable(*args) -> bool]`, , default: `None`): if not `None`, judege whether exclude this sample with this sample(`tuple`) as args\n",
    "    `cache_file` (`Optional[str]`, default: `None`): A name of json file to store the computed record of results of sort or filterout.   \n",
    "  \"\"\"\n",
    "  def __init__(self, dataset, pad_idx, sort=True, filterout=None, cache_file=None, **kwargs):\n",
    "    if pad_idx is not None:\n",
    "      kwargs['before_batch'] = partial(pad_input_chunk, pad_idx=pad_idx, pad_first=False)\n",
    "\n",
    "    cache_file = Path(cache_file) if cache_file else None\n",
    "    if cache_file and cache_file.exists():\n",
    "      with cache_file.open(mode='r') as f: self.samples = json.load(f)\n",
    "    elif sort or filterout:\n",
    "      if cache_file: cache_file.touch()\n",
    "      try:\n",
    "        if filterout is None: filterout = lambda *args: False\n",
    "        self.samples = [ (i,len(sample[0])) for i, sample in tqdm(enumerate(dataset), leave=False) if not filterout(*sample) ]\n",
    "        if sort: self.samples.sort(key=lambda t:t[1], reverse=True)\n",
    "      except Exception as e:\n",
    "        os.remove(cache_file)\n",
    "        raise e\n",
    "      if cache_file:\n",
    "        with cache_file.open(mode='w') as f: json.dump(self.samples, f)\n",
    "    else:\n",
    "      self.samples = [ (i,None) for i in range(len(dataset))]\n",
    "\n",
    "    store_attr(self, 'pad_idx,sort,filterout,cache_file')\n",
    "    super().__init__(dataset, **kwargs)\n",
    "    if sort: self.shuffle=False\n",
    "    self.n = len(self.samples)\n",
    "  \n",
    "  def create_item(self, i): return self.dataset[self.samples[i][0]]\n",
    "\n",
    "  def new(self, dataset, **kwargs):\n",
    "    return super().new(dataset=dataset, pad_idx=self.pad_idx, sort=self.sort, filterout=self.filterout, **kwargs)\n",
    "\n",
    "class HF_Dataset(FilteredBase):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    `hf_dset` (`nlp.Dataset`)\n",
    "    `cols`: with one of the following signature:\n",
    "    - `cols`(`List[str]`): \n",
    "      - if of length 1, regard the 1st element as text\n",
    "      - if of length 2, regrad the 1st element as text, 2nd as category\n",
    "    - `cols`(`Dict[Fastai2 Semantic Tesor]`): {`inp_col`:tensor type}: output sample as tuple of values of `inp_col` in order, and encode/decode with the tensor type,\n",
    "    `hf_tokenizer`: tokenizer of HuggingFace/Transformers\n",
    "    `pretty_show` (`Optional[bool]`, default:`False`): Show the original sentence instead of tokens.\n",
    "  \"\"\"\n",
    "  def __init__(self, hf_dset, cols, hf_tokenizer=None, pretty_show=False, n_inp=1):\n",
    "    \n",
    "    # some default setting for tensor type used in decoding\n",
    "    if isinstance(cols, list): \n",
    "      if n_inp==1: \n",
    "        if len(cols)==1: cols = {cols[0]: TensorText}\n",
    "        elif len(cols)==2: cols = {cols[0]: TensorText, cols[1]: TensorCategory}\n",
    "      else: cols = { c: noop for c in cols }\n",
    "    assert isinstance(cols, dict)\n",
    "    \n",
    "    # make dataset output pytorch tensor\n",
    "    if hf_dset.format['type'] != 'torch': \n",
    "      hf_dset.set_format( type='torch', columns=list(cols.keys()) )\n",
    "\n",
    "    # store attributes\n",
    "    store_attr(self, \"hf_dset,cols,n_inp,hf_tokenizer,pretty_show\")\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sample = self.hf_dset[idx]\n",
    "    return tuple( tensor_cls(sample[col]) for col, tensor_cls in self.cols.items() )\n",
    "\n",
    "  def __len__(self): return len(self.hf_dset)\n",
    "\n",
    "  @property\n",
    "  def column_names(self): return list(self.cols.keys())\n",
    "\n",
    "  def decode(self, o, full=True): \n",
    "    return tuple( self._decode(o_) for o_ in o )\n",
    "\n",
    "  @typedispatch\n",
    "  def _decode(self, t:torch.Tensor): assert False, \"You didn't specify a tensor type, thus not be able to decode and show.\"\n",
    "\n",
    "  @typedispatch\n",
    "  def _decode(self, t:TensorText): \n",
    "    assert self.hf_tokenizer, \"You should give huggingface tokenizer if you want to show batch.\"\n",
    "    if self.pretty_show: text = self.hf_tokenizer.decode([idx for idx in t if idx != self.hf_tokenizer.pad_token_id])\n",
    "    else: text = ' '.join(self.hf_tokenizer.convert_ids_to_tokens(t))\n",
    "    return TitledStr(text)\n",
    "\n",
    "  @typedispatch\n",
    "  def _decode(self, t:LMTensorText): return self._decode[TensorText](self, t)\n",
    "\n",
    "  @typedispatch\n",
    "  def _decode(self, t:TensorCategory): return Category(t.item())\n",
    "  \n",
    "class HF_Datasets(FilteredBase):\n",
    "  _dl_type,_dbunch_type = HF_Dataloader,DataLoaders\n",
    "  def __init__(self, hs_dsets: dict, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      `hs_dsets` (`Dict[nlp.Dataset]`): the order of dict items will be the order of `HF_Dataloader`s  \n",
    "    \"\"\"\n",
    "    self.hs_dsets = { split: HF_Dataset(dset, *args, **kwargs) for split, dset in hs_dsets.items()}\n",
    "  def subset(self, i): return list(self.hs_dsets.values())[i]\n",
    "  def __getitem__(self, split): return self.hs_dsets[split]\n",
    "  @property\n",
    "  def n_subsets(self): return len(self.hs_dsets)\n",
    "  def dataloaders(self, *args, cache_files=None, device='cpu', **kwargs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      `*args, **kwargs`: `for FilteredBase.dataloaders`\n",
    "      `cache_files` (`Optional[str]`, default:`None`): cache file names for `HF_Dataloader`s\n",
    "      `device` (`Optional[str]`, default:`'cpu'`): cuz will read a batch for test when creating `Dataloader`, so I set the default device to cpu to less the memory burden of cuda:0 \n",
    "    \"\"\"\n",
    "    dl_kwargs = kwargs.pop('dl_kwargs', [{} for _ in range(len(self.hs_dsets))])\n",
    "    if cache_files:\n",
    "      assert len(cache_files) == len(self.hs_dsets)\n",
    "      for i, dl_kwarg in enumerate(dl_kwargs): dl_kwarg['cache_file'] = cache_files[i]\n",
    "    return super().dataloaders(*args, dl_kwargs=dl_kwargs, device=device, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "823it [00:00, 4065.32it/s]"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everybody who has ever, worked in any office which contained any typewriter which had ever been used to type any letters which had to be signed by any administrator who ever worked in any department like mine will know what i mean.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hank plays the guitar and finds arrangements for all the old folk songs which are still sung in these hills, and ernie writes down all the old folk songs which are still sung in these hills.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>playing with matches is ; lots of fun, but doing, so and emptying gasoline from one can to another at the same time is a sport best reserved for arsons.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>in january 2002, a dull star in an obscure constellation suddenly became 600, 000 times more luminous than our sun, temporarily making it the brightest star in our galaxy.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>which folks up at corporate headquarters do you think that the sooner you solve this problem, the quicker you'll be able to tell t to buzz off?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>i finally worked up enough courage to ask which people up at corporate headquarters the sooner i solve this problem, the quicker i'll get free of.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ron wanted to wear a tuxedo to the party, but wear a tuxedo to the party caspar couldn't decide whether to.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>the dumplings which sasha is gobbling down faster than i can reheat the meatballs are extremely tasty, if i do say so.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>sam picked those packages up which are to be mailed tomorrow rest might, but he didn't want to do so until it had stopped raining.</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# tokenized_cola is {'train':nlp.Dataset, 'validation':nlp.Dataset, 'test':nlp.Dataset}\n",
    "cola_dsets = HF_Datasets(tokenized_cola, ['text_idxs', 'label'], hf_fast_tokenizer, pretty_show=True)\n",
    "cola_dls = cola_dsets.dataloaders(bs=32, pad_idx=hf_fast_tokenizer.pad_token_id)\n",
    "cola_dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Aggregate samples of HuggingFace/nlp dataset\n",
    "~ take traditional language model task for example ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregateTransform():\n",
    "  \"\"\"\n",
    "  Inherit this class and implement `accumulate` and `create_example`\n",
    "  \"\"\"\n",
    "  def __init__(self, hf_dset, inp_cols, out_cols, init_attrs, drop_last=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      `hf_dset` (`nlp.Dataset`)\n",
    "      `inp_cols` (`List[str]`)\n",
    "      `out_cols` (`List[str]`)\n",
    "      `init_attrs` (`List[str]`): name of attributes of children class that need to be their initial status when starts to aggregate dataset. i.e. Those defined in `__init__` and the value will changed during `accumulate`\n",
    "      `drop_last` (`Optional[bool]`, default: `False`): whether to drop the last accumulated sample.\n",
    "    \"\"\"\n",
    "    self.hf_dset = hf_dset\n",
    "    self.inp_cols, self.out_cols =  inp_cols, out_cols\n",
    "    # batched map need dataset be in python format\n",
    "    hf_dset.set_format(type=None, columns=inp_cols)\n",
    "    # dealing with last sample\n",
    "    self.last_idx = len(hf_dset) - 1\n",
    "    self.drop_last = drop_last\n",
    "    # for reset\n",
    "    self.init_attrs = init_attrs\n",
    "    self.original_vals = [deepcopy(getattr(self, attr)) for attr in init_attrs]  \n",
    "\n",
    "  def __call__(self, b, indices):\n",
    "    # `nlp.Dataset.map` first test with several samples which affects our attrs, so we need to reinitialize.\n",
    "    if 0 in indices: # reset\n",
    "      for attr,val in zip(self.init_attrs, self.original_vals): setattr(self, attr, deepcopy(val))\n",
    "\n",
    "    self.new_b = { c:[] for c in self.out_cols }\n",
    "    for z in zip(*b.values()):\n",
    "      self.accumulate(*z)\n",
    "    \n",
    "    # whehther put last example when it is last batch of `map`\n",
    "    if not self.drop_last and self.last_idx in indices: \n",
    "      self.commit_example(self.create_example())\n",
    "\n",
    "    return self.new_b\n",
    "\n",
    "  def commit_example(self, example):\n",
    "    if example is None: return\n",
    "    for col,val in example.items():\n",
    "      self.new_b[col].append(val) \n",
    "\n",
    "  def accumulate(self, *args):\n",
    "    \"\"\"\n",
    "    Given a example, do `self.commit_example(self.create_example()) when a new aggregated sample is ready.`\n",
    "    Args:\n",
    "      `args`: nlp.Dataset[i][inp_col] for inp_col in self.inp_cols\n",
    "    \"\"\" \n",
    "    raise NotImplementedError\n",
    "  \n",
    "  def create_example(self): \n",
    "    \"\"\"\n",
    "    When it is ready, create a sample (Dict[Any])\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def map(self, batch_size=1000, test_batch_size=20, **kwargs):\n",
    "    \"\"\"\n",
    "    `batch_size`: see `nlp.Dataset.map`\n",
    "    `test_batch_size` (`int`, default=`20`): we infer the new schema of the aggregated dataset by the outputs of testing that passed first `test_batch_size` samples to aggregate. Depending how many sample aggreagted can you have a sample, this number might need to be higher.\n",
    "    \"\"\"\n",
    "    test_inputs, test_indices = self.hf_dset[:test_batch_size], list(range(test_batch_size))\n",
    "    test_output = self(test_inputs,test_indices)\n",
    "    for col,val in test_output.items(): assert val, f\"Didn't get any example in test, you might want to try larger `test_batch_size` than {test_batch_size}\"\n",
    "    assert sorted(self.out_cols) == sorted(test_output.keys()), f\"Output columns are {self.out_cols}, but get example with {list(test_output.keys())}\"\n",
    "    arrow_schema = pa.Table.from_pydict(test_output).schema\n",
    "    return self.hf_dset.map(function=self, batched=True, batch_size=batch_size, with_indices=True,\n",
    "                            arrow_schema=arrow_schema, **kwargs)\n",
    "\n",
    "class LMTransform(AggregateTransform):\n",
    "  def __init__(self, hf_dset, max_len, text_col, x_text_col='x_text', y_text_col='y_text', **kwargs):\n",
    "    self.text_col, self.x_text_col, self.y_text_col = text_col, x_text_col, y_text_col\n",
    "    self._max_len = max_len + 1\n",
    "    self.residual_len, self.new_text = self._max_len, []\n",
    "    super().__init__(hf_dset, inp_cols=[text_col], out_cols=[x_text_col, y_text_col], init_attrs=['residual_len', 'new_text'], **kwargs)\n",
    "    \n",
    "\n",
    "  def accumulate(self, text): # *inp_cols\n",
    "    \"text: a list of indices\"\n",
    "    usable_len = len(text)\n",
    "    cursor = 0\n",
    "    while usable_len != 0:\n",
    "      use_len = min(usable_len, self.residual_len)\n",
    "      self.new_text += text[cursor:cursor+use_len]\n",
    "      self.residual_len -= use_len\n",
    "      usable_len -= use_len\n",
    "      cursor += use_len\n",
    "      if self.residual_len == 0:\n",
    "        self.commit_example(self.create_example())   \n",
    "\n",
    "  def create_example(self):\n",
    "    # when read all data, the accumulated new_text might be less than two characters.\n",
    "    if len(self.new_text) >= 2: \n",
    "      example = {self.x_text_col:self.new_text[:-1], self.y_text_col:self.new_text[1:]}\n",
    "    else:\n",
    "      example = None # mark \"don't commit this\"\n",
    "    # reset accumulators\n",
    "    self.new_text = []\n",
    "    self.residual_len = self._max_len\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 2/2 [00:00<00:00, 321.32it/s]Original dataset:\nnum of samples: 1043\nsecond to last sentence: John arranged for himself to get the prize.\n          last sentence: John talked to Bill about himself.\nLM dataset:\nnum of sampels: 481\nlast text (x): . john talked to bill about himself\nlast text (y): john talked to bill about himself.\n\n"
    }
   ],
   "source": [
    "cola_val = tokenized_cola['validation']\n",
    "lm_dataset = LMTransform(cola_val, max_len=20, text_col='text_idxs').map()\n",
    "\n",
    "print('Original dataset:')\n",
    "print('num of samples:', len(cola['validation']))\n",
    "print('second to last sentence:', cola['validation'][-2]['sentence'])\n",
    "print('          last sentence:', cola['validation'][-1]['sentence'])\n",
    "print('LM dataset:')\n",
    "print('num of sampels:', len(lm_dataset))\n",
    "print('last text (x):', hf_fast_tokenizer.decode(lm_dataset[-1]['x_text']))\n",
    "print('last text (y):', hf_fast_tokenizer.decode(lm_dataset[-1]['y_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey</td>\n      <td>sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey .</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less .</td>\n      <td>mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less . as</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>you eat the most , you want the least . the more you would want , the less you would</td>\n      <td>eat the most , you want the least . the more you would want , the less you would eat</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>. i demand that the more john eat , the more he pays . mary listen ##s to the grateful</td>\n      <td>i demand that the more john eat , the more he pays . mary listen ##s to the grateful dead</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>, she gets depressed . the ang ##rier mary got , the more she looked at pictures . the higher</td>\n      <td>she gets depressed . the ang ##rier mary got , the more she looked at pictures . the higher the</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>stakes , the lower his expectations are . the more fred is ob ##no ##xious , the less attention you</td>\n      <td>, the lower his expectations are . the more fred is ob ##no ##xious , the less attention you should</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>pay to him . john was lots more ob ##no ##xious than fred . the more people you give beer</td>\n      <td>to him . john was lots more ob ##no ##xious than fred . the more people you give beer to</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>, the more people get sick . the more does bill smoke , the more susan hates him . the</td>\n      <td>the more people get sick . the more does bill smoke , the more susan hates him . the more</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>pictures of him that appear in the news , the more embarrassed john becomes . every senator seems to become</td>\n      <td>of him that appear in the news , the more embarrassed john becomes . every senator seems to become more</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "lm_dl = HF_Dataloader(HF_Dataset(lm_dataset, {'x_text':LMTensorText, 'y_text':TensorText},hf_fast_tokenizer), sort=False, pad_idx=hf_fast_tokenizer.pad_token_id)\n",
    "lm_dl.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ELECTRA Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 2/2 [00:00<00:00, 414.97it/s]\n"
    }
   ],
   "source": [
    "class ELECTRADataTransform(AggregateTransform):\n",
    "  \n",
    "  def __init__(self, hf_dset, in_col, out_col, max_length, cls_idx, sep_idx):\n",
    "    self.in_col, self.out_col = in_col, out_col\n",
    "    self._current_sentences = []\n",
    "    self._current_length = 0\n",
    "    self._max_length = max_length\n",
    "    self._target_length = max_length\n",
    "    self.cls_idx, self.sep_idx = cls_idx, sep_idx\n",
    "    super().__init__(hf_dset, inp_cols=[in_col], out_cols=[out_col], \n",
    "                    init_attrs=['_current_sentences', '_current_length', '_target_length'])\n",
    "\n",
    "  # two functions required by AggregateTransform\n",
    "  def accumulate(self, tokids):\n",
    "    self.add_line(tokids)\n",
    "  \n",
    "  def create_example(self):\n",
    "    input_ids = self._create_example()\n",
    "    return {self.out_col: input_ids}\n",
    "\n",
    "  def add_line(self, tokids):\n",
    "    \"\"\"Adds a line of text to the current example being built.\"\"\"\n",
    "    self._current_sentences.append(tokids)\n",
    "    self._current_length += len(tokids)\n",
    "    if self._current_length >= self._target_length:\n",
    "      self.commit_example(self.create_example())\n",
    "\n",
    "  def _create_example(self):\n",
    "    \"\"\"Creates a pre-training example from the current list of sentences.\"\"\"\n",
    "    # small chance to only have one segment as in classification tasks\n",
    "    if random.random() < 0.1:\n",
    "      first_segment_target_length = 100000\n",
    "    else:\n",
    "      # -3 due to not yet having [CLS]/[SEP] tokens in the input text\n",
    "      first_segment_target_length = (self._target_length - 3) // 2\n",
    "\n",
    "    first_segment = []\n",
    "    second_segment = []\n",
    "    for sentence in self._current_sentences:\n",
    "      # the sentence goes to the first segment if (1) the first segment is\n",
    "      # empty, (2) the sentence doesn't put the first segment over length or\n",
    "      # (3) 50% of the time when it does put the first segment over length\n",
    "      if (len(first_segment) == 0 or\n",
    "          len(first_segment) + len(sentence) < first_segment_target_length or\n",
    "          (len(second_segment) == 0 and\n",
    "           len(first_segment) < first_segment_target_length and\n",
    "           random.random() < 0.5)):\n",
    "        first_segment += sentence\n",
    "      else:\n",
    "        second_segment += sentence\n",
    "\n",
    "    # trim to max_length while accounting for not-yet-added [CLS]/[SEP] tokens\n",
    "    first_segment = first_segment[:self._max_length - 2]\n",
    "    second_segment = second_segment[:max(0, self._max_length -\n",
    "                                         len(first_segment) - 3)]\n",
    "\n",
    "    # prepare to start building the next example\n",
    "    self._current_sentences = []\n",
    "    self._current_length = 0\n",
    "    # small chance for random-length instead of max_length-length example\n",
    "    if random.random() < 0.05:\n",
    "      self._target_length = random.randint(5, self._max_length)\n",
    "    else:\n",
    "      self._target_length = self._max_length\n",
    "\n",
    "    return self._make_example(first_segment, second_segment)\n",
    "\n",
    "  def _make_example(self, first_segment, second_segment):\n",
    "    \"\"\"Converts two \"segments\" of text into a tf.train.Example.\"\"\"\n",
    "    input_ids = [self.cls_idx] + first_segment + [self.sep_idx]\n",
    "    if second_segment:\n",
    "      input_ids += second_segment + [self.sep_idx]\n",
    "    return input_ids\n",
    "\n",
    "edset = ELECTRADataTransform(cola_val, 'text_idxs', 'tokids', 50, hf_fast_tokenizer.cls_token_id, hf_fast_tokenizer.sep_token_id).map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] the sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey . [SEP] the mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less . as you eat the most , [SEP]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[CLS] the more you would want , the less you would eat . i demand that the more john eat , the more he pays . mary listen ##s to the grateful dead , she gets depressed . the ang ##rier mary got , the more she looked at [SEP]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[CLS] the higher the stakes , the lower his expectations are . john was lots more ob ##no ##xious than fred . [SEP] the more fred is ob ##no ##xious , the less attention you should pay to him . the more people you give beer to , the [SEP]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[CLS] the more does bill smoke , the more susan hates him . who does john visit sally because he likes ? [SEP] the more pictures of him that appear in the news , the more embarrassed john becomes . every senator seems to become more corrupt , as [SEP]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[CLS] marianne did not leave . he could not ] have been working . he can not have been working . [SEP] you will believe bob . john has not kissed mary . i said that never in my life had i seen a place like bangor . mickey [SEP]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[CLS] there tended to be a lot of discussion . john tried to be a good boy . john is eager . we want john to win . [SEP] the box contained the ball from the tree . the tube was escaped by gas . water bubble ##d up [SEP]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[CLS] the tub leaked water . what the water did to the bottle was fill it . john owns the book . [SEP] what the water did to the whole bottle was fill it . the tank leaked the fluid free . john lay the ball in the box [SEP]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[CLS] most people probably consider , even though the courts didn ' t actually find , klaus guilty of murder . [SEP] mary beautifully plays the violin . clearly , john probably will immediately learn french perfectly . sue gave to bill a book . the men will all [SEP]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>[CLS] they represented seriously to the dean mary as a genuine linguist . us love they . it is nice to go abroad . [SEP] mary intended john to go abroad . i remembered having kissed mary . i can ' t believe fred won ' t , either [SEP]</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "e_dl = HF_Dataloader(HF_Dataset(edset, ['tokids'], hf_fast_tokenizer), pad_idx=hf_fast_tokenizer.pad_token_id, sort=False)\n",
    "e_dl.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test caching and filtering feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'train': 26, 'validation': 2, 'test': 6}\n"
    }
   ],
   "source": [
    "l = 23\n",
    "num = {}\n",
    "for split in tokenized_cola:\n",
    "  num[split] = reduce(lambda sum, sample: sum+(1 if len(sample['text_idxs'])==l else 0), \n",
    "                      tokenized_cola[split], 0)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-6a20dc8f1b48>, line 1)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-6a20dc8f1b48>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for f in ['/tmp/cctrain.json','/tmp/ccval.json', '/tmp/cctest.json']\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for f in ['/tmp/cctrain.json','/tmp/ccval.json', '/tmp/cctest.json']\n",
    "  if Path(f).exists(): os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everybody who has ever, worked in any office which contained any typewriter which had ever been used to type any letters which had to be signed by any administrator who ever worked in any department like mine will know what i mean.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hank plays the guitar and finds arrangements for all the old folk songs which are still sung in these hills, and ernie writes down all the old folk songs which are still sung in these hills.</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "ccola_dsets = HF_Datasets(tokenized_cola, ['text_idxs', 'label'], hf_fast_tokenizer, pretty_show=True)\n",
    "ccola_dls = ccola_dsets.dataloaders(bs=32, pad_idx=hf_fast_tokenizer.pad_token_id, filterout=lambda inpids,label: len(inpids)==l, cache_files=['/tmp/cctrain.json','/tmp/ccval.json', '/tmp/cctest.json'])\n",
    "ccola_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if we correctly filter out by checking the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, split in enumerate(tokenized_cola):\n",
    "  assert ccola_dls[i].n == len(tokenized_cola[split])-num[split],f\"{split}: {ccola_dls[i].n}, {len(tokenized_cola[split])}, {num[split]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we load the caches, and it should be fast and bars sholdn't appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccola_dls = ccola_dsets.dataloaders(bs=32, pad_idx=hf_fast_tokenizer.pad_token_id, filterout=lambda inpids,label: len(inpids)==l, cache_files=['/tmp/cctrain.json','/tmp/ccval.json', '/tmp/cctest.json'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}