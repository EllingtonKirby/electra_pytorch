{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace as bk\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pyarrow as pa\n",
    "import nlp\n",
    "from transformers import ElectraTokenizerFast\n",
    "hf_fast_tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-small-generator\")\n",
    "from fastai2.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HF_TokenizeTfm():\n",
    "  \n",
    "  def __init__(self, hf_dset, cols, hf_tokenizer, remove_original=False):\n",
    "    if isinstance(cols, list): cols = {c:c for c in cols}\n",
    "    assert isinstance(cols, dict)\n",
    "    self.hf_dset, self.cols, self.tokenizer = hf_dset, cols, hf_tokenizer\n",
    "    self.remove_original = remove_original\n",
    "    \"\"\"\n",
    "    If don't specify cache file name, it will be hashed binary of pickled function that\n",
    "    passed to `map`, so if you pass the same function, it knows to use cache.\n",
    "    But tokenizer can't be pickled, so use tokenizer config to make tfms use different \n",
    "    tokenizers unique.  \n",
    "    \"\"\"\n",
    "    self.tokenizer_config = hf_tokenizer.pretrained_init_configuration\n",
    "  \n",
    "  def __call__(self, example):\n",
    "    for in_col, out_col in self.cols.items():\n",
    "      example[out_col] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(example[in_col]))\n",
    "    return example\n",
    "\n",
    "  def __getstate__(self):\n",
    "    \"specify something you don't want pickle here, remember to use copy to not modfiy orginal instance\"\n",
    "    state = self.__dict__.copy() \n",
    "    state['tokenizer'] = None \n",
    "    return state\n",
    "\n",
    "  def map(self, **kwargs):\n",
    "    if self.remove_original:\n",
    "      assert 'remove_columns' not in kwargs, \"You have specified to remove all original columns.\"\n",
    "      return self.hf_dset.map(self, remove_columns=self.hf_dset.column_names, **kwargs)\n",
    "    else:\n",
    "      return self.hf_dset.map(self, **kwargs)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "8551it [00:01, 6405.28it/s]\n1043it [00:00, 9191.29it/s]\n1063it [00:00, 8597.16it/s]\n"
    }
   ],
   "source": [
    "cola = nlp.load_dataset('glue', 'cola')\n",
    "\n",
    "tokenized_cola = {}\n",
    "for split, dset in cola.items():\n",
    "  tokenized_cola[split] = HF_TokenizeTfm(dset, {'sentence':'text_idxs'}, hf_fast_tokenizer, remove_original=True).map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Integration with fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delegates()\n",
    "class HF_Dataloader(TfmdDL):\n",
    "  \n",
    "  def __init__(self, dataset, pad_idx, sort=True, **kwargs):\n",
    "    if pad_idx is not None:\n",
    "      kwargs['before_batch'] = partial(pad_input_chunk, pad_idx=pad_idx, pad_first=False)\n",
    "    if sort:\n",
    "      self.lens = [ len(sample[0]) for sample in dataset ]\n",
    "    store_attr(self, 'pad_idx,sort')\n",
    "    super().__init__(dataset, **kwargs)\n",
    "  \n",
    "  def get_idxs(self):\n",
    "    idxs = super().get_idxs()\n",
    "    if not self.sort : return idxs\n",
    "    return sorted(idxs, key=lambda i: self.lens[i], reverse=True)\n",
    "\n",
    "  def new(self, dataset, **kwargs):\n",
    "    return super().new(dataset=self.dataset, pad_idx=self.pad_idx, sort=self.sort, **kwargs)\n",
    "\n",
    "class HF_Dataset(FilteredBase):\n",
    "  \n",
    "  def __init__(self, hf_dset, cols, hf_tokenizer=None, pretty_show=False, n_inp=1):\n",
    "    \n",
    "    # some default setting for tensor type used in decoding\n",
    "    if isinstance(cols, list): \n",
    "      if n_inp==1: \n",
    "        if len(cols)==1: cols = {cols[0]: TensorText}\n",
    "        elif len(cols)==2: cols = {cols[0]: TensorText, cols[1]: TensorCategory}\n",
    "      else: cols = { c: noop for c in cols }\n",
    "    assert isinstance(cols, dict)\n",
    "    \n",
    "    # make dataset output pytorch tensor\n",
    "    if hf_dset.format['type'] != 'torch': \n",
    "      hf_dset.set_format( type='torch', columns=list(cols.keys()) )\n",
    "\n",
    "    # store attributes\n",
    "    store_attr(self, \"hf_dset,cols,n_inp,hf_tokenizer,pretty_show\")\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sample = self.hf_dset[idx]\n",
    "    return tuple( tensor_cls(sample[col]) for col, tensor_cls in self.cols.items() )\n",
    "\n",
    "  def __len__(self): return len(self.hf_dset)\n",
    "\n",
    "  @property\n",
    "  def column_names(self): return list(self.cols.keys())\n",
    "\n",
    "  def decode(self, o, full=True): \n",
    "    return tuple( self._decode(o_) for o_ in o )\n",
    "\n",
    "  @typedispatch\n",
    "  def _decode(self, t:TensorText): \n",
    "    assert self.hf_tokenizer, \"You should give huggingface tokenizer if you want to show batch.\"\n",
    "    if self.pretty_show: text = self.hf_tokenizer.decode([idx for idx in t if idx != self.hf_tokenizer.pad_token_id])\n",
    "    else: text = ' '.join(self.hf_tokenizer.convert_ids_to_tokens(t))\n",
    "    return TitledStr(text)\n",
    "\n",
    "  @typedispatch\n",
    "  def _decode(self, t:LMTensorText): return self._decode[TensorText](self, t)\n",
    "\n",
    "  @typedispatch\n",
    "  def _decode(self, t:TensorCategory): return Category(t.item())\n",
    "  \n",
    "class HF_Datasets(FilteredBase):\n",
    "  _dl_type,_dbunch_type = HF_Dataloader,DataLoaders\n",
    "  def __init__(self, hs_dsets: dict, *args, **kwargs):\n",
    "    self.hs_dsets = { split: HF_Dataset(dset, *args, **kwargs) for split, dset in hs_dsets.items()}\n",
    "  def subset(self, i): return list(self.hs_dsets.values())[i]\n",
    "  def __getitem__(self, split): return self.hs_dsets[split]\n",
    "  @property\n",
    "  def n_subsets(self): return len(self.hs_dsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everybody who has ever, worked in any office which contained any typewriter which had ever been used to type any letters which had to be signed by any administrator who ever worked in any department like mine will know what i mean.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hank plays the guitar and finds arrangements for all the old folk songs which are still sung in these hills, and ernie writes down all the old folk songs which are still sung in these hills.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>playing with matches is ; lots of fun, but doing, so and emptying gasoline from one can to another at the same time is a sport best reserved for arsons.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>in january 2002, a dull star in an obscure constellation suddenly became 600, 000 times more luminous than our sun, temporarily making it the brightest star in our galaxy.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>which folks up at corporate headquarters do you think that the sooner you solve this problem, the quicker you'll be able to tell t to buzz off?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>one of the jewish children is a spunky girl, who gave a black eye to the kid with the german roots before the start of the war.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>will put a picture of bill on your desk before tomorrow, this girl in the red coat will put a picture of bill on your desk before tomorrow.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>the dumplings which sasha is gobbling down faster than i can reheat the meatballs are extremely tasty, if i do say so.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>sam picked those packages up which are to be mailed tomorrow rest might, but he didn't want to do so until it had stopped raining.</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "cola_dsets = HF_Datasets(tokenized_cola, ['text_idxs', 'label'], hf_fast_tokenizer, pretty_show=True)\n",
    "cola_dls = cola_dsets.dataloaders(bs=32, pad_idx=hf_fast_tokenizer.pad_token_id)\n",
    "cola_dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Aggregate samples of HuggingFace/nlp dataset\n",
    "~ take traditional language model task for example ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregateTransform():\n",
    "  def __init__(self, hf_dset, inp_cols, out_cols, init_attrs, drop_last=False):\n",
    "    self.hf_dset = hf_dset\n",
    "    self.inp_cols, self.out_cols =  inp_cols, out_cols\n",
    "    # batched map need dataset be in python format\n",
    "    hf_dset.set_format(type=None, columns=inp_cols)\n",
    "    # dealing with last sample\n",
    "    self.last_idx = len(hf_dset) - 1\n",
    "    self.drop_last = drop_last\n",
    "    # for reset\n",
    "    self.init_attrs = init_attrs\n",
    "    self.original_vals = [deepcopy(getattr(self, attr)) for attr in init_attrs]  \n",
    "\n",
    "  def __call__(self, b, indices):\n",
    "    # `nlp.Dataset.map` first test with several samples which affects our attrs, so we need to reinitialize.\n",
    "    if 0 in indices: # reset\n",
    "      for attr,val in zip(self.init_attrs, self.original_vals): setattr(self, attr, deepcopy(val))\n",
    "\n",
    "    self.new_b = { c:[] for c in self.out_cols }\n",
    "    for z in tqdm(list(zip(*b.values())), leave=False):\n",
    "      self.accumulate(*z)\n",
    "    \n",
    "    # whehther put last example when it is last batch of `map`\n",
    "    if not self.drop_last and self.last_idx in indices: \n",
    "      self.commit_example(self.create_example())\n",
    "\n",
    "    return self.new_b\n",
    "\n",
    "  def commit_example(self, example):\n",
    "    if example is None: return\n",
    "    for col,val in example.items():\n",
    "      self.new_b[col].append(val) \n",
    "\n",
    "  def accumulate(self, *args): raise NotImplementedError\n",
    "  def create_example(self): raise NotImplementedError\n",
    "\n",
    "  def map(self, batch_size=1000, test_batch_size=20, **kwargs):\n",
    "    test_inputs, test_indices = self.hf_dset[:test_batch_size], list(range(test_batch_size))\n",
    "    test_output = self(test_inputs,test_indices)\n",
    "    for col,val in test_output.items(): assert val, f\"Didn't get any example in test, you might want to try larger `test_batch_size` than {test_batch_size}\"\n",
    "    assert sorted(self.out_cols) == sorted(test_output.keys()), f\"Output columns are {self.out_cols}, but get example with {list(test_output.keys())}\"\n",
    "    arrow_schema = pa.Table.from_pydict(test_output).schema\n",
    "    return self.hf_dset.map(function=self, batched=True, batch_size=batch_size, with_indices=True,\n",
    "                            arrow_schema=arrow_schema, **kwargs)\n",
    "\n",
    "class LMTransform(AggregateTransform):\n",
    "  def __init__(self, hf_dset, max_len, text_col, x_text_col='x_text', y_text_col='y_text', **kwargs):\n",
    "    self.text_col, self.x_text_col, self.y_text_col = text_col, x_text_col, y_text_col\n",
    "    self._max_len = max_len + 1\n",
    "    self.residual_len, self.new_text = self._max_len, []\n",
    "    super().__init__(hf_dset, inp_cols=[text_col], out_cols=[x_text_col, y_text_col], init_attrs=['residual_len', 'new_text'], **kwargs)\n",
    "    \n",
    "\n",
    "  def accumulate(self, text): # *inp_cols\n",
    "    \"text: a list of indices\"\n",
    "    usable_len = len(text)\n",
    "    cursor = 0\n",
    "    while usable_len != 0:\n",
    "      use_len = min(usable_len, self.residual_len)\n",
    "      self.new_text += text[cursor:cursor+use_len]\n",
    "      self.residual_len -= use_len\n",
    "      usable_len -= use_len\n",
    "      cursor += use_len\n",
    "      if self.residual_len == 0:\n",
    "        self.commit_example(self.create_example())   \n",
    "\n",
    "  def create_example(self):\n",
    "    # when read all data, the accumulated new_text might be less than two characters.\n",
    "    if len(self.new_text) >= 2: \n",
    "      example = {self.x_text_col:self.new_text[:-1], self.y_text_col:self.new_text[1:]}\n",
    "    else:\n",
    "      example = None # mark \"don't commit this\"\n",
    "    # reset accumulators\n",
    "    self.new_text = []\n",
    "    self.residual_len = self._max_len\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0%|          | 0/2 [00:00<?, ?it/s]Original dataset:\nnum of samples: 1043\nsecond to last sentence: John arranged for himself to get the prize.\n          last sentence: John talked to Bill about himself.\nLM dataset:\nnum of sampels: 481\nlast text (x): . john talked to bill about himself\nlast text (y): john talked to bill about himself.\n"
    }
   ],
   "source": [
    "cola_val = tokenized_cola['validation']\n",
    "lm_dataset = LMTransform(cola_val, max_len=20, text_col='text_idxs').map()\n",
    "\n",
    "print('Original dataset:')\n",
    "print('num of samples:', len(cola['validation']))\n",
    "print('second to last sentence:', cola['validation'][-2]['sentence'])\n",
    "print('          last sentence:', cola['validation'][-1]['sentence'])\n",
    "print('LM dataset:')\n",
    "print('num of sampels:', len(lm_dataset))\n",
    "print('last text (x):', hf_fast_tokenizer.decode(lm_dataset[-1]['x_text']))\n",
    "print('last text (y):', hf_fast_tokenizer.decode(lm_dataset[-1]['y_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey</td>\n      <td>sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey .</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less .</td>\n      <td>mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less . as</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>you eat the most , you want the least . the more you would want , the less you would</td>\n      <td>eat the most , you want the least . the more you would want , the less you would eat</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>. i demand that the more john eat , the more he pays . mary listen ##s to the grateful</td>\n      <td>i demand that the more john eat , the more he pays . mary listen ##s to the grateful dead</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>, she gets depressed . the ang ##rier mary got , the more she looked at pictures . the higher</td>\n      <td>she gets depressed . the ang ##rier mary got , the more she looked at pictures . the higher the</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>stakes , the lower his expectations are . the more fred is ob ##no ##xious , the less attention you</td>\n      <td>, the lower his expectations are . the more fred is ob ##no ##xious , the less attention you should</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>pay to him . john was lots more ob ##no ##xious than fred . the more people you give beer</td>\n      <td>to him . john was lots more ob ##no ##xious than fred . the more people you give beer to</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>, the more people get sick . the more does bill smoke , the more susan hates him . the</td>\n      <td>the more people get sick . the more does bill smoke , the more susan hates him . the more</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>pictures of him that appear in the news , the more embarrassed john becomes . every senator seems to become</td>\n      <td>of him that appear in the news , the more embarrassed john becomes . every senator seems to become more</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "lm_dl = HF_Dataloader(HF_Dataset(lm_dataset, {'x_text':LMTensorText, 'y_text':TensorText},hf_fast_tokenizer), sort=False, pad_idx=hf_fast_tokenizer.pad_token_id)\n",
    "lm_dl.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ELECTRA Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0%|          | 0/2 [00:00<?, ?it/s]\n  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n\u001b[A\n  0%|          | 0/43 [00:00<?, ?it/s]\u001b[A\n100%|██████████| 2/2 [00:00<00:00, 109.82it/s]\n"
    }
   ],
   "source": [
    "class ELECTRADataTransform(AggregateTransform):\n",
    "  \n",
    "  def __init__(self, hf_dset, in_col, out_col, max_length, cls_idx, sep_idx):\n",
    "    self.in_col, self.out_col = in_col, out_col\n",
    "    self._current_sentences = []\n",
    "    self._current_length = 0\n",
    "    self._max_length = max_length\n",
    "    self._target_length = max_length\n",
    "    self.cls_idx, self.sep_idx = cls_idx, sep_idx\n",
    "    super().__init__(hf_dset, inp_cols=[in_col], out_cols=[out_col], \n",
    "                    init_attrs=['_current_sentences', '_current_length', '_target_length'])\n",
    "\n",
    "  # two functions required by AggregateTransform\n",
    "  def accumulate(self, tokids):\n",
    "    self.add_line(tokids)\n",
    "  \n",
    "  def create_example(self):\n",
    "    input_ids = self._create_example()\n",
    "    return {self.out_col: input_ids}\n",
    "\n",
    "  def add_line(self, tokids):\n",
    "    \"\"\"Adds a line of text to the current example being built.\"\"\"\n",
    "    self._current_sentences.append(tokids)\n",
    "    self._current_length += len(tokids)\n",
    "    if self._current_length >= self._target_length:\n",
    "      self.commit_example(self.create_example())\n",
    "\n",
    "  def _create_example(self):\n",
    "    \"\"\"Creates a pre-training example from the current list of sentences.\"\"\"\n",
    "    # small chance to only have one segment as in classification tasks\n",
    "    if random.random() < 0.1:\n",
    "      first_segment_target_length = 100000\n",
    "    else:\n",
    "      # -3 due to not yet having [CLS]/[SEP] tokens in the input text\n",
    "      first_segment_target_length = (self._target_length - 3) // 2\n",
    "\n",
    "    first_segment = []\n",
    "    second_segment = []\n",
    "    for sentence in self._current_sentences:\n",
    "      # the sentence goes to the first segment if (1) the first segment is\n",
    "      # empty, (2) the sentence doesn't put the first segment over length or\n",
    "      # (3) 50% of the time when it does put the first segment over length\n",
    "      if (len(first_segment) == 0 or\n",
    "          len(first_segment) + len(sentence) < first_segment_target_length or\n",
    "          (len(second_segment) == 0 and\n",
    "           len(first_segment) < first_segment_target_length and\n",
    "           random.random() < 0.5)):\n",
    "        first_segment += sentence\n",
    "      else:\n",
    "        second_segment += sentence\n",
    "\n",
    "    # trim to max_length while accounting for not-yet-added [CLS]/[SEP] tokens\n",
    "    first_segment = first_segment[:self._max_length - 2]\n",
    "    second_segment = second_segment[:max(0, self._max_length -\n",
    "                                         len(first_segment) - 3)]\n",
    "\n",
    "    # prepare to start building the next example\n",
    "    self._current_sentences = []\n",
    "    self._current_length = 0\n",
    "    # small chance for random-length instead of max_length-length example\n",
    "    if random.random() < 0.05:\n",
    "      self._target_length = random.randint(5, self._max_length)\n",
    "    else:\n",
    "      self._target_length = self._max_length\n",
    "\n",
    "    return self._make_example(first_segment, second_segment)\n",
    "\n",
    "  def _make_example(self, first_segment, second_segment):\n",
    "    \"\"\"Converts two \"segments\" of text into a tf.train.Example.\"\"\"\n",
    "    input_ids = [self.cls_idx] + first_segment + [self.sep_idx]\n",
    "    if second_segment:\n",
    "      input_ids += second_segment + [self.sep_idx]\n",
    "    return input_ids\n",
    "\n",
    "edset = ELECTRADataTransform(cola_val, 'text_idxs', 'tokids', 50, hf_fast_tokenizer.cls_token_id, hf_fast_tokenizer.sep_token_id).map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] the sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey . the mechanical doll wr ##ig ##gled itself loose . [SEP] if you had eaten more , you would want less . as you eat the most , [SEP]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[CLS] the more you would want , the less you would eat . i demand that the more john eat , the more he pays . [SEP] mary listen ##s to the grateful dead , she gets depressed . the ang ##rier mary got , the more she looked [SEP]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[CLS] the higher the stakes , the lower his expectations are . the more fred is ob ##no ##xious , the less attention you should pay to him . [SEP] john was lots more ob ##no ##xious than fred . the more people you give beer to , the [SEP]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[CLS] the more does bill smoke , the more susan hates him . who does john visit sally because he likes ? [SEP] the more pictures of him that appear in the news , the more embarrassed john becomes . every senator seems to become more corrupt , as [SEP]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[CLS] marianne did not leave . he could not ] have been working . he can not have been working . you will believe bob . [SEP] john has not kissed mary . i said that never in my life had i seen a place like bangor . mickey [SEP]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[CLS] there tended to be a lot of discussion . john tried to be a good boy . john is eager . [SEP] we want john to win . the box contained the ball from the tree . the tube was escaped by gas . water bubble ##d up [SEP]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[CLS] the tub leaked water . what the water did to the bottle was fill it . what the water did to the whole bottle was fill it . [SEP] the tank leaked the fluid free . john lay the ball in the box . john owns the book [SEP]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[CLS] most people probably consider , even though the courts didn ' t actually find , klaus guilty of murder . [SEP] mary beautifully plays the violin . clearly , john probably will immediately learn french perfectly . sue gave to bill a book . the men will all [SEP]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>[CLS] they represented seriously to the dean mary as a genuine linguist . us love they . [SEP] it is nice to go abroad . mary intended john to go abroad . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "e_dl = HF_Dataloader(HF_Dataset(edset, ['tokids'], hf_fast_tokenizer), pad_idx=hf_fast_tokenizer.pad_token_id, sort=False)\n",
    "e_dl.show_batch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}