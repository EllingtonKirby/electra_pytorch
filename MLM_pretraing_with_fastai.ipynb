{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from functools import partial\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from IPython.core.debugger import set_trace as bk\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import nlp\n",
    "from transformers import ElectraModel, ElectraConfig, ElectraTokenizerFast, ElectraForMaskedLM,ElectraForPreTraining\n",
    "from fastai2.text.all import *\n",
    "import wandb\n",
    "from fastai2.callback.wandb import WandbCallback\n",
    "from _utils.would_like_to_pr import *\n",
    "from _utils.huggingface import *\n",
    "from _utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = MyConfig({\n",
    "    'device': 'cuda:0',\n",
    "    'size': 'small',\n",
    "    'use_fp16': True,\n",
    "    'gen_smooth_label': False,\n",
    "    'disc_smooth_label': False,\n",
    "    'balanced_label': False,\n",
    "    'tfdata': False,\n",
    "    'sort_sample': True,\n",
    "    'shuffle': True,\n",
    "    'percent': False,\n",
    "    # cache the data under it\n",
    "    'cache_dir': Path.home()/\"datasets\", # ! should be `pathlib.Path` istance\n",
    "    # cache checkpoints under checkpoint_dir/electra_pretrain\n",
    "    'checkpoint_dir': Path.home()/'checkpoints', # ! should be `pathlib.Path` istance\n",
    "})\n",
    "\n",
    "i = ['small', 'base', 'large'].index(c.size)\n",
    "c.mask_prob = [0.15, 0.15, 0.25][i]\n",
    "c.lr = [5e-4, 2e-4, 2e-4][i]\n",
    "c.bs = [128, 256, 2048][i]\n",
    "c.steps = [10**6, 766*1000, 400*1000][i]\n",
    "c.max_length = [128, 512, 512][i]\n",
    "c.cache_dir.mkdir(exist_ok=True,parents=True)\n",
    "c.checkpoint_dir.mkdir(exist_ok=True)\n",
    "gen_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-generator')\n",
    "disc_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-discriminator')\n",
    "hf_tokenizer = ElectraTokenizerFast.from_pretrained(f\"google/electra-{c.size}-generator\")\n",
    "if c.size in ['small', 'base']:\n",
    "  wiki_cache_dir = c.cache_dir/\"wikipedia/20200501.en/1.0.0\"\n",
    "  book_cache_dir = c.cache_dir/\"bookcorpus/plain_text/1.0.0\"\n",
    "  wbdl_cache_dir = c.cache_dir/\"wikibook_dl\"\n",
    "  wbdl_cache_dir.mkdir(exist_ok=True)\n",
    "print(os.getpid())\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if c.size in ['small', 'base'] and not c.tfdata:\n",
    "  \n",
    "  # wiki\n",
    "  if (wiki_cache_dir/f\"wiki_electra_{c.max_length}.arrow\").exists():\n",
    "    print('loading the electra data (wiki)')\n",
    "    wiki = nlp.Dataset.from_file(str(wiki_cache_dir/f\"wiki_electra_{c.max_length}.arrow\"))\n",
    "  else:\n",
    "    print('load/download wiki dataset')\n",
    "    wiki = nlp.load_dataset('wikipedia', '20200501.en', cache_dir=c.cache_dir)['train']\n",
    "  \n",
    "    print('creat data from wiki dataset for ELECTRA')\n",
    "    wiki = ELECTRADataTransform(wiki, is_docs=True, text_col={'text':'input_ids'}, max_length=c.max_length, hf_toker=hf_tokenizer).map(cache_file_name=str(wiki_cache_dir/f\"wiki_electra_{c.max_length}.arrow\"))\n",
    "\n",
    "  # bookcorpus\n",
    "  if (book_cache_dir/f\"book_electra_{c.max_length}.arrow\").exists():\n",
    "    print('loading the electra data (BookCorpus)')\n",
    "    book = nlp.Dataset.from_file(str(book_cache_dir/f\"book_electra_{c.max_length}.arrow\"))\n",
    "  else:\n",
    "    print('load/download BookCorpus dataset')\n",
    "    book = nlp.load_dataset('bookcorpus', cache_dir=c.cache_dir)['train']\n",
    "  \n",
    "    print('creat data from BookCorpus dataset for ELECTRA')\n",
    "    book = ELECTRADataTransform(book, is_docs=False, text_col={'text':'input_ids'}, max_length=c.max_length, hf_toker=hf_tokenizer).map(cache_file_name=str(book_cache_dir/f\"book_electra_{c.max_length}.arrow\"))\n",
    "\n",
    "  wb_data = HF_MergedDataset(wiki, book)\n",
    "  wb_dsets = HF_Datasets({'train': wb_data}, cols=['input_ids'], hf_toker=hf_tokenizer)\n",
    "  dls = wb_dsets.dataloaders(bs=c.bs, \n",
    "                             shuffle_train=c.shuffle,\n",
    "                             srtkey_fc=None if c.sort_sample else False, \n",
    "                             cache_dir=Path.home()/'datasets/wikibook_dl', cache_name='dl_{split}.json')\n",
    "\n",
    "else: # for large size\n",
    "  #raise NotImplementedError\n",
    "  import tensorflow as tf\n",
    "  tf.compat.v1.enable_eager_execution()\n",
    "  from electra.configure_pretraining import PretrainingConfig\n",
    "  from electra.pretrain.pretrain_data import get_input_fn\n",
    "  class TFDataLoader():\n",
    "    def __init__(self, batch_size, size, device='cpu'):\n",
    "      self.input_func = get_input_fn(PretrainingConfig('eeee', '../electra/data', **{'model_size': size}), True)\n",
    "      self.bs = batch_size\n",
    "      self.device = device\n",
    "      self.n_inp = 1 # for fastai to split x and y\n",
    "    def __iter__(self):\n",
    "      infinite_data = self.input_func({'batch_size':self.bs})\n",
    "      for features in infinite_data:\n",
    "        yield (torch.tensor(features['input_ids'].numpy(), dtype=torch.long, device=self.device),)\n",
    "    def to(self, device):\n",
    "      self.device = device\n",
    "      return self\n",
    "    def __len__(self): return 62500 # 6.25% of 10**6\n",
    "  dls = DataLoaders(TFDataLoader(c.bs, c.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Masked language model objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLM objective callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modified from HuggingFace/transformers (https://github.com/huggingface/transformers/blob/0a3d0e02c5af20bfe9091038c4fd11fb79175546/src/transformers/data/data_collator.py#L102). It is\n",
    "- few ms faster: intead of a[b] a on gpu b on cpu, tensors here are all in the same device\n",
    "- few tens of us faster: in how we create special token mask\n",
    "- doesn't require huggingface tokenizer\n",
    "- cost you only 20 ms on a (128,128) tensor, so dynamic masking is cheap   \n",
    "\"\"\"\n",
    "# https://github.com/huggingface/transformers/blob/1789c7daf1b8013006b0aef6cb1b8f80573031c5/examples/run_language_modeling.py#L179\n",
    "def mask_tokens(inputs, mask_token_index, vocab_size, special_token_indices, mlm_probability=0.15, ignore_index=-100):\n",
    "  \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
    "  \"ignore_index in nn.CrossEntropy is default to -100, so you don't need to specify ignore_index in loss\"\n",
    "  \n",
    "  device = inputs.device\n",
    "  labels = inputs.clone()\n",
    "  # We sample a few tokens in each sequence for masked-LM training (with probability mlm_probability defaults to 0.15 in Bert/RoBER\n",
    "  probability_matrix = torch.full(labels.shape, mlm_probability, device=device)\n",
    "  special_tokens_mask = torch.full(inputs.shape, False, dtype=torch.bool, device=device)\n",
    "  for sp_id in special_token_indices:\n",
    "    special_tokens_mask = special_tokens_mask | (inputs==sp_id)\n",
    "  probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "  mlm_mask = torch.bernoulli(probability_matrix).bool()\n",
    "  labels[~mlm_mask] = ignore_index  # We only compute loss on masked tokens\n",
    "\n",
    "  # 80% of the time, we replace masked input tokens with mask_token\n",
    "  mask_token_mask = torch.bernoulli(torch.full(labels.shape, 0.8, device=device)).bool() & mlm_mask\n",
    "  inputs[mask_token_mask] = mask_token_index\n",
    "\n",
    "  # 10% of the time, we replace masked input tokens with random word\n",
    "  replace_token_mask = torch.bernoulli(torch.full(labels.shape, 0.5, device=device)).bool() & mlm_mask & ~mask_token_mask\n",
    "  random_words = torch.randint(vocab_size, labels.shape, dtype=torch.long, device=device)\n",
    "  inputs[replace_token_mask] = random_words[replace_token_mask]\n",
    "\n",
    "  # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "  return inputs, labels, mlm_mask\n",
    "\n",
    "class MaskedLMCallback(Callback):\n",
    "  @delegates(mask_tokens)\n",
    "  def __init__(self, mask_tok_id, special_tok_ids, vocab_size, ignore_index=-100, for_electra=False, **kwargs):\n",
    "    self.ignore_index = ignore_index\n",
    "    self.for_electra = for_electra\n",
    "    self.mask_tokens = partial(mask_tokens,\n",
    "                               mask_token_index=mask_tok_id,\n",
    "                               special_token_indices=special_tok_ids,\n",
    "                               vocab_size=vocab_size,\n",
    "                               ignore_index=-100,\n",
    "                               **kwargs)\n",
    "\n",
    "  def begin_batch(self):\n",
    "    text_indices = self.xb[0]\n",
    "    masked_inputs, labels, is_mlm_applied = self.mask_tokens(text_indices)\n",
    "    if self.for_electra:\n",
    "      self.learn.xb, self.learn.yb = (masked_inputs, is_mlm_applied, labels), (labels,)\n",
    "    else:\n",
    "      self.learn.xb, self.learn.yb = (masked_inputs,), (labels,)\n",
    "\n",
    "  @delegates(TfmdDL.show_batch)\n",
    "  def show_batch(self, dl, idx_show_ignored, verbose=True, **kwargs):\n",
    "    b = dl.one_batch()\n",
    "    inputs = b[0]\n",
    "    masked_inputs, labels, is_mlm_applied = self.mask_tokens(inputs.clone())\n",
    "    # check\n",
    "    assert torch.equal(is_mlm_applied, labels!=self.ignore_index)\n",
    "    assert torch.equal((~is_mlm_applied *masked_inputs + is_mlm_applied * labels), inputs)\n",
    "    # change symbol to show the ignored position\n",
    "    labels[labels==self.ignore_index] = idx_show_ignored\n",
    "    # some notice to help understand the masking mechanism\n",
    "    if verbose: \n",
    "      print(\"We won't count loss from position where y is ignore index\")\n",
    "      print(\"Notice 1. Positions have label token in y will be either [Mask]/other token/orginal token in x\")\n",
    "      print(\"Notice 2. Special tokens (CLS, SEP) won't be masked.\")\n",
    "      print(\"Notice 3. Dynamic masking: every time you run gives you different results.\")\n",
    "    # show\n",
    "    tfm_b =(masked_inputs, is_mlm_applied, labels, labels) if self.for_electra else (masked_inputs, labels)   \n",
    "    dl.show_batch(b=tfm_b, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_cb = MaskedLMCallback(mask_tok_id=hf_tokenizer.mask_token_id, \n",
    "                          special_tok_ids=hf_tokenizer.all_special_ids, \n",
    "                          vocab_size=hf_tokenizer.vocab_size,\n",
    "                          mlm_probability=c.mask_prob,\n",
    "                          for_electra=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ELECTRA (replaced token detection objective)\n",
    "see details in paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELECTRAModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, generator, discriminator, pad_idx):\n",
    "    super().__init__()\n",
    "    self.generator, self.discriminator = generator,discriminator\n",
    "    self.pad_idx = pad_idx\n",
    "    self.gumbel_dist = torch.distributions.gumbel.Gumbel(0.,1.)\n",
    "    self.toker = hf_tokenizer\n",
    "\n",
    "  def to(self, *args, **kwargs):\n",
    "    super().to(*args, **kwargs)\n",
    "    a_tensor = next(self.parameters())\n",
    "    device, dtype = a_tensor.device, a_tensor.dtype\n",
    "    self.gumbel_dist = torch.distributions.gumbel.Gumbel(torch.tensor(0., device=device, dtype=dtype), torch.tensor(1., device=device, dtype=dtype))\n",
    "\n",
    "  def forward(self, masked_inputs, is_mlm_applied, labels):\n",
    "    # masked_inp_ids: (B,L)\n",
    "    # ignored: (B,L)\n",
    "    \n",
    "    gen_logits = self.generator(masked_inputs) # (B, L, vocab size)\n",
    "\n",
    "    # add gumbel noise and then sample\n",
    "    pred_toks = (gen_logits + self.gumbel_dist.sample(gen_logits.shape)).argmax(dim=-1)\n",
    "    # use predicted token to fill 15%(mlm prob) mlm applied positions\n",
    "    generated = ~is_mlm_applied * masked_inputs + is_mlm_applied * pred_toks # (B,L)\n",
    "    # not equal to generator predicted and is at mlm applied position\n",
    "    is_replaced = (pred_toks != labels) * is_mlm_applied # (B, L)\n",
    "\n",
    "    disc_logits = self.discriminator(generated) # (B, L)\n",
    "\n",
    "    return gen_logits, generated, disc_logits, is_replaced\n",
    "\n",
    "  def gumbel_softmax(self, logits):\n",
    "    \"reimplement it cuz there is a bug in torch.nn.functional.gumbel_softmax when fp16 (https://github.com/pytorch/pytorch/issues/41663)\"\n",
    "    \"This is equal to the code of official ELECTRA repo. standard gumbel dist. = -ln(-ln(standard uniform dist.))\"\n",
    "    gumbel_noise = self.gumbel_dist.sample(logits.shape)\n",
    "    return F.softmax(logits + gumbel_noise, dim=-1)\n",
    "\n",
    "class ELECTRALoss():\n",
    "  def __init__(self, pad_idx, loss_weights=(1.0, 50.0), gen_label_smooth=False, disc_label_smooth=False):\n",
    "    self.pad_idx = pad_idx\n",
    "    self.loss_weights = loss_weights\n",
    "    if gen_label_smooth:\n",
    "      eps = gen_label_smooth if isinstance(gen_label_smooth, float) else 0.1\n",
    "      self.gen_loss_fc = LabelSmoothingCrossEntropyFlat(eps=eps)\n",
    "    else:\n",
    "      self.gen_loss_fc = CrossEntropyLossFlat()\n",
    "    self.disc_loss_fc = nn.BCEWithLogitsLoss()\n",
    "    self.disc_label_smooth = disc_label_smooth\n",
    "    \n",
    "  def __call__(self, pred, targ_ids):\n",
    "    gen_logits, generated, disc_logits, is_replaced = pred\n",
    "    gen_loss = self.gen_loss_fc(gen_logits.float(), targ_ids) # ignore position where targ_id==-100\n",
    "    if c.balanced_label:\n",
    "      non_mlm_pos = targ_ids == -100\n",
    "      non_pad = generated != self.pad_idx\n",
    "      rlm_mask = torch.full(non_pad.shape, c.mask_prob/(1-c.mask_prob), device=non_pad.device)\n",
    "      rlm_mask = rlm_mask * non_pad * non_mlm_pos\n",
    "      rlm_mask = (torch.bernoulli(rlm_mask).bool() + ~non_mlm_pos).bool()\n",
    "      disc_logits = disc_logits.masked_select(rlm_mask) # -> 1d tensor\n",
    "      is_replaced = is_replaced.masked_select(rlm_mask) # -> 1d tensor\n",
    "    else:\n",
    "      non_pad = generated != self.pad_idx\n",
    "      disc_logits = disc_logits.masked_select(non_pad) # -> 1d tensor\n",
    "      is_replaced = is_replaced.masked_select(non_pad) # -> 1d tensor\n",
    "    if self.disc_label_smooth:\n",
    "      eps = self.disc_label_smooth if isinstance(self.disc_label_smooth, float) else 0.1\n",
    "      zeros = ~is_replaced\n",
    "      is_replaced = is_replaced.float().masked_fill(zeros, eps)\n",
    "    disc_loss = self.disc_loss_fc(disc_logits.float(), is_replaced.float())\n",
    "    return gen_loss * self.loss_weights[0] + disc_loss * self.loss_weights[1]\n",
    "\n",
    "  def decodes(self, pred):\n",
    "    gen_logits, generated, disc_logits, is_replaced = pred\n",
    "    gen_pred = gen_logits.argmax(dim=-1)\n",
    "    disc_pred = disc_logits > 0\n",
    "    return gen_pred, generated, disc_pred, is_replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_warmup_and_decay(pct_now, lr_max, end_lr, decay_power, total_steps,warmup_pct=None, warmup_steps=None):\n",
    "  assert warmup_pct or warmup_steps\n",
    "  if warmup_steps: warmup_pct = warmup_steps/total_steps\n",
    "  \"\"\"\n",
    "  end_lr: the end learning rate for linear decay\n",
    "  warmup_pct: percentage of training steps to for linear increase\n",
    "  pct_now: percentage of traning steps we have gone through, notice pct_now=0.0 when calculating lr for first batch.\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  pct updated after_batch, but global_step (in tf) seems to update before optimizer step,\n",
    "  so pct is actually (global_step -1)/total_steps \n",
    "  \"\"\"\n",
    "  fixed_pct_now = pct_now + 1/total_steps\n",
    "  \"\"\"\n",
    "  According to source code of the official repository, it seems they merged two lr schedule (warmup and linear decay)\n",
    "  sequentially, instead of split training into two phases for each, this might because they think when in the early\n",
    "  phase of training, pct is low, and thus the decaying formula makes little difference to lr.\n",
    "  \"\"\"\n",
    "  decayed_lr = (lr_max-end_lr) * (1-fixed_pct_now)**decay_power + end_lr # https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/polynomial_decay\n",
    "  warmed_lr = decayed_lr * min(1.0, fixed_pct_now / warmup_pct) # https://github.com/google-research/electra/blob/81f7e5fc98b0ad8bfd20b641aa8bc9e6ac00c8eb/model/optimization.py#L44\n",
    "  return warmed_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_shedule = ParamScheduler({'lr': partial(linear_warmup_and_decay,\n",
    "                                            lr_max=c.lr,\n",
    "                                            end_lr=0.0,\n",
    "                                            decay_power=1,\n",
    "                                            warmup_steps=10000 if not c.percent else int(0.1 * c.percent * 10**6),\n",
    "                                            total_steps=c.steps if not c.percent else int(c.percent * c.steps))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now_time():\n",
    "  now_time = datetime.now(timezone(timedelta(hours=+8)))\n",
    "  name = str(now_time)[6:-13].replace(' ', '_').replace(':', '-')\n",
    "  return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "electra_model = ELECTRAModel(HF_Model(ElectraForMaskedLM, gen_config, hf_tokenizer, variable_sep=True), \n",
    "                             HF_Model(ElectraForPreTraining, disc_config, hf_tokenizer, variable_sep=True),\n",
    "                             hf_tokenizer.pad_token_id,)\n",
    "electra_loss_func = ELECTRALoss(pad_idx=hf_tokenizer.pad_token_id, gen_label_smooth=c.gen_smooth_label, disc_label_smooth=c.disc_smooth_label)\n",
    "\n",
    "dls.to(torch.device(c.device))\n",
    "run_name = now_time()\n",
    "print(run_name)\n",
    "learn = Learner(dls, electra_model,\n",
    "                loss_func=electra_loss_func,\n",
    "                opt_func=partial(Adam, eps=1e-6,),\n",
    "                path=str(c.checkpoint_dir),\n",
    "                model_dir='electra_pretrain',\n",
    "                cbs=[mlm_cb,\n",
    "                    RunSteps(c.steps, [0.0625, 0.125, 0.5, 1.0], run_name+\"_{percent}\") if not c.percent else RunSteps(int(c.steps*c.percent)),\n",
    "                    ],\n",
    "                )\n",
    "if c.use_fp16: learn = learn.to_fp16()\n",
    "learn.fit(9999, cbs=[lr_shedule])\n",
    "if c.percent: learn.save(run_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}