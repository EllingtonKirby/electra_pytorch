{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "from IPython.core.debugger import set_trace as bk\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import nlp\n",
    "from transformers import ElectraModel, ElectraConfig, ElectraTokenizerFast, ElectraForPreTraining\n",
    "from fastai2.text.all import *\n",
    "import wandb\n",
    "from fastai2.callback.wandb import WandbCallback\n",
    "from _utils.would_like_to_pr import *\n",
    "from _utils.huggingface import *\n",
    "from _utils.wsc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 'small'\n",
    "assert SIZE in ['small', 'base', 'large']\n",
    "I = ['small', 'base', 'large'].index(SIZE)\n",
    "CONFIG = {\n",
    "  'mask_prob':[0.15, 0.15, 0.25],\n",
    "  'lr':[5e-4, 2e-4, 2e-4],\n",
    "  'bs':[128, 256, 2048],\n",
    "  'steps':[10**6, 766*1000, 400*1000],\n",
    "  'max_length':[128, 512, 512],\n",
    "}\n",
    "config = {k:vs[I] for k,vs in CONFIG.items()}\n",
    "config.update({\n",
    "  'use_fp16': True,\n",
    "  'sort_sample': True,\n",
    "  'smooth_label': True,\n",
    "  'shuffle': True,\n",
    "})\n",
    "print(config)\n",
    "\n",
    "model_config = ElectraConfig.from_pretrained(f'google/electra-{SIZE}-discriminator')\n",
    "hf_tokenizer = ElectraTokenizerFast.from_pretrained(f\"google/electra-{SIZE}-generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir=Path.home()/\"datasets\"\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "if SIZE in ['small', 'base']:\n",
    "  wiki_cache_dir = cache_dir/\"wikipedia/20200501.en/1.0.0\"\n",
    "  book_cache_dir = cache_dir/\"bookcorpus/plain_text/1.0.0\"\n",
    "  wbdl_cache_dir = cache_dir/\"wikibook_dl\"\n",
    "  wbdl_cache_dir.mkdir(exist_ok=True)\n",
    "max_length = config['max_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cache_dir.exists():\n",
    "  print('create cache direcotry')\n",
    "  cache_dir.mkdir(parents=True) # create all parents needed\n",
    "\n",
    "if SIZE in ['small', 'base']:\n",
    "  \n",
    "  # wiki\n",
    "  if (wiki_cache_dir/f\"wiki_electra_{max_length}.arrow\").exists():\n",
    "    print('loading the electra data (wiki)')\n",
    "    wiki = nlp.Dataset.from_file(str(wiki_cache_dir/f\"wiki_electra_{max_length}.arrow\"))\n",
    "  else:\n",
    "    print('load/download wiki dataset')\n",
    "    wiki = nlp.load_dataset('wikipedia', '20200501.en', cache_dir=cache_dir)['train']\n",
    "  \n",
    "    print('load/make tokenized wiki dataset')\n",
    "    wiki = HF_TokenizeTfm(wiki, cols={'text':'tokids'}, hf_tokenizer=hf_tokenizer, remove_original=True).map(cache_file_name=str(wiki_cache_dir/'wiki_tokenized.arrow'))\n",
    "  \n",
    "    print('creat data from wiki dataset for ELECTRA')\n",
    "    wiki = ELECTRADataTransform(wiki, in_col='tokids', out_col='inpput_ids', max_length=max_length, cls_idx=hf_tokenizer.cls_token_id, sep_idx=hf_tokenizer.sep_token_id).map(cache_file_name=str(wiki_cache_dir/f\"wiki_electra_{max_length}.arrow\"))\n",
    "\n",
    "  # bookcorpus\n",
    "  if (book_cache_dir/f\"book_electra_{max_length}.arrow\").exists():\n",
    "    print('loading the electra data (BookCorpus)')\n",
    "    book = nlp.Dataset.from_file(str(book_cache_dir/f\"book_electra_{max_length}.arrow\"))\n",
    "  else:\n",
    "    print('load/download BookCorpus dataset')\n",
    "    book = nlp.load_dataset('/home/yisiang/nlp/datasets/bookcorpus/bookcorpus.py', cache_dir=cache_dir)['train']\n",
    " \n",
    "    print('load/make tokenized BookCorpus dataset')\n",
    "    book = HF_TokenizeTfm(book, cols={'text':'tokids'}, hf_tokenizer=hf_tokenizer, remove_original=True).map(cache_file_name=str(book_cache_dir/'book_tokenized.arrow'))\n",
    "  \n",
    "    print('creat data from BookCorpus dataset for ELECTRA')\n",
    "    book = ELECTRADataTransform(book, in_col='tokids', out_col='inpput_ids', max_length=max_length, cls_idx=hf_tokenizer.cls_token_id, sep_idx=hf_tokenizer.sep_token_id).map(cache_file_name=str(book_cache_dir/f\"book_electra_{max_length}.arrow\"))\n",
    "\n",
    "  wb_data = HF_MergedDataset(wiki, book)\n",
    "  wb_dsets = HF_Datasets({'train': wb_data}, cols=['inpput_ids'], hf_toker=hf_tokenizer)\n",
    "  dls = wb_dsets.dataloaders(bs=config['bs'],pad_idx=hf_tokenizer.pad_token_id, \n",
    "                             shuffle_train=config['shuffle'], drop_last=False, \n",
    "                             srtkey_fc=None if config['sort_sample'] else False, \n",
    "                             cache_dir=Path.home()/'datasets/wikibook_dl', cache_name='dl_{split}.json')\n",
    "\n",
    "else: # for large size\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Masked language model objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLM objective callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modified from HuggingFace/transformers (https://github.com/huggingface/transformers/blob/0a3d0e02c5af20bfe9091038c4fd11fb79175546/src/transformers/data/data_collator.py#L102). It is\n",
    "- few ms faster: intead of a[b] a on gpu b on cpu, tensors here are all in the same device\n",
    "- few tens of us faster: in how we create special token mask\n",
    "- doesn't require huggingface tokenizer\n",
    "- cost you only 20 ms on a (128,128) tensor, so dynamic masking is cheap   \n",
    "\"\"\"\n",
    "# https://github.com/huggingface/transformers/blob/1789c7daf1b8013006b0aef6cb1b8f80573031c5/examples/run_language_modeling.py#L179\n",
    "def mask_tokens(inputs, mask_token_index, vocab_size, special_token_indices, mlm_probability=0.15, ignore_index=-100):\n",
    "  \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
    "  \"ignore_index in nn.CrossEntropy is default to -100, so you don't need to specify ignore_index in loss\"\n",
    "  \n",
    "  device = inputs.device\n",
    "  labels = inputs.clone()\n",
    "  # We sample a few tokens in each sequence for masked-LM training (with probability mlm_probability defaults to 0.15 in Bert/RoBER\n",
    "  probability_matrix = torch.full(labels.shape, mlm_probability, device=device)\n",
    "  special_tokens_mask = torch.full(inputs.shape, False, dtype=torch.bool, device=device)\n",
    "  for sp_id in special_token_indices:\n",
    "    special_tokens_mask = special_tokens_mask | (inputs==sp_id)\n",
    "  probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "  mlm_mask = torch.bernoulli(probability_matrix).bool()\n",
    "  labels[~mlm_mask] = ignore_index  # We only compute loss on masked tokens\n",
    "\n",
    "  # 80% of the time, we replace masked input tokens with mask_token\n",
    "  mask_token_mask = torch.bernoulli(torch.full(labels.shape, 0.8, device=device)).bool() & mlm_mask\n",
    "  inputs[mask_token_mask] = mask_token_index\n",
    "\n",
    "  # 10% of the time, we replace masked input tokens with random word\n",
    "  replace_token_mask = torch.bernoulli(torch.full(labels.shape, 0.5, device=device)).bool() & mlm_mask & ~mask_token_mask\n",
    "  random_words = torch.randint(vocab_size, labels.shape, dtype=torch.long, device=device)\n",
    "  inputs[replace_token_mask] = random_words[replace_token_mask]\n",
    "\n",
    "  # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "  return inputs, labels, ~mlm_mask\n",
    "\n",
    "class MaskedLMCallback(Callback):\n",
    "  @delegates(mask_tokens)\n",
    "  def __init__(self, mask_tok_id, special_tok_ids, vocab_size, ignore_index=-100, output_ignore_mask=False, **kwargs):\n",
    "    self.ignore_index = ignore_index\n",
    "    self.output_ignore_mask = output_ignore_mask\n",
    "    self.mask_tokens = partial(mask_tokens,\n",
    "                               mask_token_index=mask_tok_id,\n",
    "                               special_token_indices=special_tok_ids,\n",
    "                               vocab_size=vocab_size,\n",
    "                               ignore_index=-100,\n",
    "                               **kwargs)\n",
    "\n",
    "  def begin_batch(self):\n",
    "    text_indices = self.xb[0]\n",
    "    masked_inputs, labels, ignored = self.mask_tokens(text_indices)\n",
    "    if self.output_ignore_mask:\n",
    "      self.learn.xb, self.learn.yb = (masked_inputs, ignored), (labels,)\n",
    "    else:\n",
    "      self.learn.xb, self.learn.yb = (masked_inputs,), (labels,)\n",
    "\n",
    "  def mask(self, tokids):\n",
    "    # a function could be used w/o learner\n",
    "    return self.mask_tokens(tokids)\n",
    "\n",
    "  @delegates(TfmdDL.show_batch)\n",
    "  def show_batch(self, dl, verbose=True, show_ignore_idx=None, **kwargs):\n",
    "    b = dl.one_batch()\n",
    "    masked_inputs, labels, ignored = self.mask_tokens(b[0])\n",
    "    if show_ignore_idx:\n",
    "      labels[labels==self.ignore_index] = show_ignore_idx\n",
    "    if verbose: \n",
    "      print(\"We won't count loss from position where y is ignore index\")\n",
    "      print(\"Notice 1. Positions have label token in y will be either [Mask]/other token/orginal token in x\")\n",
    "      print(\"Notice 2. Special tokens (CLS, SEP) won't be masked.\")\n",
    "      print(\"Notice 3. Dynamic masking: every time you run gives you different results.\")\n",
    "    dl.show_batch(b=(masked_inputs, labels), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_cb = MaskedLMCallback(mask_tok_id=hf_tokenizer.mask_token_id, \n",
    "                          special_tok_ids=hf_tokenizer.all_special_ids, \n",
    "                          vocab_size=hf_tokenizer.vocab_size,\n",
    "                          mlm_probability=config['mask_prob'],\n",
    "                          output_ignore_mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ELECTRA (replaced token detection objective)\n",
    "see details in paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELECTRAModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, generator, discriminator, pad_idx):\n",
    "    super().__init__()\n",
    "    self.generator, self.discriminator = generator,discriminator\n",
    "    self.pad_idx = pad_idx\n",
    "\n",
    "  def forward(self, masked_inp_ids, ignored):\n",
    "    # masked_inp_ids: (B,L)\n",
    "    # ignored: (B,L)\n",
    "\n",
    "    non_pad = masked_inp_ids != self.pad_idx\n",
    "    gen_logits = self.generator(masked_inp_ids) # (B, L, vocab size)\n",
    "\n",
    "    # tokens output by generator\n",
    "    pred_toks = gen_logits.argmax(dim=-1) # (B, L)\n",
    "    # use predicted token to fill 15%(mlm prob) positions\n",
    "    generated = ignored * masked_inp_ids + ~ignored * pred_toks # (B,L)\n",
    "    # is masked token and not equal to predicted\n",
    "    is_replaced = (generated != masked_inp_ids) # (B, L)\n",
    "    \n",
    "    disc_logits = self.discriminator(generated) # (B, L)\n",
    "\n",
    "    return gen_logits, disc_logits, is_replaced, non_pad\n",
    "\n",
    "class ELECTRALoss():\n",
    "  def __init__(self, pad_idx, loss_weights=(1.0, 50.0), label_smooth=False):\n",
    "    self.pad_idx = pad_idx\n",
    "    self.loss_weights = loss_weights\n",
    "    self.gen_loss_fc = LabelSmoothingCrossEntropyFlat() if label_smooth else CrossEntropyLossFlat()\n",
    "    self.disc_loss_fc = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "  def __call__(self, pred, targ_ids):\n",
    "    gen_logits, disc_logits, is_replaced = [t.to(dtype=torch.float) for t in pred[:-1]]\n",
    "    non_pad = pred[-1]\n",
    "    gen_loss = self.gen_loss_fc(gen_logits, targ_ids) # ignore position where targ_id==-100\n",
    "    disc_logits = disc_logits.masked_select(non_pad) # 1d tensor\n",
    "    is_replaced = is_replaced.masked_select(non_pad) # 1d tensor\n",
    "    disc_loss = self.disc_loss_fc(disc_logits, is_replaced)\n",
    "    return gen_loss * self.loss_weights[0] + disc_loss * self.loss_weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_warmup_and_decay(pct_now, lr_max, end_lr, decay_power, total_steps,warmup_pct=None, warmup_steps=None):\n",
    "  assert warmup_pct or warmup_steps\n",
    "  if warmup_steps: warmup_pct = warmup_steps/total_steps\n",
    "  \"\"\"\n",
    "  end_lr: the end learning rate for linear decay\n",
    "  warmup_pct: percentage of training steps to for linear increase\n",
    "  pct_now: percentage of traning steps we have gone through, notice pct_now=0.0 when calculating lr for first batch.\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  pct updated after_batch, but global_step (in tf) seems to update before optimizer step,\n",
    "  so pct is actually (global_step -1)/total_steps \n",
    "  \"\"\"\n",
    "  fixed_pct_now = pct_now + 1/total_steps\n",
    "  \"\"\"\n",
    "  According to source code of the official repository, it seems they merged two lr schedule (warmup and linear decay)\n",
    "  sequentially, instead of split training into two phases for each, this might because they think when in the early\n",
    "  phase of training, pct is low, and thus the decaying formula makes little difference to lr.\n",
    "  \"\"\"\n",
    "  decayed_lr = (lr_max-end_lr) * (1-fixed_pct_now)**decay_power + end_lr # https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/polynomial_decay\n",
    "  warmed_lr = decayed_lr * min(1.0, fixed_pct_now / warmup_pct) # https://github.com/google-research/electra/blob/81f7e5fc98b0ad8bfd20b641aa8bc9e6ac00c8eb/model/optimization.py#L44\n",
    "  return warmed_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_shedule = ParamScheduler({'lr': partial(linear_warmup_and_decay,\n",
    "                                            lr_max=config['lr'],\n",
    "                                            end_lr=0.0,\n",
    "                                            decay_power=1,\n",
    "                                            warmup_steps=10000,\n",
    "                                            total_steps=config['steps'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_dir=Path.home()/'checkpoints'\n",
    "cb_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def now_time():\n",
    "  now_time = datetime.now(timezone(timedelta(hours=+8)))\n",
    "  name = str(now_time)[6:-13].replace(' ', '_').replace(':', '-')\n",
    "  return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_model = ELECTRAModel(HF_Model(ElectraForMaskedLM, model_config, hf_tokenizer,variable_sep=True), \n",
    "                             HF_Model(ElectraForPreTraining, model_config, hf_tokenizer,variable_sep=True),\n",
    "                             hf_tokenizer.mask_token_id,)\n",
    "electra_loss_func = ELECTRALoss(pad_idx=hf_tokenizer.pad_token_id, label_smooth=config['smooth_label']) # label smooth applied only on loss of generator\n",
    "\n",
    "dls.to(torch.device('cuda:2'))\n",
    "run_name = now_time()\n",
    "print(run_name)\n",
    "learn = Learner(dls, electra_model,\n",
    "                loss_func=electra_loss_func,\n",
    "                opt_func=partial(Adam, eps=1e-6,),\n",
    "                path=str(cb_dir),\n",
    "                model_dir='electra_pretrain',\n",
    "                cbs=[mlm_cb,\n",
    "                    RunSteps(config['steps'], [0.5, 1.0], run_name+\"_{percent}\"),\n",
    "                    ],\n",
    "                )\n",
    "if config['use_fp16']: learn = learn.to_fp16()\n",
    "learn.fit(9999, cbs=[lr_shedule])\n",
    "learn.save(run_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}