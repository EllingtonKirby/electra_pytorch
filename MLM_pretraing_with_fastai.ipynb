{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from functools import partial\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from IPython.core.debugger import set_trace as bk\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import torch.tensor as T\n",
    "import nlp\n",
    "from transformers import ElectraModel, ElectraConfig, ElectraTokenizerFast, ElectraForMaskedLM,ElectraForPreTraining\n",
    "from fastai2.text.all import *\n",
    "import wandb\n",
    "from fastai2.callback.wandb import WandbCallback\n",
    "from _utils.would_like_to_pr import *\n",
    "from _utils.huggingface import *\n",
    "from _utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = MyConfig({\n",
    "    'device': 'cuda:3',\n",
    "    'size': 'small',\n",
    "    'sampling': 'fp32_gumbel',\n",
    "    'electra_mask_style': True,\n",
    "    'gen_smooth_label': False,\n",
    "    'disc_smooth_label': False,\n",
    "    'sort_sample': True,\n",
    "    'shuffle': True,\n",
    "})\n",
    "\n",
    "i = ['small', 'base', 'large'].index(c.size)\n",
    "c.mask_prob = [0.15, 0.15, 0.25][i]\n",
    "c.lr = [5e-4, 2e-4, 2e-4][i]\n",
    "c.bs = [128, 256, 2048][i]\n",
    "c.steps = [10**6, 766*1000, 400*1000][i]\n",
    "c.max_length = [128, 512, 512][i]\n",
    "generator_size_divisor = [4, 3, 4][i]\n",
    "disc_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-discriminator')\n",
    "gen_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-generator')\n",
    "# note that public electra-small model is actually small++ and don't scale down generator size \n",
    "gen_config.hidden_size = int(disc_config.hidden_size/generator_size_divisor)\n",
    "gen_config.num_attention_heads = int(disc_config.num_attention_heads/generator_size_divisor)\n",
    "gen_config.intermediate_size = int(disc_config.intermediate_size/generator_size_divisor)\n",
    "hf_tokenizer = ElectraTokenizerFast.from_pretrained(f\"google/electra-{c.size}-generator\")\n",
    "# check\n",
    "assert c.sampling in ['fp32_gumbel', 'fp16_gumbel', 'multinomial']\n",
    "# path to data\n",
    "Path('./checkpoints/pretrain').mkdir(exist_ok=True,parents=True)\n",
    "if c.size in ['small', 'base']:\n",
    "  wiki_cache_dir = Path(\"./datasets/wikipedia/20200501.en/1.0.0\")\n",
    "  book_cache_dir = Path(\"./datasets/bookcorpus/plain_text/1.0.0\")\n",
    "  wbdl_cache_dir = Path(\"./datasets/wikibook_dl\")\n",
    "  wbdl_cache_dir.mkdir(exist_ok=True)\n",
    "# print info\n",
    "print(\"process id: {os.getpid()}\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if c.size in ['small', 'base']:\n",
    "  \n",
    "  # wiki\n",
    "  if (wiki_cache_dir/f\"wiki_electra_{c.max_length}.arrow\").exists():\n",
    "    print('loading the electra data (wiki)')\n",
    "    wiki = nlp.Dataset.from_file(str(wiki_cache_dir/f\"wiki_electra_{c.max_length}.arrow\"))\n",
    "  else:\n",
    "    print('load/download wiki dataset')\n",
    "    wiki = nlp.load_dataset('wikipedia', '20200501.en', cache_dir='./datasets')['train']\n",
    "  \n",
    "    print('creat data from wiki dataset for ELECTRA')\n",
    "    wiki = ELECTRADataTransform(wiki, is_docs=True, text_col={'text':'input_ids'}, max_length=c.max_length, hf_toker=hf_tokenizer).map(cache_file_name=str(wiki_cache_dir/f\"wiki_electra_{c.max_length}.arrow\"))\n",
    "\n",
    "  # bookcorpus\n",
    "  if (book_cache_dir/f\"book_electra_{c.max_length}.arrow\").exists():\n",
    "    print('loading the electra data (BookCorpus)')\n",
    "    book = nlp.Dataset.from_file(str(book_cache_dir/f\"book_electra_{c.max_length}.arrow\"))\n",
    "  else:\n",
    "    print('load/download BookCorpus dataset')\n",
    "    book = nlp.load_dataset('bookcorpus', cache_dir='./datasets')['train']\n",
    "  \n",
    "    print('creat data from BookCorpus dataset for ELECTRA')\n",
    "    book = ELECTRADataTransform(book, is_docs=False, text_col={'text':'input_ids'}, max_length=c.max_length, hf_toker=hf_tokenizer).map(cache_file_name=str(book_cache_dir/f\"book_electra_{c.max_length}.arrow\"))\n",
    "\n",
    "  wb_data = HF_MergedDataset(wiki, book)\n",
    "  wb_dsets = HF_Datasets({'train': wb_data}, cols=['input_ids'], hf_toker=hf_tokenizer)\n",
    "  dls = wb_dsets.dataloaders(bs=c.bs, \n",
    "                             shuffle_train=c.shuffle,\n",
    "                             srtkey_fc=None if c.sort_sample else False, \n",
    "                             cache_dir=Path.home()/'datasets/wikibook_dl', cache_name='dl_{split}.json')\n",
    "\n",
    "else: # for large size\n",
    "  raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Masked language model objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLM objective callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modified from HuggingFace/transformers (https://github.com/huggingface/transformers/blob/0a3d0e02c5af20bfe9091038c4fd11fb79175546/src/transformers/data/data_collator.py#L102). It is\n",
    "- few ms faster: intead of a[b] a on gpu b on cpu, tensors here are all in the same device\n",
    "- few tens of us faster: in how we create special token mask\n",
    "- doesn't require huggingface tokenizer\n",
    "- cost you only 20 ms on a (128,128) tensor, so dynamic masking is cheap   \n",
    "\"\"\"\n",
    "def mask_tokens(inputs, mask_token_index, vocab_size, special_token_indices, mlm_probability=0.15, replace_prob=0.1, orginal_prob=0.1, ignore_index=-100):\n",
    "  \"\"\" \n",
    "  Prepare masked tokens inputs/labels for masked language modeling: (1-replace_prob-orginal_prob)% MASK, replace_prob% random, orginal_prob% original within mlm_probability% of tokens in the sentence. \n",
    "  - ignore_index in nn.CrossEntropy is default to -100, so you don't need to specify ignore_index in loss\n",
    "  \"\"\"\n",
    "  \n",
    "  device = inputs.device\n",
    "  labels = inputs.clone()\n",
    "  # We sample a few tokens in each sequence for masked-LM training (with probability mlm_probability defaults to 0.15 in Bert/RoBERTA\n",
    "  probability_matrix = torch.full(labels.shape, mlm_probability, device=device)\n",
    "  special_tokens_mask = torch.full(inputs.shape, False, dtype=torch.bool, device=device)\n",
    "  for sp_id in special_token_indices:\n",
    "    special_tokens_mask = special_tokens_mask | (inputs==sp_id)\n",
    "  probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "  mlm_mask = torch.bernoulli(probability_matrix).bool()\n",
    "  labels[~mlm_mask] = ignore_index  # We only compute loss on masked tokens\n",
    "\n",
    "  # <1 - replace_prob - orginal_prob>% of the time, we replace masked input tokens with mask_token\n",
    "  mask_prob = 1 - replace_prob - orginal_prob\n",
    "  mask_token_mask = torch.bernoulli(torch.full(labels.shape, 0.8, device=device)).bool() & mlm_mask\n",
    "  inputs[mask_token_mask] = mask_token_index\n",
    "\n",
    "  # <replace_prob>% of the time, we replace masked input tokens with random word\n",
    "  if int(replace_prob)!=0:\n",
    "    rep_prob = replace_prob/(replace_prob + orginal_prob)\n",
    "    replace_token_mask = torch.bernoulli(torch.full(labels.shape, 0.5, device=device)).bool() & mlm_mask & ~mask_token_mask\n",
    "    random_words = torch.randint(vocab_size, labels.shape, dtype=torch.long, device=device)\n",
    "    inputs[replace_token_mask] = random_words[replace_token_mask]\n",
    "\n",
    "  # <orginal_prob>% of the time, we keep the masked input tokens unchanged\n",
    "  return inputs, labels, mlm_mask\n",
    "\n",
    "class MaskedLMCallback(Callback):\n",
    "  @delegates(mask_tokens)\n",
    "  def __init__(self, mask_tok_id, special_tok_ids, vocab_size, ignore_index=-100, for_electra=False, **kwargs):\n",
    "    self.ignore_index = ignore_index\n",
    "    self.for_electra = for_electra\n",
    "    self.mask_tokens = partial(mask_tokens,\n",
    "                               mask_token_index=mask_tok_id,\n",
    "                               special_token_indices=special_tok_ids,\n",
    "                               vocab_size=vocab_size,\n",
    "                               ignore_index=-100,\n",
    "                               **kwargs)\n",
    "\n",
    "  def begin_batch(self):\n",
    "    text_indices = self.xb[0]\n",
    "    masked_inputs, labels, is_mlm_applied = self.mask_tokens(text_indices)\n",
    "    if self.for_electra:\n",
    "      self.learn.xb, self.learn.yb = (masked_inputs, is_mlm_applied, labels), (labels,)\n",
    "    else:\n",
    "      self.learn.xb, self.learn.yb = (masked_inputs,), (labels,)\n",
    "\n",
    "  @delegates(TfmdDL.show_batch)\n",
    "  def show_batch(self, dl, idx_show_ignored, verbose=True, **kwargs):\n",
    "    b = dl.one_batch()\n",
    "    inputs = b[0]\n",
    "    masked_inputs, labels, is_mlm_applied = self.mask_tokens(inputs.clone())\n",
    "    # check\n",
    "    assert torch.equal(is_mlm_applied, labels!=self.ignore_index)\n",
    "    assert torch.equal((~is_mlm_applied *masked_inputs + is_mlm_applied * labels), inputs)\n",
    "    # change symbol to show the ignored position\n",
    "    labels[labels==self.ignore_index] = idx_show_ignored\n",
    "    # some notice to help understand the masking mechanism\n",
    "    if verbose: \n",
    "      print(\"We won't count loss from position where y is ignore index\")\n",
    "      print(\"Notice 1. Positions have label token in y will be either [Mask]/other token/orginal token in x\")\n",
    "      print(\"Notice 2. Special tokens (CLS, SEP) won't be masked.\")\n",
    "      print(\"Notice 3. Dynamic masking: every time you run gives you different results.\")\n",
    "    # show\n",
    "    tfm_b =(masked_inputs, is_mlm_applied, labels, labels) if self.for_electra else (masked_inputs, labels)   \n",
    "    dl.show_batch(b=tfm_b, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_cb = MaskedLMCallback(mask_tok_id=hf_tokenizer.mask_token_id, \n",
    "                          special_tok_ids=hf_tokenizer.all_special_ids, \n",
    "                          vocab_size=hf_tokenizer.vocab_size,\n",
    "                          mlm_probability=c.mask_prob,\n",
    "                          replace_prob=0.0 if c.electra_mask_style else 0.1, \n",
    "                          orginal_prob=0.15 if c.electra_mask_style else 0.1,\n",
    "                          for_electra=True)\n",
    "# mlm_cb.show_batch(dls[0], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ELECTRA (replaced token detection objective)\n",
    "see details in paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELECTRAModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, generator, discriminator):\n",
    "    super().__init__()\n",
    "    self.generator, self.discriminator = generator,discriminator\n",
    "    self.gumbel_dist = torch.distributions.gumbel.Gumbel(0.,1.)\n",
    "    self.toker = hf_tokenizer\n",
    "    # tight embeddings (word, pos, token_type)\n",
    "    # Note input and output embedding of generator has been tighted by huggingface/transformers \n",
    "    self.discriminator.model.electra.embeddings = self.generator.model.electra.embeddings\n",
    "\n",
    "  def to(self, *args, **kwargs):\n",
    "    super().to(*args, **kwargs)\n",
    "    a_tensor = next(self.parameters())\n",
    "    device, dtype = a_tensor.device, a_tensor.dtype\n",
    "    if c.sampling=='fp32_gumbel': dtype = torch.float32\n",
    "    self.gumbel_dist = torch.distributions.gumbel.Gumbel(torch.tensor(0., device=device, dtype=dtype), torch.tensor(1., device=device, dtype=dtype))\n",
    "\n",
    "  def forward(self, masked_inputs, is_mlm_applied, labels):\n",
    "    # masked_inp_ids: (B,L)\n",
    "    # ignored: (B,L)\n",
    "    #assert is_mlm_applied.dtype == torch.bool\n",
    "    \n",
    "    gen_logits = self.generator(masked_inputs) # (B, L, vocab size)\n",
    "    \n",
    "    # reduce size to speed up and use\n",
    "    mlm_gen_logits = gen_logits[is_mlm_applied, :].detach() # ( #mlm_positions, vocab_size)\n",
    "    # sampling\n",
    "    pred_toks = self.sample(mlm_gen_logits)\n",
    "    # pred_toks: ( #mlm_positions, )\n",
    "    # use predicted token to fill 15%(mlm prob) mlm applied positions\n",
    "    generated = masked_inputs.clone() # (B,L)\n",
    "    generated[is_mlm_applied] = pred_toks # (B,L)\n",
    "    # not equal to generator predicted and is at mlm applied position\n",
    "    is_replaced = is_mlm_applied.clone() # (B,L)\n",
    "    is_replaced[is_mlm_applied] = (pred_toks != labels[is_mlm_applied]) # (B,L)\n",
    "\n",
    "    disc_logits = self.discriminator(generated) # (B, L)\n",
    "\n",
    "    return gen_logits, generated, disc_logits, is_replaced\n",
    "\n",
    "  def sample(self, logits):\n",
    "    \"reimplement it cuz there is a bug in torch.nn.functional.gumbel_softmax when fp16 (https://github.com/pytorch/pytorch/issues/41663)\"\n",
    "    \"This is equal to the code of official ELECTRA repo. standard gumbel dist. = -ln(-ln(standard uniform dist.))\"\n",
    "    if c.sampling == 'fp32_gumbel':\n",
    "      return (logits.float() + self.gumbel_dist.sample(logits.shape)).argmax(dim=-1)\n",
    "    elif c.sampling == 'fp16_gumbel': # 5.06 ms\n",
    "      return (logits + self.gumbel_dist.sample(logits.shape)).argmax(dim=-1)\n",
    "    elif c.sampling == 'multinomial':\n",
    "      return torch.multinomial(F.softmax(logits, dim=-1), 1).squeeze()\n",
    "\n",
    "class ELECTRALoss():\n",
    "  def __init__(self, pad_idx, loss_weights=(1.0, 50.0), gen_label_smooth=False, disc_label_smooth=False):\n",
    "    self.pad_idx = pad_idx\n",
    "    self.loss_weights = loss_weights\n",
    "    if gen_label_smooth:\n",
    "      eps = gen_label_smooth if isinstance(gen_label_smooth, float) else 0.1\n",
    "      self.gen_loss_fc = LabelSmoothingCrossEntropyFlat(eps=eps)\n",
    "    else:\n",
    "      self.gen_loss_fc = CrossEntropyLossFlat()\n",
    "    self.disc_loss_fc = nn.BCEWithLogitsLoss()\n",
    "    self.disc_label_smooth = disc_label_smooth\n",
    "    \n",
    "  def __call__(self, pred, targ_ids):\n",
    "    gen_logits, generated, disc_logits, is_replaced = pred\n",
    "    gen_loss = self.gen_loss_fc(gen_logits.float(), targ_ids) # ignore position where targ_id==-100\n",
    "    non_pad = generated != self.pad_idx\n",
    "    disc_logits = disc_logits.masked_select(non_pad) # -> 1d tensor\n",
    "    is_replaced = is_replaced.masked_select(non_pad) # -> 1d tensor\n",
    "    if self.disc_label_smooth:\n",
    "      eps = self.disc_label_smooth if isinstance(self.disc_label_smooth, float) else 0.1\n",
    "      zeros = ~is_replaced\n",
    "      is_replaced = is_replaced.float().masked_fill(zeros, eps)\n",
    "    disc_loss = self.disc_loss_fc(disc_logits.float(), is_replaced.float())\n",
    "    return gen_loss * self.loss_weights[0] + disc_loss * self.loss_weights[1]\n",
    "\n",
    "  def decodes(self, pred):\n",
    "    gen_logits, generated, disc_logits, is_replaced = pred\n",
    "    gen_pred = gen_logits.argmax(dim=-1)\n",
    "    disc_pred = disc_logits > 0\n",
    "    return gen_pred, generated, disc_pred, is_replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_warmup_and_decay(pct, lr_max, warmup_steps, total_steps, fake_total_steps=None, end_lr=0.0, decay_power=1):\n",
    "  \"\"\" pct (float): fastai count it as ith_step/num_epoch*len(dl), so we can't just use pct when our num_epoch is fake.he ith_step is count from 0, \"\"\"\n",
    "  step_i = round(pct * (fake_total_steps if fake_total_steps else total_steps)) + 1 # fastai count step from 0, so we have to add 1 back\n",
    "  # According to the original source code, two schedules take effect at the same time, but decaying schedule will be neglible in the early time.\n",
    "  decayed_lr = (lr_max-end_lr) * (1 - step_i/total_steps) ** decay_power + end_lr # https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/polynomial_decay\n",
    "  warmed_lr = decayed_lr * min(1.0, step_i/warmup_steps) # https://github.com/google-research/electra/blob/81f7e5fc98b0ad8bfd20b641aa8bc9e6ac00c8eb/model/optimization.py#L44\n",
    "  return warmed_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_shedule = ParamScheduler({'lr': partial(linear_warmup_and_decay,\n",
    "                                            lr_max=c.lr,\n",
    "                                            warmup_steps=10000,\n",
    "                                            total_steps=c.steps,\n",
    "                                            fake_total_steps=len(dls[0])*9999)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClipping(Callback):\n",
    "    \"Gradient clipping during training.\"\n",
    "    def __init__(self, clip:float = 0.):\n",
    "        self.clip = clip\n",
    "    def after_backward(self):\n",
    "        \"Clip the gradient before the optimizer step.\"\n",
    "        if self.clip: nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now_time():\n",
    "  now_time = datetime.now(timezone(timedelta(hours=+8)))\n",
    "  name = str(now_time)[6:-13].replace(' ', '_').replace(':', '-')\n",
    "  return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = HF_Model(ElectraForMaskedLM, gen_config, hf_tokenizer, variable_sep=True)\n",
    "discriminator = HF_Model(ElectraForPreTraining, disc_config, hf_tokenizer, variable_sep=True)\n",
    "electra_model = ELECTRAModel(generator, discriminator)\n",
    "electra_loss_func = ELECTRALoss(pad_idx=hf_tokenizer.pad_token_id, gen_label_smooth=c.gen_smooth_label, disc_label_smooth=c.disc_smooth_label)\n",
    "\n",
    "dls.to(torch.device(c.device))\n",
    "run_name = now_time()\n",
    "print(run_name)\n",
    "learn = Learner(dls, electra_model,\n",
    "                loss_func=electra_loss_func,\n",
    "                opt_func=partial(Adam, eps=1e-6,),\n",
    "                path='./checkpoints',\n",
    "                model_dir='pretrain',\n",
    "                cbs=[mlm_cb,\n",
    "                    RunSteps(c.steps, [0.0625, 0.125, 0.5, 1.0], run_name+\"_{percent}\"),\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "if c.device.startswith('cuda'): \n",
    "  # too large loss_scale will cause overflow and lose the batch, too small loss_scale will cause underflow or can't fully use the value of loss\n",
    "  # adjust initial loss scale and interval to re scale up loss scale cuz we have scale disc loss by 50 and we have so many training steps \n",
    "  learn = learn.to_fp16(max_loss_scale=2.**11, scale_wait=int(c.steps*0.01), clip=1.)\n",
    "else: \n",
    "  learn.add_cb(GradientClipping(1.))\n",
    "# to also exclude layernorm\n",
    "learn.create_opt()\n",
    "for p in my_bn_bias_state(learn, True): p['do_wd'] = False \n",
    "# ----------------------------\n",
    "learn.fit(9999, cbs=[lr_shedule])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}