{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from functools import partial\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from IPython.core.debugger import set_trace as bk\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import torch.tensor as T\n",
    "import nlp\n",
    "from transformers import ElectraModel, ElectraConfig, ElectraTokenizerFast, ElectraForMaskedLM,ElectraForPreTraining\n",
    "from fastai2.text.all import *\n",
    "import wandb\n",
    "from fastai2.callback.wandb import WandbCallback\n",
    "from _utils.would_like_to_pr import *\n",
    "from _utils.huggingface import *\n",
    "from _utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "170563\n{'device': 'cuda:0', 'size': 'small', 'use_fp16': True, 'gumbel': True, 'fp32_gumbel': True, 'invert_label': False, 'mc': False, 'mask': 'electra', 'gen_smooth_label': False, 'disc_smooth_label': False, 'balanced_label': False, 'load_smallplusplus': False, 'tfdata': False, 'sort_sample': False, 'shuffle': True, 'percent': False, 'cache_dir': Path('/home/yisiang/datasets'), 'checkpoint_dir': Path('/home/yisiang/checkpoints'), 'mask_prob': 0.15, 'lr': 0.0005, 'bs': 128, 'steps': 1000000, 'max_length': 128}\n"
    }
   ],
   "source": [
    "c = MyConfig({\n",
    "    'device': 'cuda:0',\n",
    "    'size': 'small',\n",
    "    'use_fp16': True,\n",
    "    'gumbel': True,\n",
    "    'fp32_gumbel': True,\n",
    "    'invert_label': False,\n",
    "    'mc': False,\n",
    "    'mask': 'electra',\n",
    "    'gen_smooth_label': False,\n",
    "    'disc_smooth_label': False,\n",
    "    'balanced_label': False,\n",
    "    'load_smallplusplus':False,\n",
    "    'tfdata': False,\n",
    "    'sort_sample': False,\n",
    "    'shuffle': True,\n",
    "    'percent': False,\n",
    "    # cache the data under it\n",
    "    'cache_dir': Path.home()/\"datasets\", # ! should be `pathlib.Path` istance\n",
    "    # cache checkpoints under checkpoint_dir/electra_pretrain\n",
    "    'checkpoint_dir': Path.home()/'checkpoints', # ! should be `pathlib.Path` istance\n",
    "})\n",
    "\n",
    "i = ['small', 'base', 'large'].index(c.size)\n",
    "c.mask_prob = [0.15, 0.15, 0.25][i]\n",
    "c.lr = [5e-4, 2e-4, 2e-4][i]\n",
    "c.bs = [128, 256, 2048][i]\n",
    "c.steps = [10**6, 766*1000, 400*1000][i]\n",
    "c.max_length = [128, 512, 512][i]\n",
    "c.cache_dir.mkdir(exist_ok=True,parents=True)\n",
    "c.checkpoint_dir.mkdir(exist_ok=True)\n",
    "gen_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-generator')\n",
    "disc_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-discriminator')\n",
    "hf_tokenizer = ElectraTokenizerFast.from_pretrained(f\"google/electra-{c.size}-generator\")\n",
    "if c.size in ['small', 'base']:\n",
    "  wiki_cache_dir = c.cache_dir/\"wikipedia/20200501.en/1.0.0\"\n",
    "  book_cache_dir = c.cache_dir/\"bookcorpus/plain_text/1.0.0\"\n",
    "  wbdl_cache_dir = c.cache_dir/\"wikibook_dl\"\n",
    "  wbdl_cache_dir.mkdir(exist_ok=True)\n",
    "print(os.getpid())\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loading the electra data (wiki)\nloading the electra data (BookCorpus)\n"
    }
   ],
   "source": [
    "if c.size in ['small', 'base'] and not c.tfdata:\n",
    "  \n",
    "  # wiki\n",
    "  if (wiki_cache_dir/f\"wiki_electra_{c.max_length}.arrow\").exists():\n",
    "    print('loading the electra data (wiki)')\n",
    "    wiki = nlp.Dataset.from_file(str(wiki_cache_dir/f\"wiki_electra_{c.max_length}.arrow\"))\n",
    "  else:\n",
    "    print('load/download wiki dataset')\n",
    "    wiki = nlp.load_dataset('wikipedia', '20200501.en', cache_dir=c.cache_dir)['train']\n",
    "  \n",
    "    print('creat data from wiki dataset for ELECTRA')\n",
    "    wiki = ELECTRADataTransform(wiki, is_docs=True, text_col={'text':'input_ids'}, max_length=c.max_length, hf_toker=hf_tokenizer).map(cache_file_name=str(wiki_cache_dir/f\"wiki_electra_{c.max_length}.arrow\"))\n",
    "\n",
    "  # bookcorpus\n",
    "  if (book_cache_dir/f\"book_electra_{c.max_length}.arrow\").exists():\n",
    "    print('loading the electra data (BookCorpus)')\n",
    "    book = nlp.Dataset.from_file(str(book_cache_dir/f\"book_electra_{c.max_length}.arrow\"))\n",
    "  else:\n",
    "    print('load/download BookCorpus dataset')\n",
    "    book = nlp.load_dataset('bookcorpus', cache_dir=c.cache_dir)['train']\n",
    "  \n",
    "    print('creat data from BookCorpus dataset for ELECTRA')\n",
    "    book = ELECTRADataTransform(book, is_docs=False, text_col={'text':'input_ids'}, max_length=c.max_length, hf_toker=hf_tokenizer).map(cache_file_name=str(book_cache_dir/f\"book_electra_{c.max_length}.arrow\"))\n",
    "\n",
    "  wb_data = HF_MergedDataset(wiki, book)\n",
    "  wb_dsets = HF_Datasets({'train': wb_data}, cols=['input_ids'], hf_toker=hf_tokenizer)\n",
    "  dls = wb_dsets.dataloaders(bs=c.bs, \n",
    "                             shuffle_train=c.shuffle,\n",
    "                             srtkey_fc=None if c.sort_sample else False, \n",
    "                             cache_dir=Path.home()/'datasets/wikibook_dl', cache_name='dl_{split}.json')\n",
    "\n",
    "else: # for large size\n",
    "  #raise NotImplementedError\n",
    "  import tensorflow as tf\n",
    "  tf.compat.v1.enable_eager_execution()\n",
    "  from electra.configure_pretraining import PretrainingConfig\n",
    "  from electra.pretrain.pretrain_data import get_input_fn\n",
    "  class TFDataLoader():\n",
    "    def __init__(self, batch_size, size, device='cpu'):\n",
    "      self.input_func = get_input_fn(PretrainingConfig('eeee', '../electra/data', **{'model_size': size}), True)\n",
    "      self.bs = batch_size\n",
    "      self.device = device\n",
    "      self.n_inp = 1 # for fastai to split x and y\n",
    "    def __iter__(self):\n",
    "      infinite_data = self.input_func({'batch_size':self.bs})\n",
    "      for features in infinite_data:\n",
    "        yield (torch.tensor(features['input_ids'].numpy(), dtype=torch.long, device=self.device),)\n",
    "    def to(self, device):\n",
    "      self.device = device\n",
    "      return self\n",
    "    def __len__(self): return 62500 # 6.25% of 10**6\n",
    "  dls = DataLoaders(TFDataLoader(c.bs, c.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Masked language model objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLM objective callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modified from HuggingFace/transformers (https://github.com/huggingface/transformers/blob/0a3d0e02c5af20bfe9091038c4fd11fb79175546/src/transformers/data/data_collator.py#L102). It is\n",
    "- few ms faster: intead of a[b] a on gpu b on cpu, tensors here are all in the same device\n",
    "- few tens of us faster: in how we create special token mask\n",
    "- doesn't require huggingface tokenizer\n",
    "- cost you only 20 ms on a (128,128) tensor, so dynamic masking is cheap   \n",
    "\"\"\"\n",
    "# https://github.com/huggingface/transformers/blob/1789c7daf1b8013006b0aef6cb1b8f80573031c5/examples/run_language_modeling.py#L179\n",
    "def mask_tokens(inputs, mask_token_index, vocab_size, special_token_indices, mlm_probability=0.15, replace_prob=0.1, orginal_prob=0.1, ignore_index=-100):\n",
    "  \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
    "  \"ignore_index in nn.CrossEntropy is default to -100, so you don't need to specify ignore_index in loss\"\n",
    "  \n",
    "  device = inputs.device\n",
    "  labels = inputs.clone()\n",
    "  # We sample a few tokens in each sequence for masked-LM training (with probability mlm_probability defaults to 0.15 in Bert/RoBERTA\n",
    "  probability_matrix = torch.full(labels.shape, mlm_probability, device=device)\n",
    "  special_tokens_mask = torch.full(inputs.shape, False, dtype=torch.bool, device=device)\n",
    "  for sp_id in special_token_indices:\n",
    "    special_tokens_mask = special_tokens_mask | (inputs==sp_id)\n",
    "  probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "  mlm_mask = torch.bernoulli(probability_matrix).bool()\n",
    "  labels[~mlm_mask] = ignore_index  # We only compute loss on masked tokens\n",
    "\n",
    "  # <1 - replace_prob - orginal_prob>% of the time, we replace masked input tokens with mask_token\n",
    "  mask_prob = 1 - replace_prob - orginal_prob\n",
    "  mask_token_mask = torch.bernoulli(torch.full(labels.shape, 0.8, device=device)).bool() & mlm_mask\n",
    "  inputs[mask_token_mask] = mask_token_index\n",
    "\n",
    "  # <replace_prob>% of the time, we replace masked input tokens with random word\n",
    "  if int(replace_prob)!=0:\n",
    "    rep_prob = replace_prob/(replace_prob + orginal_prob)\n",
    "    replace_token_mask = torch.bernoulli(torch.full(labels.shape, 0.5, device=device)).bool() & mlm_mask & ~mask_token_mask\n",
    "    random_words = torch.randint(vocab_size, labels.shape, dtype=torch.long, device=device)\n",
    "    inputs[replace_token_mask] = random_words[replace_token_mask]\n",
    "\n",
    "  # <orginal_prob>% of the time, we keep the masked input tokens unchanged\n",
    "  return inputs, labels, mlm_mask\n",
    "\n",
    "class MaskedLMCallback(Callback):\n",
    "  @delegates(mask_tokens)\n",
    "  def __init__(self, mask_tok_id, special_tok_ids, vocab_size, ignore_index=-100, for_electra=False, **kwargs):\n",
    "    self.ignore_index = ignore_index\n",
    "    self.for_electra = for_electra\n",
    "    self.mask_tokens = partial(mask_tokens,\n",
    "                               mask_token_index=mask_tok_id,\n",
    "                               special_token_indices=special_tok_ids,\n",
    "                               vocab_size=vocab_size,\n",
    "                               ignore_index=-100,\n",
    "                               **kwargs)\n",
    "\n",
    "  def begin_batch(self):\n",
    "    text_indices = self.xb[0]\n",
    "    masked_inputs, labels, is_mlm_applied = self.mask_tokens(text_indices)\n",
    "    if self.for_electra:\n",
    "      self.learn.xb, self.learn.yb = (masked_inputs, is_mlm_applied, labels), (labels,)\n",
    "    else:\n",
    "      self.learn.xb, self.learn.yb = (masked_inputs,), (labels,)\n",
    "\n",
    "  @delegates(TfmdDL.show_batch)\n",
    "  def show_batch(self, dl, idx_show_ignored, verbose=True, **kwargs):\n",
    "    b = dl.one_batch()\n",
    "    inputs = b[0]\n",
    "    masked_inputs, labels, is_mlm_applied = self.mask_tokens(inputs.clone())\n",
    "    # check\n",
    "    assert torch.equal(is_mlm_applied, labels!=self.ignore_index)\n",
    "    assert torch.equal((~is_mlm_applied *masked_inputs + is_mlm_applied * labels), inputs)\n",
    "    # change symbol to show the ignored position\n",
    "    labels[labels==self.ignore_index] = idx_show_ignored\n",
    "    # some notice to help understand the masking mechanism\n",
    "    if verbose: \n",
    "      print(\"We won't count loss from position where y is ignore index\")\n",
    "      print(\"Notice 1. Positions have label token in y will be either [Mask]/other token/orginal token in x\")\n",
    "      print(\"Notice 2. Special tokens (CLS, SEP) won't be masked.\")\n",
    "      print(\"Notice 3. Dynamic masking: every time you run gives you different results.\")\n",
    "    # show\n",
    "    tfm_b =(masked_inputs, is_mlm_applied, labels, labels) if self.for_electra else (masked_inputs, labels)   \n",
    "    dl.show_batch(b=tfm_b, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if c.mask == 'electra':\n",
    "  rep_prob, ori_prob = 0.0, 0.15\n",
    "elif c.mask == 'mc':\n",
    "  rep_prob, ori_prob = 0.0, 0.0\n",
    "else:\n",
    "  rep_prob, ori_prob = 0.1, 0.1\n",
    "mlm_cb = MaskedLMCallback(mask_tok_id=hf_tokenizer.mask_token_id, \n",
    "                          special_tok_ids=hf_tokenizer.all_special_ids, \n",
    "                          vocab_size=hf_tokenizer.vocab_size,\n",
    "                          mlm_probability=c.mask_prob,\n",
    "                          replace_prob=rep_prob, \n",
    "                          orginal_prob=ori_prob,\n",
    "                          for_electra=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ELECTRA (replaced token detection objective)\n",
    "see details in paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELECTRAModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, generator, discriminator):\n",
    "    super().__init__()\n",
    "    self.generator, self.discriminator = generator,discriminator\n",
    "    self.gumbel_dist = torch.distributions.gumbel.Gumbel(0.,1.)\n",
    "    self.toker = hf_tokenizer\n",
    "    # tight embeddings (word, pos, token_type)\n",
    "    # Note input and output embedding of generator has been tighted by huggingface/transformers \n",
    "    self.discriminator.model.electra.embeddings = self.generator.model.electra.embeddings\n",
    "\n",
    "  def to(self, *args, **kwargs):\n",
    "    super().to(*args, **kwargs)\n",
    "    a_tensor = next(self.parameters())\n",
    "    device, dtype = a_tensor.device, a_tensor.dtype\n",
    "    if c.fp32_gumbel: dtype = torch.float32\n",
    "    self.gumbel_dist = torch.distributions.gumbel.Gumbel(torch.tensor(0., device=device, dtype=dtype), torch.tensor(1., device=device, dtype=dtype))\n",
    "\n",
    "  def forward(self, masked_inputs, is_mlm_applied, labels):\n",
    "    # masked_inp_ids: (B,L)\n",
    "    # ignored: (B,L)\n",
    "    assert is_mlm_applied.dtype == torch.bool\n",
    "    \n",
    "    gen_logits = self.generator(masked_inputs) # (B, L, vocab size)\n",
    "    \n",
    "    # reduce size\n",
    "    mlm_gen_logits = gen_logits[is_mlm_applied, :].detach() # ( #mlm_positions, vocab_size)\n",
    "    # add gumbel noise and then sample\n",
    "    # will be a bottleneck if sample tensors of such size (128,128,30522) (57 ms on gpu, 7s on cpu)\n",
    "    if c.gumbel:\n",
    "      if c.fp32_gumbel:\n",
    "        pred_toks = (mlm_gen_logits.float() + self.gumbel_dist.sample(mlm_gen_logits.shape)).argmax(dim=-1)\n",
    "      else:\n",
    "        pred_toks = (mlm_gen_logits + self.gumbel_dist.sample(mlm_gen_logits.shape)).argmax(dim=-1)\n",
    "    else:\n",
    "      pred_toks = mlm_gen_logits.argmax(dim=-1)\n",
    "    # pred_toks: ( #mlm_positions, )\n",
    "    # use predicted token to fill 15%(mlm prob) mlm applied positions\n",
    "    generated = masked_inputs.clone() # (B,L)\n",
    "    generated[is_mlm_applied] = pred_toks # (B,L)\n",
    "    # not equal to generator predicted and is at mlm applied position\n",
    "    is_replaced = is_mlm_applied.clone() # (B,L)\n",
    "    is_replaced[is_mlm_applied] = (pred_toks != labels[is_mlm_applied]) # (B,L)\n",
    "\n",
    "    disc_logits = self.discriminator(generated) # (B, L)\n",
    "\n",
    "    return gen_logits, generated, disc_logits, is_replaced\n",
    "\n",
    "  def gumbel_softmax(self, logits):\n",
    "    \"reimplement it cuz there is a bug in torch.nn.functional.gumbel_softmax when fp16 (https://github.com/pytorch/pytorch/issues/41663)\"\n",
    "    \"This is equal to the code of official ELECTRA repo. standard gumbel dist. = -ln(-ln(standard uniform dist.))\"\n",
    "    gumbel_noise = self.gumbel_dist.sample(logits.shape)\n",
    "    return F.softmax(logits + gumbel_noise, dim=-1)\n",
    "\n",
    "class ELECTRALoss():\n",
    "  def __init__(self, pad_idx, loss_weights=(1.0, 50.0), gen_label_smooth=False, disc_label_smooth=False):\n",
    "    self.pad_idx = pad_idx\n",
    "    self.loss_weights = loss_weights\n",
    "    if gen_label_smooth:\n",
    "      eps = gen_label_smooth if isinstance(gen_label_smooth, float) else 0.1\n",
    "      self.gen_loss_fc = LabelSmoothingCrossEntropyFlat(eps=eps)\n",
    "    else:\n",
    "      self.gen_loss_fc = CrossEntropyLossFlat()\n",
    "    self.disc_loss_fc = nn.BCEWithLogitsLoss()\n",
    "    self.disc_label_smooth = disc_label_smooth\n",
    "    \n",
    "  def __call__(self, pred, targ_ids):\n",
    "    gen_logits, generated, disc_logits, is_replaced = pred\n",
    "    gen_loss = self.gen_loss_fc(gen_logits.float(), targ_ids) # ignore position where targ_id==-100\n",
    "    if c.balanced_label:\n",
    "      non_mlm_pos = targ_ids == -100\n",
    "      non_pad = generated != self.pad_idx\n",
    "      rlm_mask = torch.full(non_pad.shape, c.mask_prob/(1-c.mask_prob), device=non_pad.device)\n",
    "      rlm_mask = rlm_mask * non_pad * non_mlm_pos\n",
    "      rlm_mask = (torch.bernoulli(rlm_mask).bool() + ~non_mlm_pos).bool()\n",
    "      disc_logits = disc_logits.masked_select(rlm_mask) # -> 1d tensor\n",
    "      is_replaced = is_replaced.masked_select(rlm_mask) # -> 1d tensor\n",
    "    else:\n",
    "      non_pad = generated != self.pad_idx\n",
    "      disc_logits = disc_logits.masked_select(non_pad) # -> 1d tensor\n",
    "      is_replaced = is_replaced.masked_select(non_pad) # -> 1d tensor\n",
    "    if c.invert_label:\n",
    "      is_replaced = ~is_replaced\n",
    "    if self.disc_label_smooth:\n",
    "      eps = self.disc_label_smooth if isinstance(self.disc_label_smooth, float) else 0.1\n",
    "      zeros = ~is_replaced\n",
    "      is_replaced = is_replaced.float().masked_fill(zeros, eps)\n",
    "    disc_loss = self.disc_loss_fc(disc_logits.float(), is_replaced.float())\n",
    "    return gen_loss * self.loss_weights[0] + disc_loss * self.loss_weights[1]\n",
    "\n",
    "  def decodes(self, pred):\n",
    "    gen_logits, generated, disc_logits, is_replaced = pred\n",
    "    gen_pred = gen_logits.argmax(dim=-1)\n",
    "    disc_pred = disc_logits > 0\n",
    "    return gen_pred, generated, disc_pred, is_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCELECTRAModel(nn.Module):\n",
    "  \n",
    "    def __init__(self, generator, discriminator):\n",
    "        super().__init__()\n",
    "        self.generator, self.discriminator = generator,discriminator\n",
    "        self.toker = hf_tokenizer\n",
    "        # tight embeddings (word, pos, token_type)\n",
    "        # Note input and output embedding of generator has been tighted by huggingface/transformers \n",
    "        self.discriminator.model.electra.embeddings = self.generator.model.electra.embeddings\n",
    "\n",
    "    def forward(self, src_tokens, is_mlm_applied, labels):\n",
    "        masked_tokens = src_tokens == self.toker.mask_token_id\n",
    "        targets = src_tokens.clone()\n",
    "        targets[is_mlm_applied] = labels[is_mlm_applied]\n",
    "\n",
    "        if self.generator is not None:\n",
    "            # x-shape: (batch, src_len, vocab)\n",
    "            gen_x = self.generator(src_tokens)[masked_tokens,:]\n",
    "            sample_probs = torch.softmax(gen_x, -1, dtype=torch.float32).view(-1, gen_x.size(-1)).detach()\n",
    "            # sampled_tokens-shape: (batch*src_len, 1)\n",
    "            sampled_tokens = torch.multinomial(sample_probs, 1).view(-1)\n",
    "            # if self.args.debug:\n",
    "            #     import pdb\n",
    "            #     pdb.set_trace()\n",
    "\n",
    "            # detach the gradient bp and construct a new input\n",
    "            src_tokens = src_tokens.clone()\n",
    "            if masked_tokens is not None:\n",
    "                src_tokens[masked_tokens] = sampled_tokens\n",
    "            else:\n",
    "                src_tokens = sampled_tokens.view(src_tokens.size())\n",
    "        \n",
    "        # for discriminator, predict all of the tokens\n",
    "        disc_x = self.discriminator(src_tokens)\n",
    "        \n",
    "        return gen_x, disc_x, src_tokens, masked_tokens, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCElectraLoss():\n",
    "\n",
    "    def __init__(self, pad_idx, loss_weights=(1.0, 50.0), gen_label_smooth=False, disc_label_smooth=False):\n",
    "        self.padding_idx = pad_idx\n",
    "        self.loss_weights = loss_weights\n",
    "\n",
    "    def __call__(self, pred, labels):\n",
    "        \"\"\"Compute the loss for the given sample.\n",
    "        Returns a tuple with three elements:\n",
    "        1) the loss\n",
    "        2) the sample size, which is used as the denominator for the gradient\n",
    "        3) logging outputs to display while training\n",
    "        \"\"\"\n",
    "        gen_logits, disc_output, disc_tokens, masked_tokens, targets = pred\n",
    "        \n",
    "        not_pad_tokens = targets.ne(self.padding_idx)\n",
    "        \n",
    "        sample_size = masked_tokens.int().sum().item()\n",
    "\n",
    "        # (Rare case) When all tokens are masked, the model results in empty\n",
    "        # tensor and gives CUDA error.\n",
    "        if sample_size == 0:\n",
    "            masked_tokens = None\n",
    "\n",
    "        if sample_size != 0:\n",
    "            _targets = targets[masked_tokens]\n",
    "\n",
    "        gen_loss = F.nll_loss(\n",
    "            F.log_softmax(\n",
    "                gen_logits.view(-1, gen_logits.size(-1)),\n",
    "                dim=-1,\n",
    "                dtype=torch.float32,\n",
    "            ),\n",
    "            _targets.view(-1),\n",
    "            reduction='sum',\n",
    "            ignore_index=self.padding_idx,\n",
    "        )\n",
    "        \n",
    "        disc_targets = disc_tokens.eq(targets)[not_pad_tokens].float()\n",
    "\n",
    "        disc_loss = F.binary_cross_entropy_with_logits(disc_output[not_pad_tokens].float().view(-1),\n",
    "            disc_targets.view(-1), reduction='sum')\n",
    "\n",
    "        disc_sample_size = not_pad_tokens.int().sum().item()\n",
    "\n",
    "        loss = gen_loss + self.loss_weights[1] * disc_loss * sample_size / disc_sample_size\n",
    "\n",
    "        tp = ((disc_output[not_pad_tokens].float().view(-1) >= 0) & (disc_targets == 1)).long().sum()\n",
    "        fp = ((disc_output[not_pad_tokens].float().view(-1) >= 0) & (disc_targets == 0)).long().sum()\n",
    "        fn = ((disc_output[not_pad_tokens].float().view(-1) < 0) & (disc_targets == 1)).long().sum()\n",
    "        tn = ((disc_output[not_pad_tokens].float().view(-1) < 0) & (disc_targets == 0)).long().sum()\n",
    "        assert (tp + fp + tn + fn) == disc_targets.size(0), 'invalid size'\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_warmup_and_decay(pct_now, lr_max, end_lr, decay_power, total_steps,warmup_pct=None, warmup_steps=None):\n",
    "  assert warmup_pct or warmup_steps\n",
    "  if warmup_steps: warmup_pct = warmup_steps/total_steps\n",
    "  \"\"\"\n",
    "  end_lr: the end learning rate for linear decay\n",
    "  warmup_pct: percentage of training steps to for linear increase\n",
    "  pct_now: percentage of traning steps we have gone through, notice pct_now=0.0 when calculating lr for first batch.\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  pct updated after_batch, but global_step (in tf) seems to update before optimizer step,\n",
    "  so pct is actually (global_step -1)/total_steps \n",
    "  \"\"\"\n",
    "  fixed_pct_now = pct_now + 1/total_steps\n",
    "  \"\"\"\n",
    "  According to source code of the official repository, it seems they merged two lr schedule (warmup and linear decay)\n",
    "  sequentially, instead of split training into two phases for each, this might because they think when in the early\n",
    "  phase of training, pct is low, and thus the decaying formula makes little difference to lr.\n",
    "  \"\"\"\n",
    "  decayed_lr = (lr_max-end_lr) * (1-fixed_pct_now)**decay_power + end_lr # https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/polynomial_decay\n",
    "  warmed_lr = decayed_lr * min(1.0, fixed_pct_now / warmup_pct) # https://github.com/google-research/electra/blob/81f7e5fc98b0ad8bfd20b641aa8bc9e6ac00c8eb/model/optimization.py#L44\n",
    "  return warmed_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_shedule = ParamScheduler({'lr': partial(linear_warmup_and_decay,\n",
    "                                            lr_max=c.lr,\n",
    "                                            end_lr=0.0,\n",
    "                                            decay_power=1,\n",
    "                                            warmup_steps=10000 if not c.percent else int(0.1 * c.percent * 10**6),\n",
    "                                            total_steps=c.steps if not c.percent else int(c.percent * c.steps))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now_time():\n",
    "  now_time = datetime.now(timezone(timedelta(hours=+8)))\n",
    "  name = str(now_time)[6:-13].replace(' ', '_').replace(':', '-')\n",
    "  return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "8-03_07-51-11\n"
    }
   ],
   "source": [
    "if c.load_smallplusplus:\n",
    "    generator = HF_Model(ElectraForMaskedLM, f'google/electra-{c.size}-generator' , hf_tokenizer, variable_sep=True)\n",
    "    discriminator = HF_Model(ElectraForPreTraining, f\"google/electra-{c.size}-discriminator\", hf_tokenizer, variable_sep=True)\n",
    "else:\n",
    "    generator = HF_Model(ElectraForMaskedLM, gen_config, hf_tokenizer, variable_sep=True)\n",
    "    discriminator = HF_Model(ElectraForPreTraining, disc_config, hf_tokenizer, variable_sep=True)\n",
    "if c.mc:\n",
    "    electra_model = MCELECTRAModel(generator, discriminator)\n",
    "    electra_loss_func = MCElectraLoss(pad_idx=hf_tokenizer.pad_token_id)\n",
    "else:\n",
    "    electra_model = ELECTRAModel(generator, discriminator)\n",
    "    electra_loss_func = ELECTRALoss(pad_idx=hf_tokenizer.pad_token_id, gen_label_smooth=c.gen_smooth_label, disc_label_smooth=c.disc_smooth_label)\n",
    "\n",
    "dls.to(torch.device(c.device))\n",
    "run_name = now_time()\n",
    "print(run_name)\n",
    "learn = Learner(dls, electra_model,\n",
    "                loss_func=electra_loss_func,\n",
    "                opt_func=partial(Adam, eps=1e-6,),\n",
    "                path=str(c.checkpoint_dir),\n",
    "                model_dir='electra_pretrain',\n",
    "                cbs=[mlm_cb,\n",
    "                    RunSteps(c.steps, [0.0625, 0.125, 0.5, 1.0], run_name+\"_{percent}\") if not c.percent else RunSteps(int(c.steps*c.percent))\n",
    "                    ],\n",
    "                )\n",
    "if c.use_fp16: learn = learn.to_fp16(clip=1.)\n",
    "# to also exclude layernorm\n",
    "learn.create_opt()\n",
    "for p in my_bn_bias_state(learn, True): p['do_wd'] = False \n",
    "# ----------------------------\n",
    "learn.fit(9999, cbs=[lr_shedule])\n",
    "if c.percent: learn.save(run_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}