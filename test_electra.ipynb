{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace as bk\n",
    "import numpy\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.tensor as T\n",
    "from transformers import ElectraModel, ElectraConfig, ElectraTokenizerFast, ElectraForMaskedLM,ElectraForPreTraining\n",
    "from fastai2.text.all import *\n",
    "from _utils.would_like_to_pr import *\n",
    "from _utils.huggingface import *\n",
    "from _utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = numpy.load('features.npy', allow_pickle=True).item()\n",
    "masked_inputs = numpy.load('masked_inputs.npy', allow_pickle=True).item()\n",
    "mlm_output = numpy.load('mlm_output.npy', allow_pickle=True).item()\n",
    "fake_data = numpy.load('fake_data.npy', allow_pickle=True).item()\n",
    "disc_output = numpy.load('disc_output.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELECTRA的code可以mask 相同位置，在製造generated的時候會把相同位置預測的id平均，在計算gen_loss的時候就等同於那個位置的多算一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-38649c77299c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdisc_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "is_mlm_applied = torch.zeros(masked_inputs['input_ids'].shape, dtype=torch.long)\n",
    "for i, poses in enumerate(masked_inputs['masked_lm_positions']):\n",
    "  for j in poses:\n",
    "    if j == 0: continue\n",
    "    is_mlm_applied[i, j] = 1\n",
    "\n",
    "gen_logitss = T(mlm_output['logits'])# self.generator(masked_inputs) # (B, L, vocab size)\n",
    "gen_pred = gen_logitss.argmax(dim=-1)\n",
    "assert torch.equal(gen_pred, T(mlm_output['preds'], dtype=torch.long))\n",
    "gen_logits = torch.full([*masked_inputs['input_ids'].shape,30522], -1.)\n",
    "for i, poses in enumerate(masked_inputs['masked_lm_positions']):\n",
    "  for th, j in enumerate(poses):\n",
    "    if j == 0: continue\n",
    "    gen_logits[i, j] = gen_logitss[i, th]\n",
    "\n",
    "# add gumbel noise and then softmax\n",
    "pred_tokks = (gen_logitss + T(fake_data['noise'])).argmax(dim=-1)\n",
    "assert torch.equal(pred_tokks, T(fake_data['sampled_tokens']).argmax(dim=-1))\n",
    "pred_toks = torch.full(masked_inputs['input_ids'].shape, 0, dtype=torch.long)\n",
    "times = torch.full(masked_inputs['input_ids'].shape, 1, dtype=torch.long)\n",
    "for i, poses in enumerate(masked_inputs['masked_lm_positions']):\n",
    "  for th, j in enumerate(poses):\n",
    "    if j == 0: continue\n",
    "    if j in poses[:th]: times[i, j] += 1\n",
    "    pred_toks[i, j] += pred_tokks[i, th]\n",
    "pred_toks /= times\n",
    "\n",
    "# use predicted token to fill 15%(mlm prob) mlm applied positions\n",
    "generated = ~is_mlm_applied.bool() * T(masked_inputs['input_ids'], dtype=torch.long) + is_mlm_applied * pred_toks # (B,L)\n",
    "assert torch.equal(generated, T(fake_data['updated_ids'], dtype=torch.long))\n",
    "\n",
    "# not equal to generator predicted and is at mlm applied position\n",
    "labels = torch.full(masked_inputs['input_ids'].shape, -100)\n",
    "mids = T(masked_inputs['masked_lm_ids']).long()\n",
    "for i, poses in enumerate(masked_inputs['masked_lm_positions']):\n",
    "  for th, j in enumerate(poses):\n",
    "    if j == 0: continue\n",
    "    labels[i, j] = mids[i, th]\n",
    "is_replaced = (pred_toks != labels) * is_mlm_applied # (B, L)\n",
    "assert torch.equal(is_replaced.long(), T(fake_data['is_fake_tokens']).long())\n",
    "\n",
    "disc_logits = T(disc_output['logits']) # self.discriminator(generated) # (B, L)\n",
    "disc_pred = disc_logits > 0\n",
    "assert torch.equal(disc_pred.long(), T(disc_output['preds']).long())\n",
    "\n",
    "gen_loss = CrossEntropyLossFlat()(gen_logits.float(), labels.long()) # ignore position where targ_id==-100\n",
    "assert torch.allclose(gen_loss, T(mlm_output['loss']))\n",
    "\n",
    "non_pad = generated != 0\n",
    "disc_logits = disc_logits.masked_select(non_pad) # -> 1d tensor\n",
    "is_replaced = is_replaced.masked_select(non_pad) # -> 1d tensor\n",
    "disc_loss = nn.BCEWithLogitsLoss()(disc_logits.float(), is_replaced.float())\n",
    "assert torch.allclose(disc_loss, T(disc_output['loss']))\n",
    "\n",
    "total_loss=gen_loss * 1. + disc_loss * 50.\n",
    "assert torch.equal(total_loss, T(disc_output['total_loss']))"
   ]
  }
 ]
}