{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Try TextDataloader",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14939a42dae64fc587d1a14eacf69d71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9011f446875f40f19552f5f378b0edb9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2c8f3df20eb74b25868d6413e0d8b887",
              "IPY_MODEL_0d8cf87bd6a247d8b13f033d290273db"
            ]
          }
        },
        "9011f446875f40f19552f5f378b0edb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c8f3df20eb74b25868d6413e0d8b887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3798107ac120453682b3aa8cf189c5cf",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fa3782bcb2c34f459570eb6fabeab0fc"
          }
        },
        "0d8cf87bd6a247d8b13f033d290273db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_22e43617f9054d6f8bae7b58cc409c0b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 779kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7afacb1da0ad4329ace68c11741cf956"
          }
        },
        "3798107ac120453682b3aa8cf189c5cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fa3782bcb2c34f459570eb6fabeab0fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "22e43617f9054d6f8bae7b58cc409c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7afacb1da0ad4329ace68c11741cf956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9jAg3I25Kfr",
        "colab_type": "code",
        "outputId": "839b39a7-8aab-4aec-f900-7d4ca7780074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "!git clone https://github.com/richardyy1188/Pretrain-MLM-and-finetune-on-GLUE-with-fastai.git\n",
        "%pip install -q fastai2 transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Pretrain-MLM-and-finetune-on-GLUE-with-fastai'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 30 (delta 10), reused 25 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 194kB 4.5MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 645kB 18.2MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 23.6MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 39.9MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 14.1MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omRD3-nx5SVs",
        "colab_type": "code",
        "outputId": "89b89ee6-4d61-4555-f24a-69e4134675d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "14939a42dae64fc587d1a14eacf69d71",
            "9011f446875f40f19552f5f378b0edb9",
            "2c8f3df20eb74b25868d6413e0d8b887",
            "0d8cf87bd6a247d8b13f033d290273db",
            "3798107ac120453682b3aa8cf189c5cf",
            "fa3782bcb2c34f459570eb6fabeab0fc",
            "22e43617f9054d6f8bae7b58cc409c0b",
            "7afacb1da0ad4329ace68c11741cf956"
          ]
        }
      },
      "source": [
        "%cd Pretrain-MLM-and-finetune-on-GLUE-with-fastai\n",
        "\n",
        "from IPython.core.debugger import set_trace as bk\n",
        "from functools import partial\n",
        "import pickle\n",
        "import torch\n",
        "from fastai2.text.all import *\n",
        "from transformers import ElectraTokenizer\n",
        "hf_tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-generator\")\n",
        "from _utils.hf_transformers_integration import HF_Tokenizer, HF_TextBlock, HFModelWrapper\n",
        "from _utils.demo_data import load_demo_dataframe"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Pretrain-MLM-and-finetune-on-GLUE-with-fastai\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14939a42dae64fc587d1a14eacf69d71",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-SSKytH_6ns",
        "colab_type": "code",
        "outputId": "7319e259-8c41-4116-9967-5b48270a6711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "# from the lyrics of \"Avicii - Waiting For Love\"\n",
        "data={'text':[\"Monday left me broken\",\"Tuesday I was through with hoping\",\"Wednesday my empty arms were open\",\"Thursday waiting for love, waiting for love\",\"Thank the stars it's Friday\",\"I'm burning like a fire gone wild on Saturday\",\"Guess I won't be coming to church on Sunday\",\"I'll be waiting for love, waiting for love\",\"To come around\",],'is_valid':[False]*7 + [True]*2}\n",
        "data['length'] = [ len(t.split()) for t in data['text']]\n",
        "df = pd.DataFrame(data=data)\n",
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Monday left me broken</td>\n",
              "      <td>False</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tuesday I was through with hoping</td>\n",
              "      <td>False</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Wednesday my empty arms were open</td>\n",
              "      <td>False</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thursday waiting for love, waiting for love</td>\n",
              "      <td>False</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thank the stars it's Friday</td>\n",
              "      <td>False</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I'm burning like a fire gone wild on Saturday</td>\n",
              "      <td>False</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Guess I won't be coming to church on Sunday</td>\n",
              "      <td>False</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I'll be waiting for love, waiting for love</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>To come around</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            text  is_valid  length\n",
              "0                          Monday left me broken     False       4\n",
              "1              Tuesday I was through with hoping     False       6\n",
              "2              Wednesday my empty arms were open     False       6\n",
              "3    Thursday waiting for love, waiting for love     False       7\n",
              "4                    Thank the stars it's Friday     False       5\n",
              "5  I'm burning like a fire gone wild on Saturday     False       9\n",
              "6    Guess I won't be coming to church on Sunday     False       9\n",
              "7     I'll be waiting for love, waiting for love      True       8\n",
              "8                                 To come around      True       3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLiBRkiM5hgb",
        "colab_type": "text"
      },
      "source": [
        "#1. TextDataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xuPaHcu5jbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@delegates()\n",
        "class TextDataloader(TfmdDL):\n",
        "  def __init__(self, dataset, max_seq_len=float('inf'), sort_by_len=True, agg_mode=None, ignore_gt_maxlen=True, remove_heads=False, remove_tails=False, bos_idx_add=None, eos_idx_add=None, **kwargs):\n",
        "    super().__init__(dataset, **kwargs)\n",
        "    assert agg_mode in [None, 'lm', 'lines', 'window']\n",
        "    assert not (agg_mode and max_seq_len is None) \n",
        "    self.sort_by_len = sort_by_len and agg_mode in [None, 'lines'] # sorting makes sense only with these modes\n",
        "    ignore_gt_maxlen = ignore_gt_maxlen and agg_mode in [None, 'lines'] and max_seq_len is not None\n",
        "    first_text_tensor = dataset[0][0]\n",
        "    device, dtype = first_text_tensor.device, first_text_tensor.dtype\n",
        "    self.bos = torch.tensor([bos_idx_add] if bos_idx_add is not None else [], device=device, dtype=dtype)\n",
        "    self.eos = torch.tensor([eos_idx_add] if eos_idx_add is not None else [], device=device, dtype=dtype)\n",
        "    self.add_bos_or_eos = bos_idx_add or eos_idx_add\n",
        "\n",
        "    store_attr(self,'dataset,max_seq_len,sort_by_len,agg_mode,ignore_gt_maxlen,remove_heads,remove_tails,bos_idx_add,eos_idx_add')\n",
        "    \n",
        "    self.samples = L()\n",
        "    # residual_len will reset to initial_residual_len\n",
        "    # lm mode: max_seq_len text and 1 right-shift text, so take max_seq_len + 1 window\n",
        "    self.initial_residual_len = max_seq_len + 1 if agg_mode=='lm' else max_seq_len \n",
        "    # keep spaces to add bos to final text \n",
        "    if bos_idx_add is not None: self.initial_residual_len -= 1\n",
        "    if eos_idx_add is not None: self.initial_residual_len -= 1\n",
        "    self.residual_len, self.new_sample = self.initial_residual_len, []\n",
        "    # only use [start:end] text to concatenate (if needed)\n",
        "    self.start = 0 if not remove_heads else 1\n",
        "    self.end = None if not remove_tails else -1\n",
        "\n",
        "    for i, sample in enumerate(dataset):\n",
        "      line_len = len(sample[0])\n",
        "      if remove_heads: line_len -= 1\n",
        "      if remove_tails: line_len -= 1\n",
        "      \n",
        "      if max_seq_len is not None and line_len > self.initial_residual_len and agg_mode in [None, 'lines']:\n",
        "        if ignore_gt_maxlen: continue\n",
        "        else: raise ValueError(f'The {i} th text line in dataset has length {line_len}(without removing head or tail, {len(sample[0])}), and is longer than max length {self.initial_residual_len}(without add bos or eos, {max_seq_len})')\n",
        "        \n",
        "      if agg_mode is None: self.samples.append( (line_len, i) )\n",
        "      elif agg_mode == 'lines': self._accumulate_lines(i, line_len)\n",
        "      else: self._accumulate_window(i, line_len)\n",
        "    \n",
        "    if agg_mode is not None and self.new_sample:\n",
        "      if agg_mode == 'lines': self.samples.append((self.max_seq_len-self.residual_len, self.new_sample))\n",
        "      else: self.samples.append(self.new_sample)\n",
        "\n",
        "    # specify total number of samples\n",
        "    self.n = len(self.samples)\n",
        "      \n",
        "  def _accumulate_lines(self, i, line_len):\n",
        "    if line_len <= self.residual_len:\n",
        "      self.new_sample.append(i)\n",
        "      self.residual_len -= line_len\n",
        "    else:\n",
        "      self.samples.append((self.max_seq_len-self.residual_len, self.new_sample))\n",
        "      self.new_sample = [i]\n",
        "      self.residual_len = self.initial_residual_len - line_len\n",
        "\n",
        "  def _accumulate_window(self, i, line_len):\n",
        "    usable_len = line_len\n",
        "    cursor = self.start\n",
        "    while usable_len != 0:\n",
        "      use_len = min(usable_len, self.residual_len)\n",
        "      self.new_sample.append((i, cursor, cursor+use_len))\n",
        "      self.residual_len -= use_len\n",
        "      usable_len -= use_len\n",
        "      cursor += use_len\n",
        "      if self.residual_len == 0:\n",
        "        self.samples.append(self.new_sample)\n",
        "        self.new_sample = []\n",
        "        self.residual_len = self.initial_residual_len\n",
        "\n",
        "  def create_item(self, s):\n",
        "    if self.agg_mode is None:\n",
        "      \"samples = [ (length, idx), ... ]\"\n",
        "      idx = self.samples[s][1]\n",
        "      sample = self.dataset[idx]\n",
        "      line = sample[0][self.start:self.end]\n",
        "      text = torch.cat([self.bos, line, self.eos]) if self.add_bos_or_eos else line\n",
        "      return ( TensorText(text), *sample[1:] )\n",
        "    elif self.agg_mode == 'lines':\n",
        "      \"samples = [ (length, [idx, idx, ...]) , ... ]\"\n",
        "      agg = [ self.dataset[idx][0][self.start:self.end] for idx in self.samples[s][1] ]\n",
        "      agg_text = concat(self.bos, *agg, self.eos) if self.add_bos_or_eos else concat(*agg)\n",
        "      return (TensorText(agg_text), )\n",
        "    else: # window or lm\n",
        "      \"samples = [ (idx,start,end) ]\"\n",
        "      agg = [ self.dataset[idx][0][start:end] for idx,start,end in self.samples[s] ]\n",
        "      agg_text = concat(self.bos, *agg, self.eos) if self.add_bos_or_eos else concat(*agg)\n",
        "      if self.agg_mode == 'window':\n",
        "        return (TensorText(agg_text), )\n",
        "      else: # 'lm'\n",
        "        return (LMTensorText(agg_text[:-1]), TensorText(agg_text[1:]))\n",
        "\n",
        "  def shuffle_fn(self, idxs):\n",
        "    if self.agg_mode in ['lm', 'window']:\n",
        "      self.samples.shuffle()\n",
        "    else:\n",
        "      self.samples.sort(key=lambda s: s[0])\n",
        "    return idxs\n",
        "\n",
        "  def cache(self, file_path):\n",
        "    tmp = self.dataset\n",
        "    self.dataset = None\n",
        "    torch.save(self, file_path)\n",
        "    self.dataset = tmp\n",
        "\n",
        "  @classmethod\n",
        "  def from_cache(cls, file_path, dataset):\n",
        "    dl = torch.load(file_path)\n",
        "    dl.dataset = dataset\n",
        "    return dl\n",
        "\n",
        "  @delegates(TfmdDL.new)\n",
        "  def new(self, dataset=None, **kwargs):\n",
        "    return super().new(dataset=dataset,\n",
        "                       max_seq_len=self.max_seq_len,\n",
        "                       sort_by_len=self.sort_by_len,\n",
        "                       agg_mode=self.agg_mode,\n",
        "                       ignore_gt_maxlen=False, # You can't discard data from valid set, especially test set\n",
        "                       remove_heads=self.remove_heads,\n",
        "                       remove_tails=self.remove_tails,\n",
        "                       bos_idx_add=self.bos_idx_add,\n",
        "                       eos_idx_add=self.eos_idx_add,\n",
        "                       **kwargs,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tchYkt4w5mDr",
        "colab_type": "text"
      },
      "source": [
        "# 2. Try"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCQiYfE4GIRk",
        "colab_type": "text"
      },
      "source": [
        "We'll try different param of `TextDataloader` to show its capability, but **!! it doesn't mean these are the best practices. !!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3m_Qjh97lgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = DataBlock(splitter=ColSplitter(),\n",
        "              blocks=HF_TextBlock.from_df('text', hf_tokenizer),\n",
        "              get_x=ColReader('text'),)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylPTnYJk5o09",
        "colab_type": "text"
      },
      "source": [
        "Default behavior:\n",
        "* a line a sample\n",
        "* collect samples by their length. (try to make samples with the same length as a batch, to reduce number of pad)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oWfgKqJ5kjN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "daa01597-9c44-4c90-c989-a6a234dc1607"
      },
      "source": [
        "default_dls = db.dataloaders(df, bs=4, shuffle_train=True, dl_type=TextDataloader)\n",
        "default_dls.show_batch(max_n=4)\n",
        "\"\"\"\n",
        "We sort the sample by its length.\n",
        "Observe that the 3rd sample of batch is Friday (9 tokens) but not Thursday (10 tokens), \n",
        "thus we can reduce number of pad need to add, \n",
        "becuase we have to make all samples in a batch the same legth.\n",
        "\"\"\"\n",
        "print('x batch size:', default_dls.one_batch()[0].shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] [PAD] [PAD] [PAD]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] tuesday i was through with hoping [SEP] [PAD]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[CLS] wednesday my empty arms were open [SEP] [PAD]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[CLS] thank the stars it ' s friday [SEP]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "x batch size: torch.Size([4, 9])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPbXr196wKks",
        "colab_type": "text"
      },
      "source": [
        "**Window mode**\n",
        "* Want to use broader context\n",
        "* sliding context window\n",
        "* less pad (only samples in the last batch may have pad)\n",
        "* every sample is of `max_seq_len` length. (Unless the last batch only have one sample shorter than `max_seq_len`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v-Zo8jTwKST",
        "colab_type": "code",
        "outputId": "9e7081ef-7615-4526-d782-133b157f096e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "window_dls = db.dataloaders(df, shuffle_train=False, bs=2,\n",
        "                     dl_type=partial(TextDataloader,\n",
        "                                     max_seq_len=15,\n",
        "                                     agg_mode='window',\n",
        "                                     remove_heads=True,\n",
        "                                     remove_tails=True,\n",
        "                                     bos_idx_add=hf_tokenizer.cls_token_id,\n",
        "                                     eos_idx_add=hf_tokenizer.sep_token_id))\n",
        "window_dls.show_batch(max_n=2)\n",
        "\"\"\"\n",
        "To use CLS...SEP format, first remove heads(CLS) and tails(SEP) for every line,\n",
        "and then add bos(CLS) and eos(SEP) to the head and tail of concatenated sequence.\n",
        "\"\"\"\n",
        "print('x batch size:', window_dls.one_batch()[0].shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken tuesday i was through with hoping wednesday my empty [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] arms were open thursday waiting for love , waiting for love thank the [SEP]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "x batch size: torch.Size([2, 15])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVIiXWnkrTWZ",
        "colab_type": "text"
      },
      "source": [
        "**Lines mode**\n",
        "* Want to attend to wider context, but also don't want shattered sentence.\n",
        "* Sequentially concat lines.\n",
        "* Note that `max_seq_len` is not definitely length of sample, and increasing it doesn't definitely increase number of pads used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r98xX2c3Ws9j",
        "colab_type": "code",
        "outputId": "4190828e-fe2a-4c02-9a94-1742e04905a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "lines_dls = db.dataloaders(df, shuffle_train=False, bs=2,\n",
        "                     dl_type=partial(TextDataloader,\n",
        "                                     max_seq_len=13,\n",
        "                                     agg_mode='lines',\n",
        "                                     remove_heads=True,\n",
        "                                     bos_idx_add=hf_tokenizer.cls_token_id))\n",
        "lines_dls.show_batch(max_n=2)\n",
        "\"\"\"\n",
        "To get CLS ... SEP ... SEP format, we remove head (CLS) for every line,\n",
        "and add back an bos (CLS) to head of concated sample.\n",
        "\"\"\"\n",
        "print('x batch size:', lines_dls.one_batch()[0].shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] tuesday i was through with hoping [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] wednesday my empty arms were open [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "x batch size: torch.Size([2, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWHqTA6EzwjU",
        "colab_type": "text"
      },
      "source": [
        "**(Traditional) Language model mode**\n",
        "* predict i th token in y, using 0~i-1 tokens in x\n",
        "* sliding context window\n",
        "* samples in the last batch may have pad\n",
        "* every sample is of `max_seq_len` length. (Unless the last batch only have one sample shorter than `max_seq_len`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hijemiYrr9kV",
        "colab_type": "code",
        "outputId": "1c41ca67-93f3-4a34-a899-b55537b93c52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "lm_dls = db.dataloaders(df, shuffle_train=False, bs=2,\n",
        "                        dl_type=partial(TextDataloader,\n",
        "                                        max_seq_len=7,\n",
        "                                        agg_mode='lm',))\n",
        "lm_dls.show_batch(max_n=2)\n",
        "print('x batch size:', lm_dls.one_batch()[0].shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] [CLS]</td>\n",
              "      <td>monday left me broken [SEP] [CLS] tuesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i was through with hoping [SEP] [CLS]</td>\n",
              "      <td>was through with hoping [SEP] [CLS] wednesday</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "x batch size: torch.Size([2, 7])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XHQc_d4HKFS",
        "colab_type": "text"
      },
      "source": [
        "# 3. Speed comparison to existing dataloader for text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmiZeQRBKiyT",
        "colab_type": "text"
      },
      "source": [
        "Create datasets first to not count the time of creating datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeratlXOHnk1",
        "colab_type": "code",
        "outputId": "e27b9013-9c0d-4ccd-90de-45d97fbcf570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "another_df = load_demo_dataframe()\n",
        "print(len(another_df))\n",
        "another_datasets = db.datasets(another_df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14489\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfawXIM1adk_",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Compare time for initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAD-2rNmKQlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataloaders_from_db_and_datasets(db, dsets, path='.', verbose=False, **kwargs):\n",
        "    kwargs = {**db.dls_kwargs, **kwargs, 'verbose': verbose}\n",
        "    return dsets.dataloaders(path=path, after_item=db.item_tfms, after_batch=db.batch_tfms, **kwargs)\n",
        "get_dataloaders = partial(dataloaders_from_db_and_datasets, db, another_datasets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VynAPJE_IT_-",
        "colab_type": "code",
        "outputId": "b21154b0-895c-43b2-877e-c91520994525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%timeit get_dataloaders(dl_type=SortedDL)\n",
        "%timeit get_dataloaders(dl_type=partial(TextDataloader, sort_by_len=True))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 8.64 s per loop\n",
            "1 loop, best of 3: 7.61 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qot1RTQzO-8A",
        "colab_type": "code",
        "outputId": "f46b3bb8-b4c6-48af-fe68-a1b59f36e71e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%timeit get_dataloaders(dl_type=LMDataLoader)\n",
        "%timeit get_dataloaders(dl_type=partial(TextDataloader, max_seq_len=72, agg_mode='lm',))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 7.72 s per loop\n",
            "1 loop, best of 3: 7.68 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y425qGZVanSX",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Compare time for load batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ximrGyasjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We reinitialize because assignment in %timeit is local to %timeit special function scope \n",
        "sorted_dls = get_dataloaders(dl_type=SortedDL)\n",
        "my_sorted_dls = get_dataloaders(dl_type=partial(TextDataloader, sort_by_len=True))\n",
        "LM_dls = get_dataloaders(dl_type=LMDataLoader)\n",
        "my_LM_dls = get_dataloaders(dl_type=partial(TextDataloader, max_seq_len=72, agg_mode='lm',))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_OWDBNab1Zw",
        "colab_type": "code",
        "outputId": "d8416a06-f080-4894-a58a-a3e833c66d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%timeit for b in sorted_dls.train: pass\n",
        "%timeit for b in my_sorted_dls.train: pass"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 5.54 s per loop\n",
            "1 loop, best of 3: 5.84 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4EwgBkXcHmx",
        "colab_type": "code",
        "outputId": "61bc043b-eaed-428f-ab07-c0cd8973e7a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%timeit for b in LM_dls.train: pass\n",
        "%timeit for b in my_LM_dls.train: pass"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 9.58 s per loop\n",
            "1 loop, best of 3: 6.31 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8Bw8VZWIMaX",
        "colab_type": "text"
      },
      "source": [
        "# 4. Cache\n",
        "So you don't need to initailize dataloader from scratch every time.\n",
        "\n",
        "Note that we cache mainly internal record of which sample should concatenate with which sample, but not the dataset itself. If you want cachable dataset, take a look at huggingface/nlp\n",
        "\n",
        "You should pass the same dataset, especially note that order of samples should be as the same as the original one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCRw7-lmk7Vg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataloaders_from_cache(db, source, file_paths, path='.', device=None):\n",
        "  device = default_device()\n",
        "  file_paths = L(file_paths).map(lambda p: Path(p))\n",
        "  datasets = db.datasets(source)\n",
        "  dl_s = L()\n",
        "  for i, f in enumerate(file_paths):\n",
        "    dl_s.append(TextDataloader.from_cache(f, datasets.subset(i)))\n",
        "  return DataLoaders(*dl_s, path=path, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OorV2Brov8--",
        "colab_type": "text"
      },
      "source": [
        "## 5.1 Test Cache"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7dQJcjvk9PZ",
        "colab_type": "code",
        "outputId": "ad4e1b0e-09f6-40c4-b7bd-c30f9cdfaa57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "default_dls.train.cache('train.pkl')\n",
        "default_dls.valid.cache('valid.pkl')\n",
        "loaded_default_dls = dataloaders_from_cache(db, df, ['train.pkl','valid.pkl'])\n",
        "loaded_default_dls.show_batch()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] [PAD] [PAD] [PAD]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] tuesday i was through with hoping [SEP] [PAD]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[CLS] wednesday my empty arms were open [SEP] [PAD]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[CLS] thank the stars it ' s friday [SEP]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y5mqAqvvdwM",
        "colab_type": "code",
        "outputId": "155d52b5-3fde-4dcf-8fce-a8595b9daf99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "window_dls.train.cache('train.pkl')\n",
        "window_dls.valid.cache('valid.pkl')\n",
        "loaded_window_dls = dataloaders_from_cache(db, df, ['train.pkl','valid.pkl'])\n",
        "loaded_window_dls.show_batch()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken tuesday i was through with hoping wednesday my empty [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] arms were open thursday waiting for love , waiting for love thank the [SEP]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZZDRvAuvoUW",
        "colab_type": "code",
        "outputId": "fd64fc0d-84b0-4c44-d6ac-96225d81ddbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "lines_dls.train.cache('train.pkl')\n",
        "lines_dls.valid.cache('valid.pkl')\n",
        "loaded_lines_dls = dataloaders_from_cache(db, df, ['train.pkl','valid.pkl'])\n",
        "loaded_lines_dls.show_batch()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] tuesday i was through with hoping [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] wednesday my empty arms were open [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iah2CRiLvvb7",
        "colab_type": "code",
        "outputId": "6ace5cc1-331d-4f9d-f248-09ade0eabe7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "lm_dls.train.cache('train.pkl')\n",
        "lm_dls.valid.cache('valid.pkl')\n",
        "loaded_lm_dls = dataloaders_from_cache(db, df, ['train.pkl','valid.pkl'])\n",
        "loaded_lm_dls.show_batch()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] [CLS]</td>\n",
              "      <td>monday left me broken [SEP] [CLS] tuesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i was through with hoping [SEP] [CLS]</td>\n",
              "      <td>was through with hoping [SEP] [CLS] wednesday</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}