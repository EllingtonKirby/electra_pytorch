{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finetune GLUE with fastai.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "p_P8P4hffUr8",
        "yKxetBkqfdgm",
        "MqtDnxK9ZYQP",
        "gebKI3-ywWNS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uns2VbU0Yj5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try: import nlp\n",
        "except ImportError:\n",
        "  %pip install -q fastai2 transformers nlp\n",
        "  !rm -rf Pretrain-MLM-and-finetune-on-GLUE-with-fastai\n",
        "  !git clone https://github.com/richardyy1188/Pretrain-MLM-and-finetune-on-GLUE-with-fastai.git\n",
        "  exit() # In Colab, to use the newer installed pyarrow, you need to restart your session for first use"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugFdx5v_YxzA",
        "colab_type": "code",
        "outputId": "99070a06-1145-471d-894a-1c535027c6f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd Pretrain-MLM-and-finetune-on-GLUE-with-fastai\n",
        "\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "from IPython.core.debugger import set_trace as bk\n",
        "import pandas as pd\n",
        "from sklearn import metrics as skm\n",
        "from scipy import stats as spm\n",
        "from torch import nn\n",
        "import nlp\n",
        "from transformers import ElectraModel, ElectraTokenizer, ElectraTokenizerFast\n",
        "from fastai2.text.all import *\n",
        "from _utils.would_like_to_pr import TextDataloader\n",
        "from _utils.hf_transformers_integration import HFModelWrapper"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Pretrain-MLM-and-finetune-on-GLUE-with-fastai\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wb0zB6iYyvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" tokenizer and fast tokenizer\n",
        "We use normal tokenizer to get vocab, use fast tokenizer to convert tokens to ids.\n",
        "Because we can't get vocab from fast tokenizer and fast tokenizer is faster,\n",
        "and they share the same token-id mapping.\n",
        "\"\"\"\n",
        "hf_tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
        "hf_fast_tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-small-discriminator\")\n",
        "base_model = ElectraModel.from_pretrained('google/electra-small-discriminator')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoI6QffPY6Jv",
        "colab_type": "text"
      },
      "source": [
        "# 1. Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_P8P4hffUr8",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zfrnOzY9u6",
        "colab_type": "text"
      },
      "source": [
        "**Download, Preprocess, and Cache**\n",
        "\n",
        "In Colab, it takes you 20+ minutes for the first time, and seconds for subsequent calls. \n",
        "\n",
        "It will cost serveral minutes if you reset runtime even load the cache, but it's not true when you just restart the runtime. So I guess they may keep someth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-PLcDIRY6y-",
        "colab_type": "code",
        "outputId": "cf282dd9-4955-402a-ff8b-dbc932b7b8b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# create a 'glue' folder under it, and all cache files will be under 'glue'\n",
        "cache_dir=Path('/content/drive/My Drive/datasets')\n",
        "cache_dir.mkdir(parents=True, exist_ok=True) # create recursively if not exist\n",
        "\n",
        "def textcols(dataset):\n",
        "  \"Infer text cols of different GLUE datasets in huggingface/nlp\"\n",
        "  column_names = dataset.column_names\n",
        "  if 'question' in column_names: return ['question', 'sentence']\n",
        "  elif 'sentence1' in column_names: return ['sentence1', 'sentence2']\n",
        "  elif 'question1' in column_names: return ['question1','question2']\n",
        "  elif 'premise' in column_names: return ['premise','hypothesis']\n",
        "  elif 'sentence' in column_names: return ['sentence']\n",
        "\n",
        "\"\"\"\n",
        "quote from transformers tutorial of fastai (http://dev.fast.ai/tutorial.transformers)\n",
        "we don't use the tokenizer.encode method since it does some additional preprocessing for the model after tokenizing and numericalizing (the aprt throwing a warning before). Here we don't need any post-processing so it's fine to skip it.\n",
        "\"\"\"\n",
        "def tokenize_sents(example, cols):\n",
        "  if len(cols)==1:\n",
        "    example['input_ids'] = hf_fast_tokenizer.convert_tokens_to_ids(hf_fast_tokenizer.tokenize(f\"[CLS] {example[cols[0]]} [SEP]\"))\n",
        "  elif len(cols)==2:\n",
        "    example['input_ids'] = concat(hf_fast_tokenizer.convert_tokens_to_ids(hf_fast_tokenizer.tokenize(f'[CLS] {example[cols[0]]} [SEP]')),\n",
        "                                  hf_fast_tokenizer.convert_tokens_to_ids(hf_fast_tokenizer.tokenize(f'{example[cols[1]]} [SEP]')))\n",
        "  else: raise ValueError()\n",
        "  return example\n",
        "\n",
        "glue_dsets = {}\n",
        "for glue_task in ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', 'qnli', 'rte', 'wnli', 'ax']:\n",
        "  task = nlp.load_dataset('glue', glue_task, cache_dir=cache_dir)\n",
        "  glue_dsets[glue_task] = {}\n",
        "  print(f'loading processed datasets of {glue_task} ...')\n",
        "  for split in task.keys():\n",
        "    raw_dataset = task[split]\n",
        "    cache_file = Path(raw_dataset.cache_files[0]['filename']).parent / f'tokenized_{split}.arrow'\n",
        "    if cache_file.exists():dataset = nlp.Dataset.from_file(str(cache_file))\n",
        "    else: dataset = raw_dataset.map(partial(tokenize_sents, cols=textcols(raw_dataset)),\n",
        "                                    cache_file_name=str(cache_file))\n",
        "    glue_dsets[glue_task][split] = dataset "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading processed datasets of cola ...\n",
            "loading processed datasets of sst2 ...\n",
            "loading processed datasets of mrpc ...\n",
            "loading processed datasets of qqp ...\n",
            "loading processed datasets of stsb ...\n",
            "loading processed datasets of mnli ...\n",
            "loading processed datasets of qnli ...\n",
            "loading processed datasets of rte ...\n",
            "loading processed datasets of wnli ...\n",
            "loading processed datasets of ax ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKxetBkqfdgm",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 integration of fastai and huggingface/nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ9vazyOZFIL",
        "colab_type": "text"
      },
      "source": [
        "I don't know how to let `Learner` validate two validation sets every epochs, so I just merged `mnli validation mismatched` and `mnli validation matched`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Oc3ZjYwZGLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HF_MergedDataset():\n",
        "  def __init__(self, *datasets):\n",
        "    self.dsets = datasets\n",
        "  def __len__(self):\n",
        "    return reduce(lambda a,d: a+len(d), self.dsets, 0)\n",
        "  def __getitem__(self, i):\n",
        "    for dset in self.dsets:\n",
        "      if i < len(dset): return dset[i]\n",
        "      else: i -= len(dset)\n",
        "  def set_format(self, type, columns):\n",
        "    for dset in self.dsets: dset.set_format(type, columns)\n",
        "  @property\n",
        "  def cache_files(self):\n",
        "    return concat(*[ds.cache_files for ds in self.dsets])\n",
        "    \n",
        "glue_dsets['mnli']['validation'] = HF_MergedDataset(glue_dsets['mnli']['validation_matched'], glue_dsets['mnli']['validation_mismatched'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcElSf3JZLy7",
        "colab_type": "text"
      },
      "source": [
        "**Novel huggingface/nlp integration**, which mimics `fastai2.data.core.Datasets`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seSUELaTZQBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HF_Dataset():\n",
        "\n",
        "  \"\"\" Inheritance by object composition\n",
        "  I want this class behave like nlp.arrow_dataset.Dataset, and overload some methods. (Inheritance),\n",
        "  But I don't know how to initialize a nlp.arrow_dataset.Dataset with existing Dataset properly and without additional cost such as a new copy.\n",
        "  \n",
        "  So I add every attributes/methods of nlp.arrow_dataset.Dataset, ans pass execution to composed Dataset.\n",
        "  Notice that __init__, __repr__,__getattribute__,__new__ should'nt be added, when doing this I call it Inheritance by object composition\n",
        "  ,otherwise it won't work for the reason I don't know.\n",
        "  \"\"\"\n",
        "  for attr_name, attr in nlp.arrow_dataset.Dataset.__dict__.items():\n",
        "    if attr_name not in ['__init__', '__repr__','__getattribute__','__new__'] + ['__getitem__','__iter__',]:\n",
        "      if callable(attr): exec(f'def {attr_name}(self,*args,**kwargs): return self.dataset.{attr_name}(*args,**kwargs)')\n",
        "      else: exec(f'@property\\ndef {attr_name}(self): return self.dataset.{attr_name}')\n",
        "\n",
        "  def __init__(self, dataset, cols, encode_types, decode_funcs, decode_types):\n",
        "    store_attr(self, 'dataset,cols,encode_types,decode_funcs,decode_types')\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    sample = self.dataset[i]\n",
        "    return tuple( enc_type(sample[col]) for col, enc_type in zip(self.cols, self.encode_types))\n",
        "\n",
        "  def __iter__(self):\n",
        "    \"\"\"\n",
        "    default __iter__ will iter until get IndexError, \n",
        "    but ArrowDataset gives you ValueError when out of index.\n",
        "    So we have to explicitly define __iter__ method\n",
        "    \"\"\"\n",
        "    for i in range(len(self)): yield self[i] \n",
        "\n",
        "  def decode(self, o, full=True): return tuple( de_type(de_fc(o_)) for o_,de_fc,de_type in zip(o,self.decode_funcs,self.decode_types))\n",
        "  #def __len__(self): return len(self.dataset)\n",
        "\n",
        "class HF_Datasets(FilteredBase):\n",
        "  def __init__(self, datasets, cols, encode_types, decode_funcs, decode_types):\n",
        "    assert len(cols) == len(decode_funcs) == len(encode_types) == len(decode_types) == len(decode_funcs)\n",
        "    for ds in datasets: ds.set_format(type='torch', columns=cols)\n",
        "    self.datasets = L(HF_Dataset(ds, cols, encode_types, decode_funcs, decode_types) for ds in datasets)\n",
        "  def subset(self, i): return self.datasets[i]\n",
        "  def __getitem__(self, i): return self.datasets[i]\n",
        "  @property\n",
        "  def n_subsets(self): return len(self.datasets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4mAI-scZSQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_decode_fc(x, pretty=True):\n",
        "  if pretty:\n",
        "    return hf_fast_tokenizer.decode([idx for idx in x if idx != hf_fast_tokenizer.pad_token_id])\n",
        "  else:\n",
        "    tokens = hf_fast_tokenizer.convert_ids_to_tokens(x)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "@delegates(FilteredBase.dataloaders)\n",
        "def get_glue_dls(task_name, **kwargs):\n",
        "  assert task_name in ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', 'qnli', 'rte', 'wnli', 'ax']\n",
        "  splits = ['train','validation']+(['test'] if task_name != 'mnli' else ['test_matched', 'test_mismatched'])\n",
        "  if task_name == 'ax': splits = ['test']\n",
        "  arrow_dsets = [glue_dsets[task_name][s] for s in splits ]\n",
        "  show_pretty = kwargs.pop('show_pretty', True) \n",
        "  dsets = HF_Datasets(datasets=arrow_dsets, \n",
        "                      cols=['input_ids', 'label'],\n",
        "                      encode_types=[TensorText, noop],\n",
        "                      decode_funcs=[partial(text_decode_fc,pretty=show_pretty),noop],\n",
        "                      decode_types=[TitledStr, lambda x: Category(x.item())])\n",
        "  dl_cache_files = [Path(dset.cache_files[0]['filename']).parent/f'dl_{s}.pth' for dset, s in zip(dsets,splits)]\n",
        "  if all([ p.exists() for p in dl_cache_files]):\n",
        "    device = kwargs.pop('device', default_device())\n",
        "    dl_s = [TextDataloader.from_cache(f, dsets[i], **kwargs) for i, f in enumerate(dl_cache_files)]  \n",
        "    dls = DataLoaders(*dl_s, device=device)\n",
        "  else:\n",
        "    dls = dsets.dataloaders(before_batch=partial(pad_input_chunk,pad_first=False,pad_idx=hf_fast_tokenizer.pad_token_id,),\n",
        "                             dl_type=partial(TextDataloader, sort_by_len=False),\n",
        "                             **kwargs)\n",
        "    for dl, cache_f in zip(dls,dl_cache_files): dl.cache(cache_f)\n",
        "  return dls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqtDnxK9ZYQP",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Get dataloaders fror each dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VYQubDaZbRv",
        "colab_type": "text"
      },
      "source": [
        "**[CoLA](https://nyu-mll.github.io/CoLA/)** (*The Corpus of Linguistic Acceptability*):\n",
        "\n",
        "\n",
        "Check whether a sentence is linguistically acceptable. \n",
        "\n",
        "(0: unacceptable, 1: acceptable) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhZ3HD7VZalK",
        "colab_type": "code",
        "outputId": "130a4ed2-4276-4ea4-fe55-ce2e3b1063a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "cola_dls = get_glue_dls('cola', show_bar=False)\n",
        "print(f\"Dataset size (train/valid/test): {len(cola_dls[0].dataset)}/{len(cola_dls[1].dataset)}/{len(cola_dls[2].dataset)}\")\n",
        "cola_dls.show_batch(max_n=2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size (train/valid/test): 8551/1043/1063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] our friends won't buy this analysis, let alone the next one we propose. [SEP]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] one more pseudo generalization and i'm giving up. [SEP]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZJ2zGxJZiyq",
        "colab_type": "text"
      },
      "source": [
        "**Note**: for the readibility, we won't show pad and result of sentencepiece ('##...') here, which are the actual results in a batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1zl6tm6Zk02",
        "colab_type": "code",
        "outputId": "44a0c899-3961-4681-b2ba-e0bd1c64a311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "get_glue_dls('cola', show_bar=False, show_pretty=False).show_batch(max_n=2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] one more pseudo general ##ization and i ' m giving up . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STiSJYdbZns1",
        "colab_type": "text"
      },
      "source": [
        "**[SST-2](https://nlp.stanford.edu/sentiment/index.html)** (*The Stanford Sentiment Treebank*): \n",
        "\n",
        "Identify the sentiment of a work/phrase/sentence. \n",
        "\n",
        "(1: positvie, 0: negative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouo5jT9sZnKG",
        "colab_type": "code",
        "outputId": "99f8d2b9-0d3b-4177-c53c-ba9f71829823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "sst2_dls = get_glue_dls('sst2')\n",
        "print(f\"Dataset size (train/valid/test): {len(sst2_dls[0].dataset)}/{len(sst2_dls[1].dataset)}/{len(sst2_dls[2].dataset)}\")\n",
        "sst2_dls.show_batch(max_n=2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size (train/valid/test): 67349/872/1821\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] hide new secretions from the parental units [SEP]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] contains no wit, only labored gags [SEP]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wecP4wm0ZqfX",
        "colab_type": "text"
      },
      "source": [
        "**[MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398)** (*Microsoft Research Paraphrase Corpus*): \n",
        "\n",
        "Whether each pair captures a paraphrase/semantic equivalence relationship. \n",
        "\n",
        "(1: yes, 0: no)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T54EE4WZxgW",
        "colab_type": "code",
        "outputId": "86ced50a-3933-4092-af4f-7e2b617e6b1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "mrpc_dls = get_glue_dls('mrpc',show_bar=False)\n",
        "print(f\"Dataset size (train/valid/test): {len(sst2_dls.train_ds)}/{len(sst2_dls.valid_ds)}/{len(sst2_dls[2])}\")\n",
        "mrpc_dls.show_batch(max_n=2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size (train/valid/test): 67349/872/29\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] amrozi accused his brother, whom he called \" the witness \", of deliberately distorting his evidence. [SEP] referring to him as only \" the witness \", amrozi accused his brother of deliberately distorting his evidence. [SEP]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] yucaipa owned dominick's before selling the chain to safeway in 1998 for $ 2. 5 billion. [SEP] yucaipa bought dominick's in 1995 for $ 693 million and sold it to safeway for $ 1. 8 billion in 1998. [SEP]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znmNswfyZuCb",
        "colab_type": "text"
      },
      "source": [
        "**[STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark)** (*Semantic Textual Similarity Benchmark*):\n",
        "\n",
        "Score the similarity of meanings of two sentences. The only regression task in GLUE \n",
        "\n",
        "(0.0 ~ 5.0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "631vsrFwaDHB",
        "colab_type": "code",
        "outputId": "6dbf5d07-8191-4288-f24f-d90c6f1851d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "stsb_dls = get_glue_dls('stsb',show_bar=False)\n",
        "print(f\"Dataset size (train/valid/test): {len(stsb_dls.train_ds)}/{len(stsb_dls.valid_ds)}/{len(stsb_dls[2])}\")\n",
        "stsb_dls.show_batch(max_n=2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size (train/valid/test): 5749/1500/22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] a plane is taking off. [SEP] an air plane is taking off. [SEP]</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] a man is playing a large flute. [SEP] a man is playing a flute. [SEP]</td>\n",
              "      <td>3.799999952316284</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR_ok10uaCYM",
        "colab_type": "text"
      },
      "source": [
        "**[QQP](https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)** (*Quora Question Pairs*)\n",
        "\n",
        "Check whether two questions are duplicated. \n",
        "\n",
        "(0: no, 1: duplicated)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWQc8402aFJb",
        "colab_type": "code",
        "outputId": "438cb813-e338-47ec-cd82-09179d9ca8ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "qqp_dls = get_glue_dls('qqp')\n",
        "print(f\"Dataset size (train/valid/test): {len(qqp_dls.train_ds)}/{len(qqp_dls.valid_ds)}/{len(qqp_dls[2])}\")\n",
        "qqp_dls.show_batch(max_n=2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size (train/valid/test): 363849/40430/6109\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] how is the life of a math student? could you describe your own experiences? [SEP] which level of prepration is enough for the exam jlpt5? [SEP]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] how do i control my horny emotions? [SEP] how do you control your horniness? [SEP]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnZm1zqEaHHB",
        "colab_type": "text"
      },
      "source": [
        "**[MNLI](https://cims.nyu.edu/~sbowman/multinli/)** (*The Multi-Genre NLI Corpus*)\n",
        "\n",
        "Whether the premise (sentence 1) entails the hypothesis (sentence 2) (entailment), contradicts the hypothesis (contradiction), or neither (neutral) \n",
        "\n",
        "(0: entailment, 1: neutral, 2: contradiction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PRuXBJ8aJTH",
        "colab_type": "code",
        "outputId": "6fdb5792-a1bd-4ff2-8dbf-71cd27bc7161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "mnli_dls = get_glue_dls('mnli')\n",
        "print(f\"Dataset size (train/valid/test_matched/test_mismatched): {len(mnli_dls[0].dataset)}/{len(mnli_dls[1].dataset)}/{len(mnli_dls[2].dataset)}/{len(mnli_dls[3].dataset)}\")\n",
        "mnli_dls.show_batch(max_n=2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size (train/valid/test_matched/test_mismatched): 392702/19647/9796/9847\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the braves decide to call to recall a guy from triple a then a double a guy goes up to replace him and a single a guy goes up to replace him [SEP] you lose the things to the following level if the people recall. [SEP]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJKg-BSkaPN5",
        "colab_type": "text"
      },
      "source": [
        "**QNLI** (*The Stanford Question Answering Dataset*):\n",
        "\n",
        "The task is to determine whether the context sentence (sentence 2) contains the answer to the question (sentence 1).\n",
        "\n",
        "(0: entailment, 1: not_entailment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIK9BAnBaLbj",
        "colab_type": "code",
        "outputId": "bc5d1360-7e2f-4b1b-ef05-382b576bc2c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "qnli_dls = get_glue_dls('qnli')\n",
        "print(f\"Dataset size (train/valid/test): {len(qnli_dls[0].dataset)}/{len(qnli_dls[1].dataset)}/{len(qnli_dls[2].dataset)}\")\n",
        "qnli_dls.show_batch(max_n=2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size (train/valid/test): 104743/5463/5463\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] when did the third digimon series begin? [SEP] unlike the two seasons before it and most of the seasons that followed, digimon tamers takes a darker and more realistic approach to its story featuring digimon who don't reincarnate after their deaths and more complex character development in the original japanese. [SEP]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] which missile batteries often have individual launchers several kilometres from one another? [SEP] when manpads is operated by specialists, batteries may have several dozen teams deploying separately in small sections ; self - propelled air defence guns may deploy in pairs. [SEP]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2OPdiDpaS6v",
        "colab_type": "text"
      },
      "source": [
        "**[RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment)** (*Recognizing_Textual_Entailment*):\n",
        "\n",
        "Whether hypothesis (sentence 2) is entailed (can be inferred) from the premise (sentence 1).\n",
        "\n",
        "(0: entailment, 1: not_entailment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cODdx4WyaVRh",
        "colab_type": "code",
        "outputId": "ee1601b2-5357-450d-df4e-6476aa010577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "rte_dls = get_glue_dls('rte', show_bar=False)\n",
        "print(f\"Dataset size (train/valid/test): {len(rte_dls[0].dataset)}/{len(rte_dls[1].dataset)}/{len(rte_dls[2].dataset)}\")\n",
        "rte_dls.show_batch(max_n=2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size (train/valid/test): 2490/277/3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] no weapons of mass destruction found in iraq yet. [SEP] weapons of mass destruction found in iraq. [SEP]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] a place of sorrow, after pope john paul ii died, became a place of celebration, as roman catholic faithful gathered in downtown chicago to mark the installation of new pope benedict xvi. [SEP] pope benedict xvi is the new leader of the roman catholic church. [SEP]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-iIX2EaaUt-",
        "colab_type": "text"
      },
      "source": [
        "**[WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html)** (*The Winograd Schema Challenge*)\n",
        "\n",
        "Check whether sentence 2 (which is rephrased sentence of sentence 1) correctly solve the pronoun in sentence 1.\n",
        "\n",
        "(0: wrong, 1: correct)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7wGvrihaZAj",
        "colab_type": "code",
        "outputId": "d2979865-81b5-44e9-fad3-f217105f6ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "wnli_dls = get_glue_dls('wnli', show_bar=False)\n",
        "print(f\"Dataset size (train/valid/test): {len(wnli_dls[0].dataset)}/{len(wnli_dls[1].dataset)}/{len(wnli_dls[2].dataset)}\")\n",
        "wnli_dls.show_batch(max_n=2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size (train/valid/test): 635/71/146\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] i stuck a pin through a carrot. when i pulled the pin out, it had a hole. [SEP] the carrot had a hole. [SEP]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] john couldn't see the stage with billy in front of him because he is so short. [SEP] john is so short. [SEP]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RQ-BEaIaa8F",
        "colab_type": "text"
      },
      "source": [
        "**[AX](https://gluebenchmark.com/diagnostics)** (*GLUE Diagnostic Dataset*):\n",
        "\n",
        "Whether the premise (sentence 1) entails the hypothesis (sentence 2) (entailment), contradicts the hypothesis (contradiction), or neither (neutral) \n",
        "\n",
        "Test set only.\n",
        "\n",
        "The label is all -1, because this is a test dataset and the answers is kept by GLUE benchmark and not provided. The categories are the same as MNLI (entailment, neutral, contradiction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IKH0iJKadAW",
        "colab_type": "code",
        "outputId": "1f3f5bbb-d278-4adf-aab5-e66985d2a965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "ax_dls = get_glue_dls('ax')\n",
        "print(f\"Dataset size (test): {len(ax_dls[0].dataset)}\")\n",
        "ax_dls.show_batch(max_n=2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size (test): 1104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] the cat sat on the mat. [SEP] the cat did not sit on the mat. [SEP]</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] the cat did not sit on the mat. [SEP] the cat sat on the mat. [SEP]</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wRQS_H1aekI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glue_dls = {}\n",
        "for task_name in ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', 'qnli', 'rte', 'wnli', 'ax']:\n",
        "  dls = eval(f\"{task_name}_dls\")\n",
        "  if task_name != 'ax':\n",
        "    dls[0].desc_sort(); dls[1].desc_sort(); # sort by len to reduce pad, but not for test_dl which keep the order of sample to let us add the dataset idx conveniently later \n",
        "  glue_dls[task_name] = dls\n",
        "glue_dls['ax'][0].shuffle = False # fastai see 0th dl as train_dl which is default shuffled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFcM5MGaafFJ",
        "colab_type": "text"
      },
      "source": [
        "# 2. Finetuning model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUGxkcDfQGxS",
        "colab_type": "text"
      },
      "source": [
        "* ELECTRA use CLS encodings as pooled result to predict the sentence. (see [here](https://github.com/google-research/electra/blob/79111328070e491b287c307906701ebc61091eb2/model/modeling.py#L254) of its official repository)\n",
        "\n",
        "* Note that we should use different prediction head for different tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF6vajgKQKJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentencePredictHead(nn.Module):\n",
        "  \"The way that Electra and Bert do for sentence prediction task\"\n",
        "  def __init__(self, hidden_size, targ_voc_size):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(hidden_size, targ_voc_size)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "  def forward(self, x):\n",
        "    \"x: (batch size, sequence length, hidden_size)\"\n",
        "    return self.linear(self.dropout(x[:,0])) # project the first token (a special token)'s hidden encoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRoFPXjpe4Nb",
        "colab_type": "text"
      },
      "source": [
        "`SentencePredictHead`\n",
        "* change `targ_voc_size` for diffrent task\n",
        "* change `256` for your model's hidden size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3_2P-uZV8sb",
        "colab_type": "code",
        "outputId": "dc6ad9cb-7890-4050-d638-26fba48dcf86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "single_task_model = nn.Sequential(HFModelWrapper(base_model, pad_id=hf_tokenizer.pad_token_id, sep_id=hf_tokenizer.sep_token_id), # specify spe_id and wrapper will create and pass token_type_ids of sentece A/B for you\n",
        "                                  SentencePredictHead(256, targ_voc_size=2)) # <- change targ_voc_size according to the task\n",
        "inp_tensor = torch.tensor(hf_tokenizer.encode('I am the sentence A','I am the sentence B'))[None,:] \n",
        "print(f'input shape: {inp_tensor.shape} (batch size, sequence length)')\n",
        "with torch.no_grad(): out_tensor = single_task_model(inp_tensor)\n",
        "print(f'output shape: {out_tensor.shape} (batch_size, target vocab size)')\n",
        "print(f'The output is the raw score-like thing for each label')\n",
        "print(out_tensor)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape: torch.Size([1, 13]) (batch size, sequence length)\n",
            "output shape: torch.Size([1, 2]) (batch_size, target vocab size)\n",
            "The output is the raw score-like thing for each label\n",
            "tensor([[-0.0409, -0.5393]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi5_OKHqX8qw",
        "colab_type": "text"
      },
      "source": [
        "# 3. Single Task Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gebKI3-ywWNS",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 The metrics problem\n",
        "- We need single label Matthew's Correlation\n",
        "- We need Pearson Correlation, Spearman Correlation\n",
        "- We need `AccumMetric` to be able to grab the element of return of metric function if it return a collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2tS0DASwUUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_item(container, i):\n",
        "  if isinstance(i, int):\n",
        "    return container[i]\n",
        "  elif isinstance(i, str):\n",
        "    return getattr(container, i)\n",
        "  else:\n",
        "    TypeError(f'{type(i)} is not legal.')\n",
        "\n",
        "# add `get` cuz scipy spearmanr returns tuple-like object, I need to extract only correlation\n",
        "class MyAccumMetric(AccumMetric):\n",
        "  def __init__(self, func, get=None, **kwargs):\n",
        "      super().__init__(func, **kwargs)\n",
        "      self.get =  None if get is None else partial(get_item, i=get)\n",
        "  \n",
        "  @property\n",
        "  def value(self):\n",
        "      if len(self.preds) == 0: return\n",
        "      preds,targs = torch.cat(self.preds),torch.cat(self.targs)\n",
        "      if self.to_np: preds,targs = preds.numpy(),targs.numpy()\n",
        "      out = self.func(targs, preds, **self.kwargs) if self.invert_args else self.func(preds, targs, **self.kwargs)\n",
        "      return self.get(out) if self.get else out\n",
        "\n",
        "# dim -> dim_argmax cuz scipy spearman use axis\n",
        "def scim_to_fastai(func, invert_xy, get=None, is_class=True, thresh=None, dim_argmax=-1, sigmoid=None, **kwargs):\n",
        "    \"Wrap metrics of scipy scikit_learn as fastai's metric\"\n",
        "    dim_argmax = dim_argmax if is_class and thresh is None else None\n",
        "    sigmoid = sigmoid if sigmoid is not None else (is_class and thresh is not None)\n",
        "    return MyAccumMetric(func, dim_argmax=dim_argmax, sigmoid=sigmoid, thresh=thresh,\n",
        "                       to_np=True, invert_arg=invert_xy, get=get, **kwargs)\n",
        "\n",
        "\"\"\"\n",
        "There is bug in scikit_learn (https://github.com/scikit-learn/scikit-learn/issues/16924)\n",
        "so you always get `RuntimeWarning: invalid value encountered in double_scalar` and get a value 0.0\n",
        "\"\"\"\n",
        "def MatthewsCorrCoef(dim_argmax=-1, sample_weight=None):\n",
        "    \"Matthews correlation coefficient for single-label classification problems\"\n",
        "    return scim_to_fastai(skm.matthews_corrcoef, invert_xy=True, dim_argmax=dim_argmax, flatten=True, sample_weight=sample_weight)\n",
        "\n",
        "\"\"\"\n",
        "If you see PearsonRConstantInputWarning, that may mean the model always outputs a specific label,\n",
        "which is probable when you test something with very small dataset size or very short training\n",
        "\"\"\"\n",
        "# metric for STS task\n",
        "def PearsonCorrCoef(dim_argmax=-1):\n",
        "    \"Pearson correlation coefficient for single-label classification problems\"\n",
        "    return scim_to_fastai(spm.pearsonr, invert_xy=False, get=0, dim_argmax=dim_argmax, flatten=True,)\n",
        "\n",
        "\"\"\"\n",
        "For the same reason as Pearson correlation, you may see nan for value of Spearman correlation, and\n",
        "RuntimeWarning: invalid value encountered in true_divide, because there is no mutual change. \n",
        "(see https://stackoverflow.com/questions/45897003/python-numpy-corrcoef-runtimewarning-invalid-value-encountered-in-true-divide)\n",
        "\"\"\"\n",
        "# metric for STS task\n",
        "def SpearmanCorrCoef(dim_argmax=-1, axis=0, nan_policy='propagate'):\n",
        "    \"Spearman correlation coefficient for single-label classification problems\"\n",
        "    return scim_to_fastai(spm.spearmanr, invert_xy=False, get=0, dim_argmax=dim_argmax, flatten=True, axis=axis, nan_policy=nan_policy)\n",
        "\n",
        "\"\"\"\n",
        "I would like more uniform way to pass the metrics, no matter loss_func or metric,\n",
        "instantiate it and then pass.\n",
        "This uniform way also make it possible such as `metrics=[m() for m inTASK_METRICS[task]]`\n",
        "\"\"\"\n",
        "def Accuracy(axis=-1):\n",
        "  return AvgMetric(partial(accuracy, axis=axis))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mcjq0b8wbkO",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 finetune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK1tdI9HY63y",
        "colab_type": "text"
      },
      "source": [
        "Normally we do multi-task learning on all GLUE tasks first, and then fit each task individually.\n",
        "\n",
        "And also we train for a short time, so you don't need to care about the accuracy (or other metrics) is low."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLKItnWAd0a3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "single_task_learn = Learner(glue_dls['mrpc'], single_task_model,\n",
        "                            # flatten only if needed, here in fact we don't need\n",
        "                            # if regression task (STS-B), use MSELossFlat() instead\n",
        "                            loss_func=CrossEntropyLossFlat(), \n",
        "                            metrics=[F1Score(), Accuracy()]).to_fp16()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJCsfhUHwZKl",
        "colab_type": "code",
        "outputId": "88642286-d9c2-41fa-f0d6-f8a2cc89fcd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "lr, _ = single_task_learn.lr_find()                \n",
        "single_task_learn.fit_one_cycle(n_epoch=1, lr_max=lr)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.749408</td>\n",
              "      <td>0.631977</td>\n",
              "      <td>0.812227</td>\n",
              "      <td>0.683824</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c73X4fdrmnlP",
        "colab_type": "text"
      },
      "source": [
        "# 3.3 Predict the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYMsFM0Cmp1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "46a0ea1e-f5c9-4a85-dd99-7f83ecd02d37"
      },
      "source": [
        "test_pred_dir = cache_dir/'glue'/'test'\n",
        "test_pred_dir.mkdir(exist_ok=True)\n",
        "preds = single_task_learn.get_preds(dl=glue_dls['mrpc'][2], with_decoded=True)\n",
        "preds = preds[-1] # preds -> (predictions logits, targets, decoded prediction)\n",
        "cols = ['idx', *textcols(glue_dsets['mrpc']['test'])]\n",
        "test_df = pd.DataFrame( data={**{col:glue_dsets['mrpc']['test'][col] for col in cols}, \n",
        "                             'prediction': preds.tolist()} )\n",
        "test_df.to_csv( test_pred_dir/'MRPC.tsv', sep='\\t' )\n",
        "test_df.head(n=3)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>PCCW 's chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So .</td>\n",
              "      <td>Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>The world 's two largest automakers said their U.S. sales declined more than predicted last month as a late summer sales frenzy caused more of an industry backlash than expected .</td>\n",
              "      <td>Domestic sales at both GM and No. 2 Ford Motor Co. declined more than predicted as a late summer sales frenzy prompted a larger-than-expected industry backlash .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>According to the federal Centers for Disease Control and Prevention ( news - web sites ) , there were 19 reported cases of measles in the United States in 2002 .</td>\n",
              "      <td>The Centers for Disease Control and Prevention said there were 19 reported cases of measles in the United States in 2002 .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   idx  ... prediction\n",
              "0    0  ...          1\n",
              "1    1  ...          1\n",
              "2    2  ...          1\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mdBGfiZK2Hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}