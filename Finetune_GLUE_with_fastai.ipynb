{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ugFdx5v_YxzA",
    "outputId": "e6663d66-4c4b-4953-8388-7fa2b23b126a"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "from IPython.core.debugger import set_trace as bk\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import nlp\n",
    "from transformers import ElectraModel, ElectraConfig, ElectraTokenizerFast, ElectraForPreTraining\n",
    "from fastai2.text.all import *\n",
    "import wandb\n",
    "from fastai2.callback.wandb import WandbCallback\n",
    "from _utils.would_like_to_pr import *\n",
    "from _utils.huggingface import *\n",
    "from _utils.wsc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SIZE = 'small'\n",
    "assert SIZE in ['small', 'base', 'large']\n",
    "hf_tokenizer = ElectraTokenizerFast.from_pretrained(f\"google/electra-{SIZE}-discriminator\")\n",
    "electra_config = ElectraConfig.from_pretrained(f'google/electra-{SIZE}-discriminator')\n",
    "CONFIG = {\n",
    "  'lr': [3e-4, 1e-4, 5e-5],\n",
    "  'layer_lr_decay': [0.8,0.8,0.9],\n",
    "}\n",
    "I = ['small', 'base', 'large'].index(SIZE)\n",
    "config = {k:vals[I] for k,vals in CONFIG.items()}\n",
    "\n",
    "config.update({\n",
    "  'max_length': 512,\n",
    "  'use_wsc': False,\n",
    "  'use_fp16': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yoI6QffPY6Jv"
   },
   "source": [
    "# 1. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = Path.home()/'datasets'\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_P8P4hffUr8"
   },
   "source": [
    "## 1.1 Download and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "ae0863b074ff4a1a959514881fe2802f",
      "e5d5d86746ec4e2298028c57ef775d84",
      "a8b32129b9ab41d0895cb1ec445db9ca",
      "2d26c89a3d97493e911c52fa20cf79d9",
      "65eee3543a9d46adbc8037995c3a69a0",
      "9fc29a98d6be46e0974d1f0f7681d5c3",
      "5b8f1e5172204ed58bd60972289b24a8",
      "bd8da3c0b2784f2094d3b0cfa744cf87",
      "f74af04bf4764b7888f0a45e37ce85c1",
      "f35afd9d13c240aba4e0619cca0199b7",
      "a07124360f4c4249a745891d83e719f6",
      "f26a4f6d9334465d9022ab2a3401035b",
      "007556af2af8413ca47aa5f7f725e68a",
      "8d4771f53c7046f1afe7e04a3a647a31",
      "71beed6de71c4aa2862ef33e21cc58bc",
      "e1d8bdd9c1a74bc18a103bc900589c2c"
     ]
    },
    "colab_type": "code",
    "id": "U-PLcDIRY6y-",
    "outputId": "12e31f13-b70f-48c8-dd45-c6f1718b03c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def textcols(task):\n",
    "  \"Infer text cols of different GLUE datasets in huggingface/nlp\"\n",
    "  if task in ['qnli']: return ['question', 'sentence']\n",
    "  elif task in ['mrpc','stsb','wnli','rte']: return ['sentence1', 'sentence2']\n",
    "  elif task in ['qqp']: return ['question1','question2']\n",
    "  elif task in ['mnli','ax']: return ['premise','hypothesis']\n",
    "  elif task in ['cola','sst2']: return ['sentence']\n",
    "\n",
    "def tokenize_sents(example, cols):\n",
    "  example['inp_ids'] = hf_tokenizer.encode(*[ example[c] for c in cols])\n",
    "  return example\n",
    "\n",
    "def tokenize_sents_max_len(example, cols, max_length):\n",
    "  # Follow BERT and ELECTRA, we truncate examples longer than max length, see https://github.com/google-research/electra/blob/79111328070e491b287c307906701ebc61091eb2/finetune/classification/classification_tasks.py#L296\n",
    "  tokens_a = hf_tokenizer.tokenize(example[cols[0]])\n",
    "  tokens_b = hf_tokenizer.tokenize(example[cols[1]]) if len(cols)==2 else []\n",
    "  _max_length = max_length - 1 - len(cols) # preserved for cls and sep tokens\n",
    "  while True:\n",
    "    total_length = len(tokens_a) + len(tokens_b)\n",
    "    if total_length <= _max_length:\n",
    "      break\n",
    "    if len(tokens_a) > len(tokens_b):\n",
    "      tokens_a.pop()\n",
    "    else:\n",
    "      tokens_b.pop()\n",
    "  tokens = [hf_tokenizer.cls_token, *tokens_a, hf_tokenizer.sep_token]\n",
    "  if tokens_b: tokens += [*tokens_b, hf_tokenizer.sep_token]\n",
    "  example['inp_ids'] = hf_tokenizer.convert_tokens_to_ids(tokens)\n",
    "  return example\n",
    "\n",
    "# get tokenized datasets and dataloaders\n",
    "glue_dsets = {}; glue_dls = {}\n",
    "for task in ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', 'qnli', 'rte', 'wnli', 'ax']:\n",
    "  # General case and special case for WSC\n",
    "  if task == 'wnli' and config['use_wsc']:\n",
    "    benchmark, subtask = 'super_glue', 'wsc.fixed'\n",
    "    # samples in all splits are all less than 128-2, so don't need to worry about max_length\n",
    "    Tfm = partial(WSCTransform, hf_toker=hf_tokenizer)\n",
    "    cols = {'inp_ids':TensorText, 'span': noop, 'label':TensorCategory}\n",
    "    n_inp=2\n",
    "    cache_name = \"tokenized_{split}.arrow\"\n",
    "  else:\n",
    "    benchmark, subtask = 'glue', task\n",
    "    tok_func = partial(tokenize_sents_max_len, cols=textcols(task), max_length=config['max_length'])\n",
    "    Tfm = partial(HF_Transform, func=tok_func)\n",
    "    cols = ['inp_ids', 'label']\n",
    "    n_inp=1\n",
    "    cache_name = f\"tokenized_{config['max_length']}_{{split}}.arrow\"\n",
    "  # load / download datasets.\n",
    "  dsets = nlp.load_dataset(benchmark, subtask, cache_dir=cache_dir)\n",
    "  # There is two samples broken in QQP training set\n",
    "  if task=='qqp': dsets['train'] = dsets['train'].filter(lambda e: e['question2']!='',\n",
    "                                          cache_file_name=str(cache_dir/'glue/qqp/1.0.0/fixed_train.arrow'))\n",
    "  # load / make tokenized datasets\n",
    "  glue_dsets[task] = Tfm(dsets).map(cache_name=cache_name)\n",
    "  # load / make dataloaders\n",
    "  hf_dsets = HF_Datasets(glue_dsets[task], cols=cols, hf_toker=hf_tokenizer, n_inp=n_inp)\n",
    "  dl_cache_name = cache_name.replace('tokenized', 'dl').replace('.arrow', '.json')\n",
    "  glue_dls[task] = hf_dsets.dataloaders(bs=32, pad_idx=hf_tokenizer.pad_token_id, cache_name=dl_cache_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MqtDnxK9ZYQP"
   },
   "source": [
    "## 1.2 View Data\n",
    "- View raw data on [nlp-viewer]! (https://huggingface.co/nlp/viewer/)\n",
    "\n",
    "- View task description on Tensorflow dataset doc for GLUE (https://www.tensorflow.org/datasets/catalog/glue) \n",
    "\n",
    "- You may notice some text without \\[SEP\\], that is because the whole sentence is truncated by `show_batch`, you can turn it off by specify `truncated_at=None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VYQubDaZbRv"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "HhZ3HD7VZalK",
    "outputId": "7ea5edbf-6a70-4f5a-9b85-c033e7aef666",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 8551/1043/1063\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] everybody who has ever , worked in any office which contained any type ##writer which had ever been used to type any letters which had to be signed by any administrator who ever worked in any department like mine will know what i mean . [SEP]</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# CoLA (The Corpus of Linguistic Acceptability) - 0: unacceptable, 1: acceptable \n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['cola'].loaders]))\n",
    "glue_dls['cola'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZJ2zGxJZiyq"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "STiSJYdbZns1"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "ouo5jT9sZnKG",
    "outputId": "ef107490-d6b8-4551-9028-33cc299fc674",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 67349/872/1821\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] even if the en ##tic ##ing prospect of a lot of nu ##bil ##e young actors in a film about campus de ##pr ##avi ##ty did n ' t fade amid the deliberate , tires ##ome u ##gli ##ness , it would be rendered ted ##ious by ava ##ry ' s failure to construct a story with even a trace of dramatic interest . [SEP]</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# SST-2 (The Stanford Sentiment Treebank) - 1: positvie, 0: negative\n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['sst2'].loaders]))\n",
    "glue_dls['sst2'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "6T54EE4WZxgW",
    "outputId": "98e94bdb-324d-4d80-b367-7f4f6da2efb1",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 3668/408/1725\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] with cl ##ari ##tin ' s decline , sc ##hering - pl ##ough ' s best - selling products now are two drugs used together to treat hepatitis c , the anti ##vira ##l pill rib ##avi ##rin and an inter ##fer ##on medicine called peg - intro ##n . [SEP] with cl ##ari ##tin ' s decline , sc ##hering - pl ##ough ' s best - selling products are now anti ##vira ##l drug rib ##avi ##rin and an inter ##fer ##on medicine called peg - intro ##n - - two drugs used together to treat hepatitis c . [SEP]</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# MRPC (Microsoft Research Paraphrase Corpus) -  1: match, 0: no\n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['mrpc'].loaders]))\n",
    "glue_dls['mrpc'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "znmNswfyZuCb"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "631vsrFwaDHB",
    "outputId": "380a5db6-b00b-49aa-a5d7-7d4cedba576e",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 5749/1500/1379\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] iraq has been lobbying for the security council to stop using the country ' s oil revenue to pay compensation to victims of the 1991 gulf war and the salaries of the united nations monitoring , verification and inspection commission inspectors and to have all money remaining in the united nation ' s oil - for - food accounts transferred to the government ' s development fund . [SEP] iraq ' s new leaders have been lobbying for the united nations security council to stop using the iraq ' s oil revenue to pay the salaries of the inspectors and to have all money remaining in the united nation ' s oil - for - food account transferred to the iraqi government . [SEP]</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# STS-B (Semantic Textual Similarity Benchmark) - 0.0 ~ 5.0\n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['stsb'].loaders]))\n",
    "glue_dls['stsb'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "hWQc8402aFJb",
    "outputId": "8a4e6617-47e4-4a92-bc56-481730a16088",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 363847/40430/390965\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] i ' m in 12 steps program . been sober for 4 years and counting . i ' ve read about l ##sd and want to try it , if i used it once , would that be a re ##la ##pse ? [SEP] heartbreak ? heartbreak ? she ' s my girlfriend for two months , i chose her over my girlfriend for 2 years . i like her so much to the point that i can ' t let her go even if she wants to end our relationship because of the other people around us most especially her family . i do the things for her that i ' m not used to for a girl and i am willing to sacrifice everything just to have a little time with her . a little and limited time that i ' m asking from her but she</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# QQP (Quora Question Pairs) - 0: no, 1: duplicated\n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['qqp'].loaders]))\n",
    "glue_dls['qqp'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "3PRuXBJ8aJTH",
    "outputId": "89fd1479-796c-43ef-ee71-292ac6ed4bad",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/validation_matched/validation_mismatched/test_matched/test_mismatched): 392702/9815/9832/9796/9847\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] well uh that ' s kind of obvious i mean they ' re even carrying it to to where now uh that they ad ##vert ##ise on tv you know if your if you uh you know have done this or if you need this uh uh we ' ll sue for you and you don ' t have to pay us unless you but then what they don ' t tell you is that if you if they win you give them at least a third of the of the thing that they win so i don ' t know it is uh it ' s getting to be more business now rather than uh actually uh dealing with the crime than with uh um the uh punishment they the the lawyers are just in it for the money i ' m i ' m convinced i know</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# MNLI (The Multi-Genre NLI Corpus) - 0: entailment, 1: neutral, 2: contradiction\n",
    "print(\"Dataset size (train/validation_matched/validation_mismatched/test_matched/test_mismatched): {}/{}/{}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['mnli'].loaders]))\n",
    "glue_dls['mnli'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "DIK9BAnBaLbj",
    "outputId": "62ee8573-b4f6-4274-9592-44920c010674",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 104743/5463/5463\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] what time magazine founder attended yale ? [SEP] among the best - known are u . s . presidents william howard taft , gerald ford , george h . w . bush , bill clinton and george w . bush ; royals crown princess victoria bern ##ado ##tte , prince ro ##stis ##lav romano ##v and prince ak ##ii ##ki hose ##a ny ##ab ##ong ##o ; heads of state , including italian prime minister mario mont ##i , turkish prime minister tan ##su ci ##ller , mexican president ernesto ze ##di ##llo , german president karl cars ##tens , and philippines president jose pac ##iano laurel ; u . s . supreme court justices sonia soto ##may ##or , samuel ali ##to and clarence thomas ; u . s . secretaries of state john kerry , hillary clinton , cyrus vance , and dean ache ##son ; authors</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# QNLI (The Stanford Question Answering Dataset) - 0: entailment, 1: not_entailment\n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['qnli'].loaders]))\n",
    "glue_dls['qnli'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "cODdx4WyaVRh",
    "outputId": "ee44b782-8da0-42a8-ef6f-b17a841679df",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 2490/277/3000\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] sweden has plans to open a virtual embassy in second life , the virtual world home to thousands of net ##ize ##ns . companies like dell are already selling computers via the virtual world , but sweden would become the first country to have a cyber - embassy in second life . the embassy would not provide visa or perform diplomatic tasks , but would provide information on how and where to get these documents in the real world , as well as giving cultural and tourist information about the country . visitors will also be able to chat with embassy personnel . a spokesperson explained that it would be an easy and cheap way to reach young people . the idea came from the swedish institute , an agency of the foreign affairs ministry of sweden with providing information about sweden as a key purpose . the \"</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# RTE (Recognizing_Textual_Entailment) - 0: entailment, 1: not_entailment\n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['rte'].loaders]))\n",
    "glue_dls['rte'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 635/71/146\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] ta ##tya ##na knew that grandma always enjoyed serving an abundance of food to her guests . now ta ##tya ##na watched as grandma gathered ta ##tya ##na ' s small mother into a wide , sc ##ra ##wny embrace and then propelled her to the table , lifting her shaw ##l from her shoulders , seating her in the place of honor , and saying simply : \" there ' s plenty . \" [SEP] grandma gathered ta ##tya ##na ' s small mother into a wide , sc ##ra ##wny embrace and then propelled ta ##tya ##na ' s mother to the table . [SEP]</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# WSC (The Winograd Schema Challenge) - 0: wrong, 1: correct\n",
    "# There are three style, WNLI (casted in NLI type), WSC, WSC with candidates (trick used by Roberta)\n",
    "\"\"\" Note for WSC trick\n",
    "- haven't figured out when use WNLI style how to make headers so show_batch right.\n",
    "- lines are prefix, suffix, cands, cand_lens, label in order\n",
    "- cands is the concatenation of candidates, cand_lens is the lengths of candidates in order.\n",
    "\"\"\"\n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['wnli'].loaders]))\n",
    "glue_dls['wnli'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "5IKH0iJKadAW",
    "outputId": "0c5935c4-608f-426e-f3d0-9d380df14fa8",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (test): 1104\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] we manually ann ##ota ##ted 68 ##7 template ##s mapping kb pre ##dicate ##s to text for different composition ##ality types ( with 46 ##2 unique kb pre ##dicate ##s ) , and use those template ##s to modify the original web ##quest ##ions ##sp question according to the meaning of the generated spa ##r ##q ##l query . [SEP] we manually ann ##ota ##ted over 650 template ##s mapping kb pre ##dicate ##s to text for different composition ##ality types ( with 46 ##2 unique kb pre ##dicate ##s ) , and use those template ##s to modify the original web ##quest ##ions ##sp question according to the meaning of the generated spa ##r ##q ##l query . [SEP]</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# AX (GLUE Diagnostic Dataset) - 0: entailment, 1: neutral, 2: contradiction\n",
    "print(\"Dataset size (test): {}\".format(*[len(dl.dataset) for dl in glue_dls['ax'].loaders]))\n",
    "glue_dls['ax'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFcM5MGaafFJ"
   },
   "source": [
    "# 2. Finetuning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tUGxkcDfQGxS"
   },
   "source": [
    "* ELECTRA use CLS encodings as pooled result to predict the sentence. (see [here](https://github.com/google-research/electra/blob/79111328070e491b287c307906701ebc61091eb2/model/modeling.py#L254) of its official repository)\n",
    "\n",
    "* Note that we should use different prediction head instance for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MF6vajgKQKJ7"
   },
   "outputs": [],
   "source": [
    "class SentencePredictHead(nn.Module):\n",
    "  \"The way that Electra and Bert do for sentence prediction task\"\n",
    "  def __init__(self, hidden_size, targ_voc_size):\n",
    "    super().__init__()\n",
    "    self.linear = nn.Linear(hidden_size, targ_voc_size)\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "  def forward(self, x):\n",
    "    \"x: (batch size, sequence length, hidden_size)\"\n",
    "    # project the first token (a special token)'s hidden encoding\n",
    "    return self.linear(self.dropout(x[:,0])).squeeze(-1) # if regression task, squeeze to (B), else (B,#class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zi5_OKHqX8qw"
   },
   "source": [
    "# 3. Single Task Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "poNJYRwETtBo"
   },
   "source": [
    "## 3.1 Discriminative learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OyT-hy-7FXz"
   },
   "outputs": [],
   "source": [
    "# Names come from, for nm in model.named_modules(): print(nm[0])\n",
    "\n",
    "def hf_electra_param_splitter(model, num_hidden_layers, outlayer_name):\n",
    "  names = ['.embeddings', *[f'encoder.layer.{i}' for i in range(num_hidden_layers)], outlayer_name]\n",
    "  def end_with_any(name): return any( name.endswith(n) for n in names )\n",
    "  groups = [ list(mod.parameters()) for name, mod in model.named_modules() if end_with_any(name) ]\n",
    "  assert len(groups) == len(names)\n",
    "  return groups\n",
    "\n",
    "def get_layer_lrs(lr, decay_rate_of_depth, num_hidden_layers):\n",
    "  # I think input layer as bottom and output layer as top, which make 'depth' mean different from the one of official repo \n",
    "  return [ lr * (decay_rate_of_depth ** depth) for depth in reversed(range(num_hidden_layers+2))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N2ug-bscA9gC"
   },
   "source": [
    "## 3.2 Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nzqFhWQkBGhu"
   },
   "outputs": [],
   "source": [
    "def linear_warmup_and_decay(pct_now, lr_max, end_lr, decay_power, warmup_pct, total_steps):\n",
    "  \"\"\"\n",
    "  end_lr: the end learning rate for linear decay\n",
    "  warmup_pct: percentage of training steps to for linear increase\n",
    "  pct_now: percentage of traning steps we have gone through, notice pct_now=0.0 when calculating lr for first batch.\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  pct updated after_batch, but global_step (in tf) seems to update before optimizer step,\n",
    "  so pct is actually (global_step -1)/total_steps \n",
    "  \"\"\"\n",
    "  fixed_pct_now = pct_now + 1/total_steps\n",
    "  \"\"\"\n",
    "  According to source code of the official repository, it seems they merged two lr schedule (warmup and linear decay)\n",
    "  sequentially, instead of split training into two phases for each, this might because they think when in the early\n",
    "  phase of training, pct is low, and thus the decaying formula makes little difference to lr.\n",
    "  \"\"\"\n",
    "  decayed_lr = (lr_max-end_lr) * (1-fixed_pct_now)**decay_power + end_lr # https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/polynomial_decay\n",
    "  warmed_lr = decayed_lr * min(1.0, fixed_pct_now / warmup_pct) # https://github.com/google-research/electra/blob/81f7e5fc98b0ad8bfd20b641aa8bc9e6ac00c8eb/model/optimization.py#L44\n",
    "  return warmed_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Mcjq0b8wbkO"
   },
   "source": [
    "## 3.3 finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = {\n",
    "  **{ task:['MatthewsCorrCoef'] for task in ['cola']},\n",
    "  **{ task:['Accuracy'] for task in ['sst2', 'mnli', 'qnli', 'rte', 'wnli', 'snli','ax']},\n",
    "  # Note: MRPC and QQP are both binary classification problem, so we can just use fastai's default\n",
    "  # average option 'binary' without spcification of average method.\n",
    "  **{ task:['F1Score', 'Accuracy'] for task in ['mrpc', 'qqp']}, \n",
    "  **{ task:['PearsonCorrCoef', 'SpearmanCorrCoef'] for task in ['stsb']}\n",
    "}\n",
    "TARG_VOC_SIZE = {\n",
    "    **{ task:1 for task in ['stsb']},\n",
    "    **{ task:2 for task in ['cola', 'sst2', 'mrpc', 'qqp', 'qnli', 'rte', 'wnli']},\n",
    "    **{ task:3 for task in ['mnli','ax']}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSELossFlat(BaseLoss):\n",
    "\n",
    "  def __init__(self,*args, axis=-1, floatify=True, low=None, high=None, **kwargs):\n",
    "    super().__init__(nn.MSELoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)\n",
    "    self.low, self.high = low, high\n",
    "\n",
    "  def decodes(self, x):\n",
    "    if self.low is not None: x = torch.max(x, x.new_full(x.shape, self.low))\n",
    "    if self.high is not None: x = torch.min(x, x.new_full(x.shape, self.high))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glue_learner(task, one_cycle=False, device='cuda:0', run_name=None, checkpoint=None):\n",
    "  \n",
    "  # num_epochs\n",
    "  if task == 'rte': num_epochs = 10\n",
    "  else: num_epochs = 3\n",
    "\n",
    "  # dls\n",
    "  dls = glue_dls[task].to(torch.device(device))\n",
    "  # model\n",
    "  if task=='wnli' and config['use_wsc']:\n",
    "    model = ELECTRAWSCModel(HF_Model(ElectraForPreTraining, f\"google/electra-{SIZE}-discriminator\", hf_tokenizer))\n",
    "  else:\n",
    "    model = nn.Sequential(HF_Model(ElectraModel, f\"google/electra-{SIZE}-discriminator\", hf_tokenizer),\n",
    "                          SentencePredictHead(electra_config.hidden_size, targ_voc_size=TARG_VOC_SIZE[task]))\n",
    "  # loss func\n",
    "  if task == 'stsb': loss_fc = MyMSELossFlat(low=0.0, high=5.0)\n",
    "  elif task=='wnli' and config['use_wsc']: loss_fc = BCEWithLogitsLossFlat()\n",
    "  else: loss_fc = CrossEntropyLossFlat()\n",
    "  # metrics\n",
    "  metrics = [eval(f'{metric}()') for metric in METRICS[task]]\n",
    "  def sigmoid_acc(inps,targ):\n",
    "    pred = torch.sigmoid(inps) > 0.5\n",
    "    return (pred == targ).float().mean()\n",
    "  if task=='wnli' and config['use_wsc']: metrics = [sigmoid_acc]\n",
    "  # learning rate\n",
    "  splitter = partial(hf_electra_param_splitter, \n",
    "                  num_hidden_layers=electra_config.num_hidden_layers,\n",
    "                  outlayer_name= 'discriminator_predictions' if task=='wnli' and config['use_wsc'] else '1')\n",
    "  layer_lrs = get_layer_lrs(lr=config['lr'],\n",
    "                    decay_rate_of_depth=config['layer_lr_decay'],\n",
    "                    num_hidden_layers=electra_config.num_hidden_layers,)\n",
    "  lr_shedule = ParamScheduler({'lr': partial(linear_warmup_and_decay,\n",
    "                                            lr_max=np.array(layer_lrs),\n",
    "                                            end_lr=0.0,\n",
    "                                            decay_power=1,\n",
    "                                            warmup_pct=0.1,\n",
    "                                            total_steps=num_epochs*(len(dls.train)))})\n",
    "  \n",
    "  \n",
    "  # learner\n",
    "  learn = Learner(dls, model,\n",
    "                  loss_func=loss_fc, \n",
    "                  opt_func=partial(Adam, eps=1e-6,),\n",
    "                  metrics=metrics,\n",
    "                  splitter=splitter,\n",
    "                  lr=layer_lrs,\n",
    "                  path=str(Path.home()/'checkpoints'),\n",
    "                  model_dir='electra_glue',)\n",
    "  \n",
    "  # load checkpoint\n",
    "  if checkpoint: learn.load(checkpoint)\n",
    "\n",
    "  # fp16\n",
    "  if config['use_fp16']: learn = learn.to_fp16()\n",
    "\n",
    "  # \n",
    "  if run_name:\n",
    "    id = run_name.split('_')[1]\n",
    "    wandb.init(project='electra-glue', name=run_name, config={'task': task, 'id':id, 'use_fp16':config['use_fp16'], 'optim':'Adam', 'use_onecycle':False}, reinit=True)\n",
    "    learn.add_cb(WandbCallback(None, False))\n",
    "\n",
    "  # one cycle / warm up + linear decay \n",
    "  if one_cycle: return learn, partial(learn.fit_one_cycle, n_epoch=num_epochs)\n",
    "  else: return learn, partial(learn.fit, n_epoch=num_epochs, cbs=[lr_shedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand_id = random.randint(1,500)\n",
    "#rand_id = 79\n",
    "pretrained_checkpoint = Path.home()/'checkpoints/electra_pretrain/7-06_10-31-49_100%.pth'\n",
    "pretrained_checkpoint = None\n",
    "for i in range(10):\n",
    "  for task in ['cola', 'sst2', 'mrpc', 'stsb', 'qnli', 'rte', 'qqp', 'mnli', 'wnli']:\n",
    "    if task not in ['wnli']: continue\n",
    "    run_name = f\"{task}_{rand_id}_{i}\"\n",
    "    # run_name = None # set to None to skip wandb and model saving\n",
    "    learn, fit_fc = get_glue_learner(task, device='cuda:0', \n",
    "                                      run_name=run_name, checkpoint=pretrained_checkpoint) \n",
    "    fit_fc()\n",
    "    if run_name:\n",
    "      wandb.join()\n",
    "      learn.save(run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c73X4fdrmnlP"
   },
   "source": [
    "## 3.3 Predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identifier(task, split):\n",
    "  map = {'cola': 'CoLA', 'sst2':'SST-2', 'mrpc':'MRPC', 'qqp':'QQP', 'stsb':'STS-B', 'qnli':'QNLI', 'rte':'RTE', 'wnli':'WNLI', 'ax':'AX'}\n",
    "  if task =='mnli' and split == 'test_matched': return 'MNLI-m'\n",
    "  elif task == 'mnli' and split == 'test_mismatched': return 'MNLI-mm'\n",
    "  else: return map[task]\n",
    "\n",
    "class Ensemble(nn.Module):\n",
    "  def __init__(self, models, device='cuda:0'):\n",
    "    super().__init__()\n",
    "    self.models = nn.ModuleList( m.cpu() for m in models )\n",
    "    self.device = device\n",
    "  \n",
    "  def to(self, device): \n",
    "    self.device = device\n",
    "    return self\n",
    "  def getitem(self, i): return self.models[i]\n",
    "  \n",
    "  def forward(self, *args, **kwargs):\n",
    "    outs = []\n",
    "    for m in self.models:\n",
    "      m.to(self.device)\n",
    "      out = m(*args, **kwargs)\n",
    "      assert isinstance(out, torch.Tensor)\n",
    "      m.cpu()\n",
    "      outs.append(out)\n",
    "    outs = torch.stack(outs)\n",
    "    return outs.mean(dim=0)\n",
    "\n",
    "def load_model_(learn, files, device=None, **kwargs):\n",
    "  \"if multiple file passed, then load and create an ensemble. Load normally otherwise\"\n",
    "  if not isinstance(files, list): \n",
    "    learn.load(files, device=device, **kwargs)\n",
    "    return\n",
    "  if device is None: device = learn.dls.device\n",
    "  model = learn.model.cpu()\n",
    "  models = [model, *(deepcopy(model) for _ in range(len(files)-1)) ]\n",
    "  for f,m in zip(files, models):\n",
    "    file = join_path_file(f, learn.path/learn.model_dir, ext='.pth')\n",
    "    load_model(file, m, learn.opt, device='cpu', **kwargs)\n",
    "  learn.model = Ensemble(models, device)\n",
    "  return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(task, checkpoint, dl_idx=-1, output_dir=None, device='cuda:0'):\n",
    "  if output_dir is None: output_dir = cache_dir/'glue/test'\n",
    "  output_dir = Path(output_dir)\n",
    "  output_dir.mkdir(exist_ok=True)\n",
    "  device = torch.device(device)\n",
    "\n",
    "  # load checkpoint and get predictions\n",
    "  learn, _ = get_glue_learner(task, device=device)\n",
    "  load_model_(learn, checkpoint)\n",
    "  results = learn.get_preds(ds_idx=dl_idx, with_decoded=True)\n",
    "  preds = results[-1] # preds -> (predictions logits, targets, decoded prediction)\n",
    "\n",
    "  # decode target class index to its class name \n",
    "  if task in ['mnli','ax']:\n",
    "    preds = [ ['entailment','neutral','contradiction'][p] for p in preds]\n",
    "  elif task in ['qnli','rte']: \n",
    "    preds = [ ['entailment','not_entailment'][p] for p in preds ]\n",
    "  elif task == 'wnli' and config['use_wsc']:\n",
    "    preds = preds.to(dtype=torch.long).tolist()\n",
    "  else: preds = preds.tolist()\n",
    "    \n",
    "  # form test dataframe and save\n",
    "  test_df = pd.DataFrame( {'index':range(len(list(glue_dsets[task].values())[dl_idx])), 'prediction': preds} )\n",
    "  split = list(glue_dsets['mnli'].keys())[dl_idx]\n",
    "  identifier = get_identifier(task, split)\n",
    "  test_df.to_csv( output_dir/f'{identifier}.tsv', sep='\\t' )\n",
    "  return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "stsb\n 80%|███████▉  | 1100/1379 [00:00<00:00, 5491.02it/s]"
    },
    {
     "data": {
      "text/html": "",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir = 'hf_small_test'\n",
    "id = 79\n",
    "th_run = {'cola': 9, 'sst2': 1, 'mrpc': 6, 'qqp': 2, 'stsb': 4, 'qnli': 1, 'rte': 4, 'mnli': 5, 'ax': 5,\n",
    "        'wnli': 7 #[f\"wnli_254_{i}\" for i in [29,19,1,14,22,16,13,10,8,4]],\n",
    "        }\n",
    "for task, th in th_run.items():\n",
    "  if task not in ['stsb']: continue\n",
    "  print(task)\n",
    "  ckp = f\"{task}_{id}_{th}\" if task != 'ax' else f\"mnli_{id}_{th}\"\n",
    "  dl_idxs = [-1, -2] if task=='mnli' else [-1]\n",
    "  for dl_idx in dl_idxs:\n",
    "    df = predict_test(task, ckp, dl_idx, output_dir=cache_dir/f'glue/test/{dir}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "yKxetBkqfdgm",
    "MqtDnxK9ZYQP",
    "LFcM5MGaafFJ",
    "7cR6NmFsxH10",
    "n1vFihXdx7iQ",
    "pZS8owurxXUV",
    "admPAZ2M2uqO",
    "1NSoV-Np6utL"
   ],
   "name": "Finetune GLUE with fastai.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "007556af2af8413ca47aa5f7f725e68a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2d26c89a3d97493e911c52fa20cf79d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd8da3c0b2784f2094d3b0cfa744cf87",
      "placeholder": "​",
      "style": "IPY_MODEL_5b8f1e5172204ed58bd60972289b24a8",
      "value": " 29.0k/29.0k [00:00&lt;00:00, 39.1kB/s]"
     }
    },
    "5b8f1e5172204ed58bd60972289b24a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65eee3543a9d46adbc8037995c3a69a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "71beed6de71c4aa2862ef33e21cc58bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d4771f53c7046f1afe7e04a3a647a31": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fc29a98d6be46e0974d1f0f7681d5c3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a07124360f4c4249a745891d83e719f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d4771f53c7046f1afe7e04a3a647a31",
      "max": 30329,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_007556af2af8413ca47aa5f7f725e68a",
      "value": 30329
     }
    },
    "a8b32129b9ab41d0895cb1ec445db9ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fc29a98d6be46e0974d1f0f7681d5c3",
      "max": 28998,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65eee3543a9d46adbc8037995c3a69a0",
      "value": 28998
     }
    },
    "ae0863b074ff4a1a959514881fe2802f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a8b32129b9ab41d0895cb1ec445db9ca",
       "IPY_MODEL_2d26c89a3d97493e911c52fa20cf79d9"
      ],
      "layout": "IPY_MODEL_e5d5d86746ec4e2298028c57ef775d84"
     }
    },
    "bd8da3c0b2784f2094d3b0cfa744cf87": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1d8bdd9c1a74bc18a103bc900589c2c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5d5d86746ec4e2298028c57ef775d84": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f26a4f6d9334465d9022ab2a3401035b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1d8bdd9c1a74bc18a103bc900589c2c",
      "placeholder": "​",
      "style": "IPY_MODEL_71beed6de71c4aa2862ef33e21cc58bc",
      "value": " 30.3k/30.3k [00:00&lt;00:00, 392kB/s]"
     }
    },
    "f35afd9d13c240aba4e0619cca0199b7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f74af04bf4764b7888f0a45e37ce85c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a07124360f4c4249a745891d83e719f6",
       "IPY_MODEL_f26a4f6d9334465d9022ab2a3401035b"
      ],
      "layout": "IPY_MODEL_f35afd9d13c240aba4e0619cca0199b7"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}