{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ugFdx5v_YxzA",
    "outputId": "e6663d66-4c4b-4953-8388-7fa2b23b126a"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "from IPython.core.debugger import set_trace as bk\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import nlp\n",
    "from transformers import ElectraModel, ElectraConfig, ElectraTokenizerFast, ElectraForPreTraining\n",
    "from fastai2.text.all import *\n",
    "import wandb\n",
    "from fastai2.callback.wandb import WandbCallback\n",
    "from _utils.would_like_to_pr import *\n",
    "from _utils.huggingface import *\n",
    "from _utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = MyConfig({\n",
    "  'size': 'small',\n",
    "  'max_length': 512, # 128 only if ELECTRA-small (note that all public models are ++model which use 512)\n",
    "  # device to train and test\n",
    "  'device': 'cuda:0', # specify List[int] to use multi gpu (data parallel), None for using all gpu devices\n",
    "  # whether to do finetune or test\n",
    "  'do_finetune': True, # True, to do only finetuning, False to do only test\n",
    "  # the id that in the name of runs which are in the same group (to choose the best from 10 runs and ..)\n",
    "  'group_id': None, #random.randint(1,999), # None to not save checkpoint and not create wandb run \n",
    "  # checkpoint of ELECTRA from pretrain.py, note that only discriminator part of it will be extracted and used\n",
    "  'pretrained_checkpoint': None, # None to use checkpoints hubbed on Huggingface\n",
    "  # whether to use wnli trick described in the paper\n",
    "  'wsc_trick': True,\n",
    "  # where to cache the data\n",
    "  'cache_dir': Path.home()/'datasets', # ! should be `pathlib.Path` istance\n",
    "  # where to save finetuneing checkpoints\n",
    "  'ckp_dir': Path.home()/'checkpoints/electra_glue', # ! should be `pathlib.Path` istance\n",
    "  # cache_dir/glue/test/out_dir to put output files of testing. \n",
    "  'out_dir': \"hf_small_test\" ,\n",
    "  # finetuning checkpoint for testing. These will become \"ckp_dir/{task}_{group_id}_{th_run}.pth\"\n",
    "  'th_run': {'cola': 9, 'sst2': 1, 'mrpc': 6, 'qqp': 2, 'stsb': 4, 'qnli': 1, 'rte': 4, 'mnli': 5, 'ax': 5,\n",
    "             'wnli': [22,3,10,14,8,0,17,20,2,6],\n",
    "            }\n",
    "})\n",
    "\n",
    "if c.size == 'small' and c.pretrained_checkpoint: assert c.max_length == 128, \"Make sure max_length is 128 for ELECTRA-small, or comment this line if you know what you are doing.\"\n",
    "if c.pretrained_checkpoint is None: assert c.max_length == 512, \"All public models of ELECTRA is ++, and use max_length 512 when finetuning on GLUE\"\n",
    "if c.wsc_trick:\n",
    "  from _utils.wsc_trick import * # importing spacy model takes time\n",
    "if c.size == 'small': c.lr = 3e-4; c.layer_lr_decay = 0.8\n",
    "elif c.size == 'base': c.lr = 1e-4; c.layer_lr_decay = 0.8\n",
    "elif c.size == 'large': c.lr = 5e-5; c.layer_lr_decay = 0.9\n",
    "else: raise ValueError(f\"Invalid size {c.size}\")\n",
    "hf_tokenizer = ElectraTokenizerFast.from_pretrained(f\"google/electra-{c.size}-discriminator\")\n",
    "electra_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-discriminator')\n",
    "c.cache_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yoI6QffPY6Jv"
   },
   "source": [
    "# 1. Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_P8P4hffUr8"
   },
   "source": [
    "## 1.1 Download and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "ae0863b074ff4a1a959514881fe2802f",
      "e5d5d86746ec4e2298028c57ef775d84",
      "a8b32129b9ab41d0895cb1ec445db9ca",
      "2d26c89a3d97493e911c52fa20cf79d9",
      "65eee3543a9d46adbc8037995c3a69a0",
      "9fc29a98d6be46e0974d1f0f7681d5c3",
      "5b8f1e5172204ed58bd60972289b24a8",
      "bd8da3c0b2784f2094d3b0cfa744cf87",
      "f74af04bf4764b7888f0a45e37ce85c1",
      "f35afd9d13c240aba4e0619cca0199b7",
      "a07124360f4c4249a745891d83e719f6",
      "f26a4f6d9334465d9022ab2a3401035b",
      "007556af2af8413ca47aa5f7f725e68a",
      "8d4771f53c7046f1afe7e04a3a647a31",
      "71beed6de71c4aa2862ef33e21cc58bc",
      "e1d8bdd9c1a74bc18a103bc900589c2c"
     ]
    },
    "colab_type": "code",
    "id": "U-PLcDIRY6y-",
    "outputId": "12e31f13-b70f-48c8-dd45-c6f1718b03c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def textcols(task):\n",
    "  \"Infer text cols of different GLUE datasets in huggingface/nlp\"\n",
    "  if task in ['qnli']: return ['question', 'sentence']\n",
    "  elif task in ['mrpc','stsb','wnli','rte']: return ['sentence1', 'sentence2']\n",
    "  elif task in ['qqp']: return ['question1','question2']\n",
    "  elif task in ['mnli','ax']: return ['premise','hypothesis']\n",
    "  elif task in ['cola','sst2']: return ['sentence']\n",
    "\n",
    "def tokenize_sents(example, cols):\n",
    "  example['inp_ids'] = hf_tokenizer.encode(*[ example[c] for c in cols])\n",
    "  return example\n",
    "\n",
    "def tokenize_sents_max_len(example, cols, max_length):\n",
    "  # Follow BERT and ELECTRA, we truncate examples longer than max length, see https://github.com/google-research/electra/blob/79111328070e491b287c307906701ebc61091eb2/finetune/classification/classification_tasks.py#L296\n",
    "  tokens_a = hf_tokenizer.tokenize(example[cols[0]])\n",
    "  tokens_b = hf_tokenizer.tokenize(example[cols[1]]) if len(cols)==2 else []\n",
    "  _max_length = max_length - 1 - len(cols) # preserved for cls and sep tokens\n",
    "  while True:\n",
    "    total_length = len(tokens_a) + len(tokens_b)\n",
    "    if total_length <= _max_length:\n",
    "      break\n",
    "    if len(tokens_a) > len(tokens_b):\n",
    "      tokens_a.pop()\n",
    "    else:\n",
    "      tokens_b.pop()\n",
    "  tokens = [hf_tokenizer.cls_token, *tokens_a, hf_tokenizer.sep_token]\n",
    "  if tokens_b: tokens += [*tokens_b, hf_tokenizer.sep_token]\n",
    "  example['inp_ids'] = hf_tokenizer.convert_tokens_to_ids(tokens)\n",
    "  return example\n",
    "\n",
    "# get tokenized datasets and dataloaders\n",
    "glue_dsets = {}; glue_dls = {}\n",
    "for task in ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', 'qnli', 'rte', 'wnli', 'ax']:\n",
    "  # General case and special case for WSC\n",
    "  if task == 'wnli' and c.wsc_trick:\n",
    "    benchmark, subtask = 'super_glue', 'wsc'\n",
    "    # samples in all splits are all less than 128-2, so don't need to worry about max_length\n",
    "    Tfm = partial(WSCTrickTfm, hf_toker=hf_tokenizer)\n",
    "    cols = {'prefix':TensorText, 'suffix':TensorText, 'cands':TensorText, 'cand_lens':noop, 'label':TensorCategory}\n",
    "    n_inp=4\n",
    "    cache_name = \"tricked_{split}.arrow\"\n",
    "  else:\n",
    "    benchmark, subtask = 'glue', task\n",
    "    tok_func = partial(tokenize_sents_max_len, cols=textcols(task), max_length=c.max_length)\n",
    "    Tfm = partial(HF_Transform, func=tok_func)\n",
    "    cols = ['inp_ids', 'label']\n",
    "    n_inp=1\n",
    "    cache_name = f\"tokenized_{c.max_length}_{{split}}.arrow\"\n",
    "  # load / download datasets.\n",
    "  dsets = nlp.load_dataset(benchmark, subtask, cache_dir=c.cache_dir)\n",
    "  # There is two samples broken in QQP training set\n",
    "  if task=='qqp': dsets['train'] = dsets['train'].filter(lambda e: e['question2']!='',\n",
    "                                          cache_file_name=str(c.cache_dir/'glue/qqp/1.0.0/fixed_train.arrow'))\n",
    "  # \n",
    "  if task=='wnli' and c.wsc_trick: dsets['train'] = dsets['train'].filter(lambda e: e['label']==1,\n",
    "                 cache_file_name=str(c.cache_dir/'super_glue/wsc/1.0.2/filtered_tricked_train.arrow'))\n",
    "  # load / make tokenized datasets\n",
    "  glue_dsets[task] = Tfm(dsets).map(cache_name=cache_name)\n",
    "  # load / make dataloaders\n",
    "  hf_dsets = HF_Datasets(glue_dsets[task], cols=cols, hf_toker=hf_tokenizer, n_inp=n_inp)\n",
    "  dl_cache_name = cache_name.replace('tokenized', 'dl').replace('.arrow', '.json')\n",
    "  glue_dls[task] = hf_dsets.dataloaders(bs=32, cache_name=dl_cache_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MqtDnxK9ZYQP"
   },
   "source": [
    "## 1.2 View Data\n",
    "- View raw data on [nlp-viewer]! (https://huggingface.co/nlp/viewer/)\n",
    "\n",
    "- View task description on Tensorflow dataset doc for GLUE (https://www.tensorflow.org/datasets/catalog/glue) \n",
    "\n",
    "- You may notice some text without \\[SEP\\], that is because the whole sentence is truncated by `show_batch`, you can turn it off by specify `truncated_at=None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VYQubDaZbRv"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "HhZ3HD7VZalK",
    "outputId": "7ea5edbf-6a70-4f5a-9b85-c033e7aef666",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 8551/1043/1063\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] everybody who has ever , worked in any office which contained any type ##writer which had ever been used to type any letters which had to be signed by any administrator who ever worked in any department like mine will know what i mean . [SEP]</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# CoLA (The Corpus of Linguistic Acceptability) - 0: unacceptable, 1: acceptable \n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['cola'].loaders]))\n",
    "glue_dls['cola'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZJ2zGxJZiyq"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "STiSJYdbZns1"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "ouo5jT9sZnKG",
    "outputId": "ef107490-d6b8-4551-9028-33cc299fc674",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 67349/872/1821\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] even if the en ##tic ##ing prospect of a lot of nu ##bil ##e young actors in a film about campus de ##pr ##avi ##ty did n ' t fade amid the deliberate , tires ##ome u ##gli ##ness , it would be rendered ted ##ious by ava ##ry ' s failure to construct a story with even a trace of dramatic interest . [SEP]</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# SST-2 (The Stanford Sentiment Treebank) - 1: positvie, 0: negative\n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['sst2'].loaders]))\n",
    "glue_dls['sst2'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "6T54EE4WZxgW",
    "outputId": "98e94bdb-324d-4d80-b367-7f4f6da2efb1",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 3668/408/1725\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] with cl ##ari ##tin ' s decline , sc ##hering - pl ##ough ' s best - selling products now are two drugs used together to treat hepatitis c , the anti ##vira ##l pill rib ##avi ##rin and an inter ##fer ##on medicine called peg - intro ##n . [SEP] with cl ##ari ##tin ' s decline , sc ##hering - pl ##ough ' s best - selling products are now anti ##vira ##l drug rib ##avi ##rin and an inter ##fer ##on medicine called peg - intro ##n - - two drugs used together to treat hepatitis c . [SEP]</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# MRPC (Microsoft Research Paraphrase Corpus) -  1: match, 0: no\n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['mrpc'].loaders]))\n",
    "glue_dls['mrpc'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "znmNswfyZuCb"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "631vsrFwaDHB",
    "outputId": "380a5db6-b00b-49aa-a5d7-7d4cedba576e",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 5749/1500/1379\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] iraq has been lobbying for the security council to stop using the country ' s oil revenue to pay compensation to victims of the 1991 gulf war and the salaries of the united nations monitoring , verification and inspection commission inspectors and to have all money remaining in the united nation ' s oil - for - food accounts transferred to the government ' s development fund . [SEP] iraq ' s new leaders have been lobbying for the united nations security council to stop using the iraq ' s oil revenue to pay the salaries of the inspectors and to have all money remaining in the united nation ' s oil - for - food account transferred to the iraqi government . [SEP]</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# STS-B (Semantic Textual Similarity Benchmark) - 0.0 ~ 5.0\n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['stsb'].loaders]))\n",
    "glue_dls['stsb'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "hWQc8402aFJb",
    "outputId": "8a4e6617-47e4-4a92-bc56-481730a16088",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 363847/40430/390965\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] i ' m in 12 steps program . been sober for 4 years and counting . i ' ve read about l ##sd and want to try it , if i used it once , would that be a re ##la ##pse ? [SEP] heartbreak ? heartbreak ? she ' s my girlfriend for two months , i chose her over my girlfriend for 2 years . i like her so much to the point that i can ' t let her go even if she wants to end our relationship because of the other people around us most especially her family . i do the things for her that i ' m not used to for a girl and i am willing to sacrifice everything just to have a little time with her . a little and limited time that i ' m asking from her but she</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# QQP (Quora Question Pairs) - 0: no, 1: duplicated\n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['qqp'].loaders]))\n",
    "glue_dls['qqp'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "3PRuXBJ8aJTH",
    "outputId": "89fd1479-796c-43ef-ee71-292ac6ed4bad",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/validation_matched/validation_mismatched/test_matched/test_mismatched): 392702/9815/9832/9796/9847\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] well uh that ' s kind of obvious i mean they ' re even carrying it to to where now uh that they ad ##vert ##ise on tv you know if your if you uh you know have done this or if you need this uh uh we ' ll sue for you and you don ' t have to pay us unless you but then what they don ' t tell you is that if you if they win you give them at least a third of the of the thing that they win so i don ' t know it is uh it ' s getting to be more business now rather than uh actually uh dealing with the crime than with uh um the uh punishment they the the lawyers are just in it for the money i ' m i ' m convinced i know</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# MNLI (The Multi-Genre NLI Corpus) - 0: entailment, 1: neutral, 2: contradiction\n",
    "print(\"Dataset size (train/validation_matched/validation_mismatched/test_matched/test_mismatched): {}/{}/{}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['mnli'].loaders]))\n",
    "glue_dls['mnli'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "DIK9BAnBaLbj",
    "outputId": "62ee8573-b4f6-4274-9592-44920c010674",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 104743/5463/5463\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] what time magazine founder attended yale ? [SEP] among the best - known are u . s . presidents william howard taft , gerald ford , george h . w . bush , bill clinton and george w . bush ; royals crown princess victoria bern ##ado ##tte , prince ro ##stis ##lav romano ##v and prince ak ##ii ##ki hose ##a ny ##ab ##ong ##o ; heads of state , including italian prime minister mario mont ##i , turkish prime minister tan ##su ci ##ller , mexican president ernesto ze ##di ##llo , german president karl cars ##tens , and philippines president jose pac ##iano laurel ; u . s . supreme court justices sonia soto ##may ##or , samuel ali ##to and clarence thomas ; u . s . secretaries of state john kerry , hillary clinton , cyrus vance , and dean ache ##son ; authors</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# QNLI (The Stanford Question Answering Dataset) - 0: entailment, 1: not_entailment\n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['qnli'].loaders]))\n",
    "glue_dls['qnli'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "cODdx4WyaVRh",
    "outputId": "ee44b782-8da0-42a8-ef6f-b17a841679df",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 2490/277/3000\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] sweden has plans to open a virtual embassy in second life , the virtual world home to thousands of net ##ize ##ns . companies like dell are already selling computers via the virtual world , but sweden would become the first country to have a cyber - embassy in second life . the embassy would not provide visa or perform diplomatic tasks , but would provide information on how and where to get these documents in the real world , as well as giving cultural and tourist information about the country . visitors will also be able to chat with embassy personnel . a spokesperson explained that it would be an easy and cheap way to reach young people . the idea came from the swedish institute , an agency of the foreign affairs ministry of sweden with providing information about sweden as a key purpose . the \"</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# RTE (Recognizing_Textual_Entailment) - 0: entailment, 1: not_entailment\n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['rte'].loaders]))\n",
    "glue_dls['rte'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (train/valid/test): 259/104/146\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prefix</th>\n      <th>suffix</th>\n      <th>cands</th>\n      <th>cand_lens</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] after i saw bill catching flies and pulling off their wings , i boxed his ears . i showed the master the flies , some crushed and some crawling about helpless , and i showed him the wings on the window si ##ll . i never saw him so angry before ; but as bill was still howling and w ##hini ##ng , like the coward that he was , he did not give him any more punishment of that kind , but set</td>\n      <td>up on a stool for the rest of the afternoon , and said that he should not go out to play for that week . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n      <td>bill any more punishment that kind stool afternoon that week their wings ears the master wings his ears rest punishment a stool the wings the afternoon the coward the rest the flies window si ##ll coward kind the window flies master week</td>\n      <td>(1, 3, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 3, 1, 1, 2, 1, 1, 1)</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# WSC (The Winograd Schema Challenge) - 0: wrong, 1: correct\n",
    "# There are three style, WNLI (casted in NLI type), WSC, WSC with candidates (trick used by Roberta)\n",
    "\"Note for WSC trick: cands is the concatenation of candidates, cand_lens is the lengths of candidates in order.\"\n",
    "print(\"Dataset size (train/valid/test): {}/{}/{}\".format(*[len(dl.dataset) for dl in glue_dls['wnli'].loaders]))\n",
    "glue_dls['wnli'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "5IKH0iJKadAW",
    "outputId": "0c5935c4-608f-426e-f3d0-9d380df14fa8",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size (test): 1104\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inp_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] we manually ann ##ota ##ted 68 ##7 template ##s mapping kb pre ##dicate ##s to text for different composition ##ality types ( with 46 ##2 unique kb pre ##dicate ##s ) , and use those template ##s to modify the original web ##quest ##ions ##sp question according to the meaning of the generated spa ##r ##q ##l query . [SEP] we manually ann ##ota ##ted over 650 template ##s mapping kb pre ##dicate ##s to text for different composition ##ality types ( with 46 ##2 unique kb pre ##dicate ##s ) , and use those template ##s to modify the original web ##quest ##ions ##sp question according to the meaning of the generated spa ##r ##q ##l query . [SEP]</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# AX (GLUE Diagnostic Dataset) - 0: entailment, 1: neutral, 2: contradiction\n",
    "print(\"Dataset size (test): {}\".format(*[len(dl.dataset) for dl in glue_dls['ax'].loaders]))\n",
    "glue_dls['ax'].show_batch(max_n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFcM5MGaafFJ"
   },
   "source": [
    "# 2. Finetuning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tUGxkcDfQGxS"
   },
   "source": [
    "* ELECTRA use CLS encodings as pooled result to predict the sentence. (see [here](https://github.com/google-research/electra/blob/79111328070e491b287c307906701ebc61091eb2/model/modeling.py#L254) of its official repository)\n",
    "\n",
    "* Note that we should use different prediction head instance for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MF6vajgKQKJ7"
   },
   "outputs": [],
   "source": [
    "class SentencePredictHead(nn.Module):\n",
    "  \"The way that Electra and Bert do for sentence prediction task\"\n",
    "  def __init__(self, hidden_size, targ_voc_size):\n",
    "    super().__init__()\n",
    "    self.linear = nn.Linear(hidden_size, targ_voc_size)\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "  def forward(self, x):\n",
    "    \"x: (batch size, sequence length, hidden_size)\"\n",
    "    # project the first token (a special token)'s hidden encoding\n",
    "    return self.linear(self.dropout(x[:,0])).squeeze(-1) # if regression task, squeeze to (B), else (B,#class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zi5_OKHqX8qw"
   },
   "source": [
    "# 3. Single Task Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "poNJYRwETtBo"
   },
   "source": [
    "## 3.1 Discriminative learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OyT-hy-7FXz"
   },
   "outputs": [],
   "source": [
    "# Names come from, for nm in model.named_modules(): print(nm[0])\n",
    "\n",
    "def hf_electra_param_splitter(model, num_hidden_layers, outlayer_name):\n",
    "  names = ['.embeddings', *[f'encoder.layer.{i}' for i in range(num_hidden_layers)], outlayer_name]\n",
    "  def end_with_any(name): return any( name.endswith(n) for n in names )\n",
    "  groups = [ list(mod.parameters()) for name, mod in model.named_modules() if end_with_any(name) ]\n",
    "  assert len(groups) == len(names)\n",
    "  return groups\n",
    "\n",
    "def get_layer_lrs(lr, decay_rate_of_depth, num_hidden_layers):\n",
    "  # I think input layer as bottom and output layer as top, which make 'depth' mean different from the one of official repo \n",
    "  return [ lr * (decay_rate_of_depth ** depth) for depth in reversed(range(num_hidden_layers+2))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N2ug-bscA9gC"
   },
   "source": [
    "## 3.2 Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nzqFhWQkBGhu"
   },
   "outputs": [],
   "source": [
    "def linear_warmup_and_decay(pct_now, lr_max, end_lr, decay_power, warmup_pct, total_steps):\n",
    "  \"\"\"\n",
    "  end_lr: the end learning rate for linear decay\n",
    "  warmup_pct: percentage of training steps to for linear increase\n",
    "  pct_now: percentage of traning steps we have gone through, notice pct_now=0.0 when calculating lr for first batch.\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  pct updated after_batch, but global_step (in tf) seems to update before optimizer step,\n",
    "  so pct is actually (global_step -1)/total_steps \n",
    "  \"\"\"\n",
    "  fixed_pct_now = pct_now + 1/total_steps\n",
    "  \"\"\"\n",
    "  According to source code of the official repository, it seems they merged two lr schedule (warmup and linear decay)\n",
    "  sequentially, instead of split training into two phases for each, this might because they think when in the early\n",
    "  phase of training, pct is low, and thus the decaying formula makes little difference to lr.\n",
    "  \"\"\"\n",
    "  decayed_lr = (lr_max-end_lr) * (1-fixed_pct_now)**decay_power + end_lr # https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/polynomial_decay\n",
    "  warmed_lr = decayed_lr * min(1.0, fixed_pct_now / warmup_pct) # https://github.com/google-research/electra/blob/81f7e5fc98b0ad8bfd20b641aa8bc9e6ac00c8eb/model/optimization.py#L44\n",
    "  return warmed_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Mcjq0b8wbkO"
   },
   "source": [
    "## 3.3 finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = {\n",
    "  **{ task:['MatthewsCorrCoef'] for task in ['cola']},\n",
    "  **{ task:['Accuracy'] for task in ['sst2', 'mnli', 'qnli', 'rte', 'wnli', 'snli','ax']},\n",
    "  # Note: MRPC and QQP are both binary classification problem, so we can just use fastai's default\n",
    "  # average option 'binary' without spcification of average method.\n",
    "  **{ task:['F1Score', 'Accuracy'] for task in ['mrpc', 'qqp']}, \n",
    "  **{ task:['PearsonCorrCoef', 'SpearmanCorrCoef'] for task in ['stsb']}\n",
    "}\n",
    "TARG_VOC_SIZE = {\n",
    "    **{ task:1 for task in ['stsb']},\n",
    "    **{ task:2 for task in ['cola', 'sst2', 'mrpc', 'qqp', 'qnli', 'rte', 'wnli']},\n",
    "    **{ task:3 for task in ['mnli','ax']}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_glue_learner(task, run_name=None, one_cycle=False, inference=False):\n",
    "  \n",
    "  # num_epochs\n",
    "  if task == 'rte': num_epochs = 10\n",
    "  else: num_epochs = 3\n",
    "\n",
    "  # dls\n",
    "  dls = glue_dls[task]\n",
    "  if isinstance(c.device, str): dls.to(torch.device(c.device))\n",
    "  elif isinstance(c.device, list): dls.to(torch.device('cuda', c.device[0]))\n",
    "  else: dls.to(torch.device('cuda:0'))\n",
    "  # load model\n",
    "  if c.pretrained_checkpoint: \n",
    "    discriminator = HF_Model(ElectraForPreTraining, electra_config, hf_tokenizer)\n",
    "    load_part_model(c.pretrained_checkpoint, discriminator, 'discriminator')\n",
    "  else:\n",
    "    discriminator = HF_Model(ElectraForPreTraining, f\"google/electra-{c.size}-discriminator\", hf_tokenizer)\n",
    "  # model\n",
    "  if task=='wnli' and c.wsc_trick:\n",
    "    model = ELECTRAWSCTrickModel(discriminator, hf_tokenizer.pad_token_id, hf_tokenizer)\n",
    "  else: # take only base model and mount an output layer\n",
    "    model = nn.Sequential(HF_Model(discriminator.model.electra, hf_toker=hf_tokenizer), \n",
    "                          SentencePredictHead(electra_config.hidden_size, targ_voc_size=TARG_VOC_SIZE[task]))\n",
    "  # loss func\n",
    "  if task == 'stsb': loss_fc = MyMSELossFlat(low=0.0, high=5.0)\n",
    "  elif task=='wnli' and c.wsc_trick: loss_fc = ELECTRAWSCTrickLoss()\n",
    "  else: loss_fc = CrossEntropyLossFlat()\n",
    "  # metrics\n",
    "  metrics = [eval(f'{metric}()') for metric in METRICS[task]]\n",
    "  if task=='wnli' and c.wsc_trick: metrics = [accuracy_electra_wsc_trick]\n",
    "  # learning rate\n",
    "  splitter = partial(hf_electra_param_splitter, \n",
    "                     num_hidden_layers=electra_config.num_hidden_layers,\n",
    "                     outlayer_name= 'discriminator_predictions' if task=='wnli' and c.wsc_trick else '1')\n",
    "  layer_lrs = get_layer_lrs(lr=c.lr,\n",
    "                    decay_rate_of_depth=c.layer_lr_decay,\n",
    "                    num_hidden_layers=electra_config.num_hidden_layers,)\n",
    "  lr_shedule = ParamScheduler({'lr': partial(linear_warmup_and_decay,\n",
    "                                            lr_max=np.array(layer_lrs),\n",
    "                                            end_lr=0.0,\n",
    "                                            decay_power=1,\n",
    "                                            warmup_pct=0.1,\n",
    "                                            total_steps=num_epochs*(len(dls.train)))})\n",
    "\n",
    "  \n",
    "  # learner\n",
    "  learn = Learner(dls, model,\n",
    "                  loss_func=loss_fc, \n",
    "                  opt_func=partial(Adam, eps=1e-6,),\n",
    "                  metrics=metrics,\n",
    "                  splitter=splitter if not inference else trainable_params,\n",
    "                  lr=layer_lrs if not inference else defaults.lr,\n",
    "                  path=str(c.ckp_dir.parent),\n",
    "                  model_dir=c.ckp_dir.name,)\n",
    "  \n",
    "  # multi gpu\n",
    "  if isinstance(c.device, list) or c.device is None:\n",
    "    learn.model = nn.DataParallel(learn.model, device_ids=c.device)\n",
    "\n",
    "  # fp16\n",
    "  if c.device != 'cpu': learn = learn.to_fp16()\n",
    "\n",
    "  # wandb\n",
    "  if run_name:\n",
    "    wandb.init(project='electra-glue', name=run_name, config={'task': task, 'optim':'Adam', **c}, reinit=True)\n",
    "    learn.add_cb(WandbCallback(None, False))\n",
    "\n",
    "  # one cycle / warm up + linear decay \n",
    "  if one_cycle: return learn, partial(learn.fit_one_cycle, n_epoch=num_epochs)\n",
    "  else: return learn, partial(learn.fit, n_epoch=num_epochs, cbs=[lr_shedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "wnli\n> \u001b[0;32m<ipython-input-19-2719d7c747fb>\u001b[0m(9)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m      7 \u001b[0;31m      \u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_fc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_glue_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8 \u001b[0;31m      \u001b[0mbk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m----> 9 \u001b[0;31m      \u001b[0mfit_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10 \u001b[0;31m      \u001b[0;32mif\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11 \u001b[0;31m        \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\nBreakpoint 1 at /home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py:102\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    \n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "atures=256, out_features=256, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=256, out_features=256, bias=True)\n                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=256, out_features=1024, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=1024, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (discriminator_predictions): ElectraDiscriminatorPredictions(\n      (dense): Linear(in_features=256, out_features=256, bias=True)\n      (dense_prediction): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n)\n> \u001b[0;32m/home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m(128)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n\u001b[0;32m    126 \u001b[0;31m      \u001b[0;31m# get discrimiator scores for each candidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127 \u001b[0;31m      \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (#candidate, max_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 128 \u001b[0;31m      \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (#candidate,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129 \u001b[0;31m      \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130 \u001b[0;31m      \u001b[0;31m# save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n'[CLS] after i saw bill catching flies and pulling off their wings, i boxed his ears. i showed the master the flies, some crushed and some crawling about helpless, and i showed him the wings on the window sill. i never saw him so angry before ; but as bill was still howling and whining, like the coward that he was, he did not give him any more punishment of that kind, but set bill up on a stool for the rest of the afternoon, and said that he should not go out to play for that week. [SEP] [PAD] [PAD]'\n'[CLS] after i saw bill catching flies and pulling off their wings, i boxed his ears. i showed the master the flies, some crushed and some crawling about helpless, and i showed him the wings on the window sill. i never saw him so angry before ; but as bill was still howling and whining, like the coward that he was, he did not give him any more punishment of that kind, but set week up on a stool for the rest of the afternoon, and said that he should not go out to play for that week. [SEP] [PAD] [PAD]'\n> \u001b[0;32m/home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m(129)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n\u001b[0;32m    127 \u001b[0;31m      \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (#candidate, max_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128 \u001b[0;31m      \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (#candidate,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 129 \u001b[0;31m      \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130 \u001b[0;31m      \u001b[0;31m# save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131 \u001b[0;31m      \u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\ntensor([-10.8984,  -1.6104,  -4.1055,  -2.3945,  -2.5000,  -3.1758,  -4.7422,\n         -4.2500,  -1.8096,  -1.9111,  -2.3320,  -3.0508,  -5.8945,  -3.9199,\n         -2.3398,  -3.5078,  -1.1973,  -3.0625,  -4.5273,  -3.6934,  -1.4629,\n         -1.1475,  -1.0713,  -4.0820,  -4.4375,  -4.4023,  -1.8867,  -2.3730,\n         -3.1230,  -2.5879,  -2.5684,  -1.1904,  -3.7383,  -3.5859,  -3.6719,\n         -3.4980,  -4.0078,  -3.4551,  -2.6895,  -1.8975,  -3.2695,  -2.2500,\n         -2.4570,  -2.5488,  -3.6230,  -4.9297,  -3.8184,  -2.4863,  -3.0742,\n         -2.9277,  -1.7432,  -2.5391,  -2.2969,  -2.9805,  -1.4297,  -2.2734,\n         -4.8633,  -2.7715,  -1.7383,  -4.4922,  -6.4961,  -6.5195,  -7.1172,\n         -3.7773,  -2.6309,  -1.7646,  -3.4004,  -5.1641,  -4.5977,  -3.3477,\n         -4.5781,  -3.2793,  -4.2227,  -7.3789,  -2.8457,  -3.5820,  -3.4199,\n         -2.6621,  -2.6523,  -4.8320,  -1.6416,  -2.1816,  -4.2266,  -2.8203,\n         -2.5957,  -2.5996,  -4.0039,  -4.4141,  -3.5879,  -2.6816,  -4.8398,\n         -7.5391,  -3.3555,  -6.8438,  -3.8789,  -2.2246,  -4.6289,  -4.1602,\n         -2.4258,  -5.3242,  -2.5293,  -2.7363,  -3.2188,  -2.0938,  -1.9961,\n         -2.5703,  -1.3516,  -1.1074,  -1.8555,  -0.4810,  -7.1211, -11.1172,\n         12.9141,   8.9609], device='cuda:0', dtype=torch.float16,\n       grad_fn=<SelectBackward>)\n*** RuntimeError: Expected object of scalar type Bool but got scalar type Long for argument #2 'mask' in call to _th_masked_select_bool\n*** RuntimeError: Expected object of scalar type Bool but got scalar type Long for argument #2 'mask' in call to _th_masked_select_bool\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\ntensor([-2.5996], device='cuda:0', dtype=torch.float16,\n       grad_fn=<MaskedSelectBackward>)\ntensor([0.0692])\n*** RuntimeError: Expected object of scalar type Bool but got scalar type Long for argument #2 'mask' in call to _th_masked_select_bool\ntensor([2.3066], device='cuda:0', dtype=torch.float16,\n       grad_fn=<MaskedSelectBackward>)\ntensor([0.9094])\n> \u001b[0;32m/home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m(131)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n\u001b[0;32m    129 \u001b[0;31m      \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130 \u001b[0;31m      \u001b[0;31m# save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 131 \u001b[0;31m      \u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132 \u001b[0;31m      \u001b[0mn_cands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133 \u001b[0;31m    \u001b[0;31m# repack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\ntensor(-2.5996, device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward>)\ntensor(2.3066, device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward>)\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n(tensor([3021], device='cuda:0'), tensor([2151, 2062, 7750], device='cuda:0'), tensor([2008, 2785], device='cuda:0'), tensor([14708], device='cuda:0'), tensor([5027], device='cuda:0'), tensor([2008, 2733], device='cuda:0'), tensor([2037, 4777], device='cuda:0'), tensor([5551], device='cuda:0'), tensor([1996, 3040], device='cuda:0'), tensor([4777], device='cuda:0'), tensor([2010, 5551], device='cuda:0'), tensor([2717], device='cuda:0'), tensor([7750], device='cuda:0'), tensor([ 1037, 14708], device='cuda:0'), tensor([1996, 4777], device='cuda:0'), tensor([1996, 5027], device='cuda:0'), tensor([ 1996, 16592], device='cuda:0'), tensor([1996, 2717], device='cuda:0'), tensor([ 1996, 10029], device='cuda:0'), tensor([3332, 9033, 3363], device='cuda:0'), tensor([16592], device='cuda:0'), tensor([2785], device='cuda:0'), tensor([1996, 3332], device='cuda:0'), tensor([10029], device='cuda:0'), tensor([3040], device='cuda:0'), tensor([2733], device='cuda:0'))\ntensor([-3.1641, -1.2979], device='cuda:0', dtype=torch.float16,\n       grad_fn=<MaskedSelectBackward>)\ntensor(-2.2305, device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward>)\n-2.231\n> \u001b[0;32m/home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m(132)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n\u001b[0;32m    130 \u001b[0;31m      \u001b[0;31m# save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131 \u001b[0;31m      \u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 132 \u001b[0;31m      \u001b[0mn_cands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133 \u001b[0;31m    \u001b[0;31m# repack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134 \u001b[0;31m    \u001b[0mall_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (#total candidate in this batch,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n26\n26\n> \u001b[0;32m/home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m(134)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n\u001b[0;32m    132 \u001b[0;31m      \u001b[0mn_cands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133 \u001b[0;31m    \u001b[0;31m# repack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 134 \u001b[0;31m    \u001b[0mall_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (#total candidate in this batch,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135 \u001b[0;31m    \u001b[0mn_cands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mall_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n> \u001b[0;32m/home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m(135)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n\u001b[0;32m    133 \u001b[0;31m    \u001b[0;31m# repack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134 \u001b[0;31m    \u001b[0mall_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (#total candidate in this batch,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 135 \u001b[0;31m    \u001b[0mn_cands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mall_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n465\n32\n> \u001b[0;32m/home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m(136)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n\u001b[0;32m    134 \u001b[0;31m    \u001b[0mall_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (#total candidate in this batch,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135 \u001b[0;31m    \u001b[0mn_cands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 136 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mall_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138 \u001b[0;31m  \u001b[0;32mdef\u001b[0m \u001b[0mdepad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\ntorch.Size([32])\nBreakpoint 2 at /home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py:159\n> \u001b[0;32m/home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m(160)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[0;32m    158 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;31m2\u001b[0;32m   159 \u001b[0;31m  \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 160 \u001b[0;31m    \u001b[0mall_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161 \u001b[0;31m    \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n> \u001b[0;32m/home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m(161)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[1;31m2\u001b[0;32m   159 \u001b[0;31m  \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160 \u001b[0;31m    \u001b[0mall_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 161 \u001b[0;31m    \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163 \u001b[0;31m      \u001b[0mn_cand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\ntorch.Size([465])\ntorch.Size([32])\n> \u001b[0;32m/home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m(162)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[0;32m    160 \u001b[0;31m    \u001b[0mall_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161 \u001b[0;31m    \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 162 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163 \u001b[0;31m      \u001b[0mn_cand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164 \u001b[0;31m      \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n> \u001b[0;32m/home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m(163)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[0;32m    161 \u001b[0;31m    \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 163 \u001b[0;31m      \u001b[0mn_cand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164 \u001b[0;31m      \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165 \u001b[0;31m      \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\ntorch.Size([26])\n> \u001b[0;32m/home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m(164)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[0;32m    162 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163 \u001b[0;31m      \u001b[0mn_cand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 164 \u001b[0;31m      \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165 \u001b[0;31m      \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166 \u001b[0;31m      \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n> \u001b[0;32m/home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m(165)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[0;32m    163 \u001b[0;31m      \u001b[0mn_cand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164 \u001b[0;31m      \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 165 \u001b[0;31m      \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166 \u001b[0;31m      \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167 \u001b[0;31m    \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (#total candidate in this batch,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n> \u001b[0;32m/home/yisiang/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m(166)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[0;32m    164 \u001b[0;31m      \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165 \u001b[0;31m      \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 166 \u001b[0;31m      \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167 \u001b[0;31m    \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (#total candidate in this batch,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\ntensor(2.0012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\ntensor(0.6283, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\ntensor([-2.5996, -2.9590, -2.5156,  3.6309,  3.1816, -0.7173, -1.6982, -0.7002,\n        -4.4531, -2.0918, -2.7930, -2.1699, -2.4805, -1.0986, -2.4141, -1.9668,\n        -3.9219, -1.0156, -3.1699, -4.0547, -1.1328,  1.2256, -2.2305, -2.3496,\n        -2.7070,  2.3066], device='cuda:0', grad_fn=<SplitWithSizesBackward>)\ntensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n"
    },
    {
     "output_type": "error",
     "ename": "BdbQuit",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2719d7c747fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_fc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_glue_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mbk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mfit_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.7/site-packages/fastcore/utils.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0minit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'init_args'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minst\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mto_return\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.7/site-packages/fastai2/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m;\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.7/site-packages/fastai2/learner.py\u001b[0m in \u001b[0;36m_do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m;\u001b[0m                        \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCancelTrainException\u001b[0m\u001b[0;34m:\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.7/site-packages/fastai2/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.7/site-packages/fastai2/learner.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m                            \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_backward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m       \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (#total candidate in this batch,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/_utils/wsc_trick.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m       \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (#total candidate in this batch,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if c.do_finetune:\n",
    "  for i in range(1):\n",
    "    for task in ['cola', 'sst2', 'mrpc', 'stsb', 'qnli', 'rte', 'qqp', 'mnli', 'wnli']:\n",
    "      if task not in ['wnli']: continue # to only do some tasks\n",
    "      if c.group_id: run_name = f\"{task}_{c.group_id}_{i}\";\n",
    "      else: run_name = None; print(task)\n",
    "      learn, fit_fc = get_glue_learner(task, run_name)\n",
    "      bk()\n",
    "      fit_fc()\n",
    "      if run_name:\n",
    "        wandb.join()\n",
    "        learn.save(run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c73X4fdrmnlP"
   },
   "source": [
    "## 3.3 Predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identifier(task, split):\n",
    "  map = {'cola': 'CoLA', 'sst2':'SST-2', 'mrpc':'MRPC', 'qqp':'QQP', 'stsb':'STS-B', 'qnli':'QNLI', 'rte':'RTE', 'wnli':'WNLI', 'ax':'AX'}\n",
    "  if task =='mnli' and split == 'test_matched': return 'MNLI-m'\n",
    "  elif task == 'mnli' and split == 'test_mismatched': return 'MNLI-mm'\n",
    "  else: return map[task]\n",
    "\n",
    "class Ensemble(nn.Module):\n",
    "  def __init__(self, models, device='cuda:0', merge_out_fc=None):\n",
    "    super().__init__()\n",
    "    self.models = nn.ModuleList( m.cpu() for m in models )\n",
    "    self.device = device\n",
    "    self.merge_out_fc = merge_out_fc\n",
    "  \n",
    "  def to(self, device): \n",
    "    self.device = device\n",
    "    return self\n",
    "  def getitem(self, i): return self.models[i]\n",
    "  \n",
    "  def forward(self, *args, **kwargs):\n",
    "    outs = []\n",
    "    for m in self.models:\n",
    "      m.to(self.device)\n",
    "      out = m(*args, **kwargs)\n",
    "      m.cpu()\n",
    "      outs.append(out)\n",
    "    if self.merge_out_fc:\n",
    "      outs = self.merge_out_fc(outs)\n",
    "    else:\n",
    "      outs = torch.stack(outs)\n",
    "      outs = outs.mean(dim=0)\n",
    "    return outs\n",
    "\n",
    "def load_model_(learn, files, device=None, **kwargs):\n",
    "  \"if multiple file passed, then load and create an ensemble. Load normally otherwise\"\n",
    "  merge_out_fc = kwargs.pop('merge_out_fc', None)\n",
    "  if not isinstance(files, list): \n",
    "    learn.load(files, device=device, **kwargs)\n",
    "    return\n",
    "  if device is None: device = learn.dls.device\n",
    "  model = learn.model.cpu()\n",
    "  models = [model, *(deepcopy(model) for _ in range(len(files)-1)) ]\n",
    "  for f,m in zip(files, models):\n",
    "    file = join_path_file(f, learn.path/learn.model_dir, ext='.pth')\n",
    "    load_model(file, m, learn.opt, device='cpu', **kwargs)\n",
    "  learn.model = Ensemble(models, device, merge_out_fc)\n",
    "  return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(task, checkpoint, dl_idx=-1, output_dir=None, device='cuda:0'):\n",
    "  if output_dir is None: output_dir = c.cache_dir/'glue/test'\n",
    "  output_dir = Path(output_dir)\n",
    "  output_dir.mkdir(exist_ok=True)\n",
    "  device = torch.device(device)\n",
    "\n",
    "  # load checkpoint and get predictions\n",
    "  learn, _ = get_glue_learner(task, device=device, inference=True)\n",
    "  if task == 'wnli' and config['wsc_trick']:\n",
    "    load_model_(learn, checkpoint, merge_out_fc=wsc_trick_merge)\n",
    "  else:\n",
    "    load_model_(learn, checkpoint)\n",
    "  results = learn.get_preds(ds_idx=dl_idx, with_decoded=True)\n",
    "  preds = results[-1] # preds -> (predictions logits, targets, decoded prediction)\n",
    "\n",
    "  # decode target class index to its class name \n",
    "  if task in ['mnli','ax']:\n",
    "    preds = [ ['entailment','neutral','contradiction'][p] for p in preds]\n",
    "  elif task in ['qnli','rte']: \n",
    "    preds = [ ['entailment','not_entailment'][p] for p in preds ]\n",
    "  elif task == 'wnli' and c.wsc_trick:\n",
    "    preds = preds.to(dtype=torch.long).tolist()\n",
    "  else: preds = preds.tolist()\n",
    "    \n",
    "  # form test dataframe and save\n",
    "  test_df = pd.DataFrame( {'index':range(len(list(glue_dsets[task].values())[dl_idx])), 'prediction': preds} )\n",
    "  split = list(glue_dsets['mnli'].keys())[dl_idx]\n",
    "  identifier = get_identifier(task, split)\n",
    "  test_df.to_csv( output_dir/f'{identifier}.tsv', sep='\\t' )\n",
    "  return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not c.do_finetune:\n",
    "  for task, th in c.th_run.items():\n",
    "    if task not in ['wnli']: continue # to do only some task\n",
    "    print(task)\n",
    "    # ax use mnli ckp\n",
    "    if isinstance(th, int):\n",
    "      ckp = f\"{task}_{c.group_id}_{th}\" if task != 'ax' else f\"mnli_{c.group_id}_{th}\"\n",
    "    else:\n",
    "      ckp = [f\"{task}_{c.group_id}_{i}\" if task != 'ax' else f\"mnli_{c.group_id}_{i}\" for i in th]\n",
    "    # run test for all testset in this task\n",
    "    dl_idxs = [-1, -2] if task=='mnli' else [-1]\n",
    "    for dl_idx in dl_idxs:\n",
    "      df = predict_test(task, ckp, dl_idx, output_dir=c.cache_dir/f'glue/test/{c.out_dir}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "yKxetBkqfdgm",
    "MqtDnxK9ZYQP",
    "LFcM5MGaafFJ",
    "7cR6NmFsxH10",
    "n1vFihXdx7iQ",
    "pZS8owurxXUV",
    "admPAZ2M2uqO",
    "1NSoV-Np6utL"
   ],
   "name": "Finetune GLUE with fastai.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "007556af2af8413ca47aa5f7f725e68a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2d26c89a3d97493e911c52fa20cf79d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd8da3c0b2784f2094d3b0cfa744cf87",
      "placeholder": "​",
      "style": "IPY_MODEL_5b8f1e5172204ed58bd60972289b24a8",
      "value": " 29.0k/29.0k [00:00&lt;00:00, 39.1kB/s]"
     }
    },
    "5b8f1e5172204ed58bd60972289b24a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65eee3543a9d46adbc8037995c3a69a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "71beed6de71c4aa2862ef33e21cc58bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d4771f53c7046f1afe7e04a3a647a31": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fc29a98d6be46e0974d1f0f7681d5c3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a07124360f4c4249a745891d83e719f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d4771f53c7046f1afe7e04a3a647a31",
      "max": 30329,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_007556af2af8413ca47aa5f7f725e68a",
      "value": 30329
     }
    },
    "a8b32129b9ab41d0895cb1ec445db9ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fc29a98d6be46e0974d1f0f7681d5c3",
      "max": 28998,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65eee3543a9d46adbc8037995c3a69a0",
      "value": 28998
     }
    },
    "ae0863b074ff4a1a959514881fe2802f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a8b32129b9ab41d0895cb1ec445db9ca",
       "IPY_MODEL_2d26c89a3d97493e911c52fa20cf79d9"
      ],
      "layout": "IPY_MODEL_e5d5d86746ec4e2298028c57ef775d84"
     }
    },
    "bd8da3c0b2784f2094d3b0cfa744cf87": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1d8bdd9c1a74bc18a103bc900589c2c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5d5d86746ec4e2298028c57ef775d84": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f26a4f6d9334465d9022ab2a3401035b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1d8bdd9c1a74bc18a103bc900589c2c",
      "placeholder": "​",
      "style": "IPY_MODEL_71beed6de71c4aa2862ef33e21cc58bc",
      "value": " 30.3k/30.3k [00:00&lt;00:00, 392kB/s]"
     }
    },
    "f35afd9d13c240aba4e0619cca0199b7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f74af04bf4764b7888f0a45e37ce85c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a07124360f4c4249a745891d83e719f6",
       "IPY_MODEL_f26a4f6d9334465d9022ab2a3401035b"
      ],
      "layout": "IPY_MODEL_f35afd9d13c240aba4e0619cca0199b7"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}