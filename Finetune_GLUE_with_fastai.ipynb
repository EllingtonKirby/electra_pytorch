{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uns2VbU0Yj5f"
   },
   "outputs": [],
   "source": [
    "try: import nlp\n",
    "except ImportError:\n",
    "  % install nlp transformers fastai2 wandb\n",
    "  !git clone https://github.com/richardyy1188/Pretrain-MLM-and-finetune-on-GLUE-with-fastai.git\n",
    "  exit() # you may need to restart the kernel to use updated packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ugFdx5v_YxzA",
    "outputId": "e6663d66-4c4b-4953-8388-7fa2b23b126a"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "from IPython.core.debugger import set_trace as bk\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import nlp\n",
    "from transformers import ElectraModel, ElectraConfig, ElectraTokenizer, ElectraTokenizerFast\n",
    "import wandb\n",
    "!wandb login 2592cb69a3f3301bd58f4e25b5a72c1872210b7e\n",
    "from fastai2.callback.wandb import *\n",
    "from fastai2.text.all import *\n",
    "%cd Pretrain-MLM-and-finetune-on-GLUE-with-fastai\n",
    "from _utils.would_like_to_pr import *\n",
    "from _utils.hf_integration import *\n",
    "from _utils.multi_task import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6wb0zB6iYyvC"
   },
   "outputs": [],
   "source": [
    "\"\"\" tokenizer and fast tokenizer\n",
    "We use normal tokenizer to get vocab, use fast tokenizer to convert tokens to ids.\n",
    "Because we can't get vocab from fast tokenizer and fast tokenizer is faster,\n",
    "and they share the same token-id mapping.\n",
    "\"\"\"\n",
    "hf_tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
    "hf_fast_tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-small-discriminator\")\n",
    "electra_config = ElectraConfig.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yoI6QffPY6Jv"
   },
   "source": [
    "# 1. Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_P8P4hffUr8"
   },
   "source": [
    "## 1.1 Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0zfrnOzY9u6"
   },
   "source": [
    "**Download, Preprocess, and Cache**\n",
    "\n",
    "In Colab, it takes you 20+ minutes for the first time, and seconds for subsequent calls. \n",
    "\n",
    "It will cost serveral minutes if you reset runtime even load the cache, but it's not true when you just restart the runtime. So I guess they may keep someth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "ae0863b074ff4a1a959514881fe2802f",
      "e5d5d86746ec4e2298028c57ef775d84",
      "a8b32129b9ab41d0895cb1ec445db9ca",
      "2d26c89a3d97493e911c52fa20cf79d9",
      "65eee3543a9d46adbc8037995c3a69a0",
      "9fc29a98d6be46e0974d1f0f7681d5c3",
      "5b8f1e5172204ed58bd60972289b24a8",
      "bd8da3c0b2784f2094d3b0cfa744cf87",
      "f74af04bf4764b7888f0a45e37ce85c1",
      "f35afd9d13c240aba4e0619cca0199b7",
      "a07124360f4c4249a745891d83e719f6",
      "f26a4f6d9334465d9022ab2a3401035b",
      "007556af2af8413ca47aa5f7f725e68a",
      "8d4771f53c7046f1afe7e04a3a647a31",
      "71beed6de71c4aa2862ef33e21cc58bc",
      "e1d8bdd9c1a74bc18a103bc900589c2c"
     ]
    },
    "colab_type": "code",
    "id": "U-PLcDIRY6y-",
    "outputId": "12e31f13-b70f-48c8-dd45-c6f1718b03c1"
   },
   "outputs": [],
   "source": [
    "# create a 'glue' folder under it, and all cache files will be under 'glue'\n",
    "cache_dir=Path('~/datasets')\n",
    "cache_dir.mkdir(parents=True, exist_ok=True) # create recursively if not exist\n",
    "\n",
    "def textcols(dataset):\n",
    "  \"Infer text cols of different GLUE datasets in huggingface/nlp\"\n",
    "  column_names = dataset.column_names\n",
    "  if 'question' in column_names: return ['question', 'sentence']\n",
    "  elif 'sentence1' in column_names: return ['sentence1', 'sentence2']\n",
    "  elif 'question1' in column_names: return ['question1','question2']\n",
    "  elif 'premise' in column_names: return ['premise','hypothesis']\n",
    "  elif 'sentence' in column_names: return ['sentence']\n",
    "\n",
    "\"\"\"\n",
    "quote from transformers tutorial of fastai (http://dev.fast.ai/tutorial.transformers)\n",
    "we don't use the tokenizer.encode method since it does some additional preprocessing for the model after tokenizing and numericalizing (the aprt throwing a warning before). Here we don't need any post-processing so it's fine to skip it.\n",
    "\"\"\"\n",
    "def tokenize_sents(example, cols):\n",
    "  if len(cols)==1:\n",
    "    example['input_ids'] = hf_fast_tokenizer.convert_tokens_to_ids(hf_fast_tokenizer.tokenize(f\"[CLS] {example[cols[0]]} [SEP]\"))\n",
    "  elif len(cols)==2:\n",
    "    example['input_ids'] = concat(hf_fast_tokenizer.convert_tokens_to_ids(hf_fast_tokenizer.tokenize(f'[CLS] {example[cols[0]]} [SEP]')),\n",
    "                                  hf_fast_tokenizer.convert_tokens_to_ids(hf_fast_tokenizer.tokenize(f'{example[cols[1]]} [SEP]')))\n",
    "  else: raise ValueError()\n",
    "  return example\n",
    "\n",
    "glue_dsets = {}\n",
    "for glue_task in ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', 'qnli', 'rte', 'wnli', 'ax']:\n",
    "  task = nlp.load_dataset('glue', glue_task, cache_dir=cache_dir)\n",
    "  glue_dsets[glue_task] = {}\n",
    "  print(f'loading processed datasets of {glue_task} ...')\n",
    "  for split in task.keys():\n",
    "    raw_dataset = task[split]\n",
    "    cache_file = Path(raw_dataset.cache_files[0]['filename']).parent / f'tokenized_{split}.arrow'\n",
    "    if cache_file.exists():dataset = nlp.Dataset.from_file(str(cache_file))\n",
    "    else: dataset = raw_dataset.map(partial(tokenize_sents, cols=textcols(raw_dataset)),\n",
    "                                    cache_file_name=str(cache_file))\n",
    "    glue_dsets[glue_task][split] = dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yKxetBkqfdgm"
   },
   "source": [
    "## 1.2 hf/hlp datasets -> fastai dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Oc3ZjYwZGLn"
   },
   "outputs": [],
   "source": [
    "# I don't know how to let `Learner` validate two validation sets every epochs, so I just merged `mnli validation mismatched` and `mnli validation matched`.\n",
    "glue_dsets['mnli']['validation'] = HF_MergedDataset(glue_dsets['mnli']['validation_matched'], glue_dsets['mnli']['validation_mismatched'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4mAI-scZSQh"
   },
   "outputs": [],
   "source": [
    "def text_decode_fc(x, pretty=True):\n",
    "  if pretty:\n",
    "    return hf_fast_tokenizer.decode([idx for idx in x if idx != hf_fast_tokenizer.pad_token_id])\n",
    "  else:\n",
    "    tokens = hf_fast_tokenizer.convert_ids_to_tokens(x)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "@delegates(FilteredBase.dataloaders)\n",
    "def get_glue_dls(task_name, **kwargs):\n",
    "  assert task_name in ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', 'qnli', 'rte', 'wnli', 'ax']\n",
    "  splits = ['train','validation']+(['test'] if task_name != 'mnli' else ['test_matched', 'test_mismatched'])\n",
    "  if task_name == 'ax': splits = ['test']\n",
    "  arrow_dsets = [glue_dsets[task_name][s] for s in splits ]\n",
    "  show_pretty = kwargs.pop('show_pretty', True) \n",
    "  dsets = HF_Datasets(datasets=arrow_dsets, \n",
    "                      cols=['input_ids', 'label'],\n",
    "                      encode_types=[TensorText, noop],\n",
    "                      decode_funcs=[partial(text_decode_fc,pretty=show_pretty),noop],\n",
    "                      decode_types=[TitledStr, lambda x: Category(x.item())])\n",
    "  dl_cache_files = [Path(dset.cache_files[0]['filename']).parent/f'dl_{s}.pth' for dset, s in zip(dsets,splits)]\n",
    "  if all([ p.exists() for p in dl_cache_files]):\n",
    "    device = kwargs.pop('device', 'cpu')\n",
    "    dl_s = [TextDataloader.from_cache(f, dsets[i], bs=32, **kwargs) for i, f in enumerate(dl_cache_files)]\n",
    "    assert dl_s[0].max_seq_len == electra_config.max_position_embeddings\n",
    "    dls = DataLoaders(*dl_s, device=device)\n",
    "  else:\n",
    "    dls = dsets.dataloaders(before_batch=partial(pad_input_chunk,pad_first=False,pad_idx=hf_fast_tokenizer.pad_token_id,),\n",
    "                             dl_type=partial(TextDataloader, \n",
    "                                             sort_by_len=False,\n",
    "                                            max_seq_len=electra_config.max_position_embeddings,\n",
    "                                            ignore_gt_maxlen=True),\n",
    "                            bs=32,\n",
    "                            device='cpu'\n",
    "                             **kwargs)\n",
    "    for dl, cache_f in zip(dls,dl_cache_files): dl.cache(cache_f)\n",
    "  return dls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MqtDnxK9ZYQP"
   },
   "source": [
    "## 1.3 Get dataloaders fror each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VYQubDaZbRv"
   },
   "source": [
    "**[CoLA](https://nyu-mll.github.io/CoLA/)** (*The Corpus of Linguistic Acceptability*):\n",
    "\n",
    "\n",
    "Check whether a sentence is linguistically acceptable. \n",
    "\n",
    "(0: unacceptable, 1: acceptable) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "HhZ3HD7VZalK",
    "outputId": "7ea5edbf-6a70-4f5a-9b85-c033e7aef666"
   },
   "outputs": [],
   "source": [
    "cola_dls = get_glue_dls('cola')\n",
    "print(f\"Dataset size (train/valid/test): {len(cola_dls[0].dataset)}/{len(cola_dls[1].dataset)}/{len(cola_dls[2].dataset)}\")\n",
    "cola_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZJ2zGxJZiyq"
   },
   "source": [
    "**Note**: for the readibility, we won't show pad and result of sentencepiece ('##...') here, which are the actual results in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "j1zl6tm6Zk02",
    "outputId": "e6212d7a-b03b-41c4-ac46-e8a7633ad4be"
   },
   "outputs": [],
   "source": [
    "get_glue_dls('cola', show_pretty=False).show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "STiSJYdbZns1"
   },
   "source": [
    "**[SST-2](https://nlp.stanford.edu/sentiment/index.html)** (*The Stanford Sentiment Treebank*): \n",
    "\n",
    "Identify the sentiment of a work/phrase/sentence. \n",
    "\n",
    "(1: positvie, 0: negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "ouo5jT9sZnKG",
    "outputId": "ef107490-d6b8-4551-9028-33cc299fc674"
   },
   "outputs": [],
   "source": [
    "sst2_dls = get_glue_dls('sst2')\n",
    "print(f\"Dataset size (train/valid/test): {len(sst2_dls[0].dataset)}/{len(sst2_dls[1].dataset)}/{len(sst2_dls[2].dataset)}\")\n",
    "sst2_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wecP4wm0ZqfX"
   },
   "source": [
    "**[MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398)** (*Microsoft Research Paraphrase Corpus*): \n",
    "\n",
    "Whether each pair captures a paraphrase/semantic equivalence relationship. \n",
    "\n",
    "(1: yes, 0: no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "6T54EE4WZxgW",
    "outputId": "98e94bdb-324d-4d80-b367-7f4f6da2efb1"
   },
   "outputs": [],
   "source": [
    "mrpc_dls = get_glue_dls('mrpc')\n",
    "print(f\"Dataset size (train/valid/test): {len(sst2_dls.train_ds)}/{len(sst2_dls.valid_ds)}/{len(sst2_dls[2])}\")\n",
    "mrpc_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "znmNswfyZuCb"
   },
   "source": [
    "**[STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark)** (*Semantic Textual Similarity Benchmark*):\n",
    "\n",
    "Score the similarity of meanings of two sentences. The only regression task in GLUE \n",
    "\n",
    "(0.0 ~ 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "631vsrFwaDHB",
    "outputId": "380a5db6-b00b-49aa-a5d7-7d4cedba576e"
   },
   "outputs": [],
   "source": [
    "stsb_dls = get_glue_dls('stsb')\n",
    "print(f\"Dataset size (train/valid/test): {len(stsb_dls.train_ds)}/{len(stsb_dls.valid_ds)}/{len(stsb_dls[2])}\")\n",
    "stsb_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YR_ok10uaCYM"
   },
   "source": [
    "**[QQP](https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)** (*Quora Question Pairs*)\n",
    "\n",
    "Check whether two questions are duplicated. \n",
    "\n",
    "(0: no, 1: duplicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "hWQc8402aFJb",
    "outputId": "8a4e6617-47e4-4a92-bc56-481730a16088"
   },
   "outputs": [],
   "source": [
    "qqp_dls = get_glue_dls('qqp')\n",
    "print(f\"Dataset size (train/valid/test): {len(qqp_dls.train_ds)}/{len(qqp_dls.valid_ds)}/{len(qqp_dls[2])}\")\n",
    "qqp_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnZm1zqEaHHB"
   },
   "source": [
    "**[MNLI](https://cims.nyu.edu/~sbowman/multinli/)** (*The Multi-Genre NLI Corpus*)\n",
    "\n",
    "Whether the premise (sentence 1) entails the hypothesis (sentence 2) (entailment), contradicts the hypothesis (contradiction), or neither (neutral) \n",
    "\n",
    "(0: entailment, 1: neutral, 2: contradiction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "3PRuXBJ8aJTH",
    "outputId": "89fd1479-796c-43ef-ee71-292ac6ed4bad"
   },
   "outputs": [],
   "source": [
    "mnli_dls = get_glue_dls('mnli')\n",
    "print(f\"Dataset size (train/valid/test_matched/test_mismatched): {len(mnli_dls[0].dataset)}/{len(mnli_dls[1].dataset)}/{len(mnli_dls[2].dataset)}/{len(mnli_dls[3].dataset)}\")\n",
    "mnli_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PJKg-BSkaPN5"
   },
   "source": [
    "**QNLI** (*The Stanford Question Answering Dataset*):\n",
    "\n",
    "The task is to determine whether the context sentence (sentence 2) contains the answer to the question (sentence 1).\n",
    "\n",
    "(0: entailment, 1: not_entailment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "DIK9BAnBaLbj",
    "outputId": "62ee8573-b4f6-4274-9592-44920c010674"
   },
   "outputs": [],
   "source": [
    "qnli_dls = get_glue_dls('qnli')\n",
    "print(f\"Dataset size (train/valid/test): {len(qnli_dls[0].dataset)}/{len(qnli_dls[1].dataset)}/{len(qnli_dls[2].dataset)}\")\n",
    "qnli_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I2OPdiDpaS6v"
   },
   "source": [
    "**[RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment)** (*Recognizing_Textual_Entailment*):\n",
    "\n",
    "Whether hypothesis (sentence 2) is entailed (can be inferred) from the premise (sentence 1).\n",
    "\n",
    "(0: entailment, 1: not_entailment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "cODdx4WyaVRh",
    "outputId": "ee44b782-8da0-42a8-ef6f-b17a841679df"
   },
   "outputs": [],
   "source": [
    "rte_dls = get_glue_dls('rte')\n",
    "print(f\"Dataset size (train/valid/test): {len(rte_dls[0].dataset)}/{len(rte_dls[1].dataset)}/{len(rte_dls[2].dataset)}\")\n",
    "rte_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-iIX2EaaUt-"
   },
   "source": [
    "**[WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html)** (*The Winograd Schema Challenge*)\n",
    "\n",
    "Check whether sentence 2 (which is rephrased sentence of sentence 1) correctly solve the pronoun in sentence 1.\n",
    "\n",
    "(0: wrong, 1: correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "L7wGvrihaZAj",
    "outputId": "df1fad6b-d779-4fe9-f9c8-53bacf5eec07"
   },
   "outputs": [],
   "source": [
    "wnli_dls = get_glue_dls('wnli')\n",
    "print(f\"Dataset size (train/valid/test): {len(wnli_dls[0].dataset)}/{len(wnli_dls[1].dataset)}/{len(wnli_dls[2].dataset)}\")\n",
    "wnli_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0RQ-BEaIaa8F"
   },
   "source": [
    "**[AX](https://gluebenchmark.com/diagnostics)** (*GLUE Diagnostic Dataset*):\n",
    "\n",
    "Whether the premise (sentence 1) entails the hypothesis (sentence 2) (entailment), contradicts the hypothesis (contradiction), or neither (neutral) \n",
    "\n",
    "Test set only.\n",
    "\n",
    "The label is all -1, because this is a test dataset and the answers is kept by GLUE benchmark and not provided. The categories are the same as MNLI (entailment, neutral, contradiction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "5IKH0iJKadAW",
    "outputId": "0c5935c4-608f-426e-f3d0-9d380df14fa8"
   },
   "outputs": [],
   "source": [
    "ax_dls = get_glue_dls('ax')\n",
    "print(f\"Dataset size (test): {len(ax_dls[0].dataset)}\")\n",
    "ax_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wRQS_H1aekI"
   },
   "outputs": [],
   "source": [
    "glue_dls = {}\n",
    "for task_name in ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', 'qnli', 'rte', 'wnli', 'ax']:\n",
    "  dls = eval(f\"{task_name}_dls\")\n",
    "  if task_name != 'ax':\n",
    "    dls[0].desc_sort(); dls[1].desc_sort(); # sort by len to reduce pad, but not for test_dl which keep the order of sample to let us add the dataset idx conveniently later \n",
    "  glue_dls[task_name] = dls\n",
    "glue_dls['ax'][0].shuffle = False # fastai see 0th dl as train_dl which is default shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFcM5MGaafFJ"
   },
   "source": [
    "# 2. Finetuning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tUGxkcDfQGxS"
   },
   "source": [
    "* ELECTRA use CLS encodings as pooled result to predict the sentence. (see [here](https://github.com/google-research/electra/blob/79111328070e491b287c307906701ebc61091eb2/model/modeling.py#L254) of its official repository)\n",
    "\n",
    "* Note that we should use different prediction head for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MF6vajgKQKJ7"
   },
   "outputs": [],
   "source": [
    "class SentencePredictHead(nn.Module):\n",
    "  \"The way that Electra and Bert do for sentence prediction task\"\n",
    "  def __init__(self, hidden_size, targ_voc_size):\n",
    "    super().__init__()\n",
    "    self.linear = nn.Linear(hidden_size, targ_voc_size)\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "  def forward(self, x):\n",
    "    \"x: (batch size, sequence length, hidden_size)\"\n",
    "    return self.linear(self.dropout(x[:,0])) # project the first token (a special token)'s hidden encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRoFPXjpe4Nb"
   },
   "source": [
    "`SentencePredictHead`\n",
    "* change `targ_voc_size` for diffrent task\n",
    "* change `256` for your model's hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "K3_2P-uZV8sb",
    "outputId": "f114b91b-c088-4192-a193-128ede97721b"
   },
   "outputs": [],
   "source": [
    "single_task_model = nn.Sequential(HF_ModelWrapper.from_pretrained(ElectraModel, 'google/electra-small-discriminator', pad_id=hf_tokenizer.pad_token_id, sep_id=hf_tokenizer.sep_token_id), # specify spe_id and wrapper will create and pass token_type_ids of sentece A/B for you\n",
    "                                  SentencePredictHead(electra_config.hidden_size, targ_voc_size=2)) \n",
    "inp_tensor = torch.tensor(hf_tokenizer.encode('I am the sentence A','I am the sentence B'))[None,:] \n",
    "print(f'input shape: {inp_tensor.shape} (batch size, sequence length)')\n",
    "with torch.no_grad(): out_tensor = single_task_model(inp_tensor)\n",
    "print(f'output shape: {out_tensor.shape} (batch_size, target vocab size)')\n",
    "print(f'The output is the raw score-like thing for each label')\n",
    "print(out_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zi5_OKHqX8qw"
   },
   "source": [
    "# 3. Single Task Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "poNJYRwETtBo"
   },
   "source": [
    "## 3.1 Discriminative learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OyT-hy-7FXz"
   },
   "outputs": [],
   "source": [
    "# Names come from, for nm in model.named_modules(): print(nm[0])\n",
    "def hf_electra_param_splitter(model, num_hidden_layers, outlayer_name):\n",
    "  names = ['model.embeddings', *[f'model.encoder.layer.{i}' for i in range(num_hidden_layers)], outlayer_name]\n",
    "  def endswith_any(n,ns): return any([ n.endswith(suffix) for suffix in ns])\n",
    "  groups = [ list(mod.parameters()) for name, mod in model.named_modules() if endswith_any(name, names) ]\n",
    "  assert len(groups) == len(names)\n",
    "  return groups\n",
    "\n",
    "def get_layer_lrs(lr, decay_rate_of_depth, num_hidden_layers):\n",
    "  # I think input layer as bottom and output layer as top, which make 'depth' mean different from the one of official repo \n",
    "  return [ lr * (decay_rate_of_depth ** depth) for depth in reversed(range(num_hidden_layers+2))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N2ug-bscA9gC"
   },
   "source": [
    "## 3.2 Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nzqFhWQkBGhu"
   },
   "outputs": [],
   "source": [
    "def linear_warmup_and_decay(pct_now, lr_max, end_lr, decay_power, warmup_pct, total_steps):\n",
    "  \"\"\"\n",
    "  end_lr: the end learning rate for linear decay\n",
    "  warmup_pct: percentage of training steps to for linear increase\n",
    "  pct_now: percentage of traning steps we have gone through, notice pct_now=0.0 when calculating lr for first batch.\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  pct updated after_batch, but global_step (in tf) seems to update before optimizer step,\n",
    "  so pct is actually (global_step -1)/total_steps \n",
    "  \"\"\"\n",
    "  fixed_pct_now = pct_now + 1/total_steps\n",
    "  \"\"\"\n",
    "  According to source code of the official repository, it seems they merged two lr schedule (warmup and linear decay)\n",
    "  sequentially, instead of split training into two phases for each, this might because they think when in the early\n",
    "  phase of training, pct is low, and thus the decaying formula makes little difference to lr.\n",
    "  \"\"\"\n",
    "  decayed_lr = (lr_max-end_lr) * (1-fixed_pct_now)**decay_power + end_lr # https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/polynomial_decay\n",
    "  warmed_lr = decayed_lr * min(1.0, fixed_pct_now / warmup_pct) # https://github.com/google-research/electra/blob/81f7e5fc98b0ad8bfd20b641aa8bc9e6ac00c8eb/model/optimization.py#L44\n",
    "  return warmed_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Mcjq0b8wbkO"
   },
   "source": [
    "## 3.3 finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = {\n",
    "  **{ task:['MatthewsCorrCoef'] for task in ['cola']},\n",
    "  **{ task:['Accuracy'] for task in ['sst2', 'mnli', 'qnli', 'rte', 'wnli', 'snli']},\n",
    "  # Note: MRPC and QQP are both binary classification problem, so we can just use fastai's default\n",
    "  # average option 'binary' without spcification of average method.\n",
    "  **{ task:['F1Score', 'Accuracy'] for task in ['mrpc', 'qqp']}, \n",
    "  **{ task:['PearsonCorrCoef', 'SpearmanCorrCoef'] for task in ['stsb']}\n",
    "}\n",
    "TARG_VOC_SIZE = {\n",
    "    **{ task:1 for task in ['stsb']},\n",
    "    **{ task:2 for task in ['cola', 'sst2', 'mrpc', 'qqp', 'qnli', 'rte', 'wnli']},\n",
    "    **{ task:3 for task in ['mnli','ax']}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Table 7 of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glue_learner(electra_config, task, use_one_cycle=False, fp16=False,device='cuda:0'):\n",
    "    if electra_config.hidden_size == 256: learing_rate = 3e-4; lr_depth_decay=0.8\n",
    "    elif electra_config.hidden_size == 768: learing_rate = 1e-4; lr_depth_decay=0.8\n",
    "    elif electra_config.hidden_size == 1024: learing_rate = 5e-5; lr_depth_decay=0.9\n",
    "    if task == 'rte': num_epochs = 10\n",
    "    else: num_epochs = 3\n",
    "    model = nn.Sequential(HF_ModelWrapper.from_pretrained(ElectraModel, 'google/electra-small-discriminator', pad_id=hf_tokenizer.pad_token_id, sep_id=hf_tokenizer.sep_token_id), # specify spe_id and wrapper will create and pass token_type_ids of sentece A/B for you\n",
    "                                    SentencePredictHead(electra_config.hidden_size, targ_voc_size=TARG_VOC_SIZE[task]))\n",
    "    dls = glue_dls[task].to(device)\n",
    "    layer_lrs = get_layer_lrs(lr=learing_rate,\n",
    "                            decay_rate_of_depth=lr_depth_decay,\n",
    "                            num_hidden_layers=electra_config.num_hidden_layers)\n",
    "    lr_shedule = ParamScheduler({'lr': partial(linear_warmup_and_decay,\n",
    "                                            lr_max=np.array(layer_lrs),\n",
    "                                            end_lr=0.0,\n",
    "                                            decay_power=1,\n",
    "                                            warmup_pct=0.1,\n",
    "                                            total_steps=num_epochs*(len(dls.train)))})\n",
    "    learn = Learner(dls, model,\n",
    "                    loss_func=CrossEntropyLossFlat() if task != 'stsb' else MSELossFlat(), \n",
    "                    opt_func=partial(Adam, eps=1e-6,),\n",
    "                    metrics=[eval(f'{metric}()') for metric in METRICS[task]],\n",
    "                    splitter=partial(hf_electra_param_splitter, \n",
    "                                    num_hidden_layers=electra_config.num_hidden_layers, \n",
    "                                    outlayer_name='1'),\n",
    "                    lr=layer_lrs if use_one_cycle else defaults.lr,\n",
    "                    path='/home/yisiang/checkpoints',\n",
    "                    model_dir='electra_glue',)\n",
    "    if fp16: learn = learn.fp16()\n",
    "    return learn, num_epochs, None if use_one_cycle else lr_shedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_glue(task, use_one_cycle=False, fp16=False, i=None, device='cuda:0'):\n",
    "    learn, num_epochs, shed = get_glue_learner(electra_config, task,use_one_cycle=use_one_cycle, fp16=fp16,device=device)\n",
    "    if shed is None:\n",
    "        name = f\"cycle_{task}_{i}\"\n",
    "        run = wandb.init(name=name, project='hf_glue', reinit=True, config={'task': task, 'th':i, 'shed': 'one_cycle', 'fp16': fp16, 'my_model':False})\n",
    "        with run:\n",
    "            learn.fit_one_cycle(num_epochs, cbs=[WandbCallback(log_preds=False)])\n",
    "    else:\n",
    "        name = f\"linear_{task}_{i}\"\n",
    "        run = wandb.init(name=name, project='hf_glue', reinit=True,config={'task': task, 'th':i, 'shed': 'linear_warm&decay', 'fp16': fp16, 'my_model':False})\n",
    "        with run:\n",
    "            learn.fit(num_epochs, cbs=[WandbCallback(log_preds=False), shed])\n",
    "    learn.save(name, with_opt=False)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', 'qnli', 'rte', 'wnli']:\n",
    "    train_glue(task,i=0, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "iy6g3alOACMv",
    "outputId": "11aa5215-4f54-485e-85d6-35eb8233458c"
   },
   "outputs": [],
   "source": [
    "single_task_learn, num_epochs, shed = get_glue_learner(electra_config, 'mrpc',)\n",
    "if shed is None: single_task_learn.fit_one_cycle(n_epoch=num_epochs)\n",
    "else: single_task_learn.fit(n_epoch=num_epochs, cbs=[shed,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c73X4fdrmnlP"
   },
   "source": [
    "## 3.3 Predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EYMsFM0Cmp1k"
   },
   "outputs": [],
   "source": [
    "test_pred_dir = cache_dir/'glue'/'test'\n",
    "test_pred_dir.mkdir(exist_ok=True)\n",
    "preds = single_task_learn.get_preds(dl=glue_dls['mrpc'][2], with_decoded=True)\n",
    "preds = preds[-1] # preds -> (predictions logits, targets, decoded prediction)\n",
    "cols = ['idx', *textcols(glue_dsets['mrpc']['test'])]\n",
    "test_df = pd.DataFrame( data={**{col:glue_dsets['mrpc']['test'][col] for col in cols}, \n",
    "                             'prediction': preds.tolist()} )\n",
    "test_df.to_csv( test_pred_dir/'MRPC.tsv', sep='\\t' )\n",
    "test_df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multi-task learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tasks = ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', 'qnli', 'rte', 'wnli']\n",
    "tasks = ['mrpc','qqp']\n",
    "\n",
    "dls_s, pred_heads, loss_funcs, metrics_s = [], [], [], []\n",
    "for task in tasks:\n",
    "  # multi_dls\n",
    "  dls_s.append(glue_dls[task])\n",
    "  # multi_model\n",
    "  pred_heads.append(SentencePredictHead(electra_config.hidden_size, TARG_VOC_SIZE[task]))    \n",
    "  # multi_loss_func\n",
    "  loss_funcs.append(CrossEntropyLossFlat() if task != 'stsb' else MSELossFlat())\n",
    "  # multi_metrics\n",
    "  metrics_s.append([eval(f'{metric}()') for metric in METRICS[task]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head_model = MultiHeadModel(HF_ModelWrapper.from_pretrained(ElectraModel, 'google/electra-small-discriminator', pad_id=hf_tokenizer.pad_token_id, sep_id=hf_tokenizer.sep_token_id), \n",
    "                                  pred_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_learn = MultiTaskLearner(multi_dls=dls_s,\n",
    "                               opt_func=partial(Adam, eps=1e-6,),\n",
    "                               multi_model=multi_head_model,\n",
    "                               multi_loss_func=loss_funcs,\n",
    "                               task_weights=[0.5,0.5], # default as [1.] * number of tasks\n",
    "                               task_names=tasks,\n",
    "                               multi_metrics=metrics_s,\n",
    "                               splitter=partial(hf_electra_param_splitter,\n",
    "                                                num_hidden_layers=electra_config.num_hidden_layers,\n",
    "                                                outlayer_name='pred_heads'),\n",
    "                               lr=get_layer_lrs(3e-4,0.8,electra_config.num_hidden_layers),\n",
    "                               )#.to_fp16()\n",
    "\n",
    "multi_learn.fit(2, 3e-4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "yKxetBkqfdgm",
    "MqtDnxK9ZYQP",
    "LFcM5MGaafFJ",
    "7cR6NmFsxH10",
    "n1vFihXdx7iQ",
    "pZS8owurxXUV",
    "admPAZ2M2uqO",
    "1NSoV-Np6utL"
   ],
   "name": "Finetune GLUE with fastai.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "007556af2af8413ca47aa5f7f725e68a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2d26c89a3d97493e911c52fa20cf79d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd8da3c0b2784f2094d3b0cfa744cf87",
      "placeholder": "​",
      "style": "IPY_MODEL_5b8f1e5172204ed58bd60972289b24a8",
      "value": " 29.0k/29.0k [00:00&lt;00:00, 39.1kB/s]"
     }
    },
    "5b8f1e5172204ed58bd60972289b24a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65eee3543a9d46adbc8037995c3a69a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "71beed6de71c4aa2862ef33e21cc58bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d4771f53c7046f1afe7e04a3a647a31": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fc29a98d6be46e0974d1f0f7681d5c3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a07124360f4c4249a745891d83e719f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d4771f53c7046f1afe7e04a3a647a31",
      "max": 30329,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_007556af2af8413ca47aa5f7f725e68a",
      "value": 30329
     }
    },
    "a8b32129b9ab41d0895cb1ec445db9ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fc29a98d6be46e0974d1f0f7681d5c3",
      "max": 28998,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65eee3543a9d46adbc8037995c3a69a0",
      "value": 28998
     }
    },
    "ae0863b074ff4a1a959514881fe2802f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a8b32129b9ab41d0895cb1ec445db9ca",
       "IPY_MODEL_2d26c89a3d97493e911c52fa20cf79d9"
      ],
      "layout": "IPY_MODEL_e5d5d86746ec4e2298028c57ef775d84"
     }
    },
    "bd8da3c0b2784f2094d3b0cfa744cf87": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1d8bdd9c1a74bc18a103bc900589c2c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5d5d86746ec4e2298028c57ef775d84": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f26a4f6d9334465d9022ab2a3401035b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1d8bdd9c1a74bc18a103bc900589c2c",
      "placeholder": "​",
      "style": "IPY_MODEL_71beed6de71c4aa2862ef33e21cc58bc",
      "value": " 30.3k/30.3k [00:00&lt;00:00, 392kB/s]"
     }
    },
    "f35afd9d13c240aba4e0619cca0199b7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f74af04bf4764b7888f0a45e37ce85c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a07124360f4c4249a745891d83e719f6",
       "IPY_MODEL_f26a4f6d9334465d9022ab2a3401035b"
      ],
      "layout": "IPY_MODEL_f35afd9d13c240aba4e0619cca0199b7"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}