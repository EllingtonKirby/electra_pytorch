{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace as bk\n",
    "import os\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import torch\n",
    "import nlp\n",
    "from transformers import ElectraTokenizerFast\n",
    "hf_tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-small-generator\")\n",
    "from fastai2.text.all import *\n",
    "from _utils.huggingface import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize all splits of dataset at once\n",
    "`cols`(`Dict[str]`): tokenize the every column named key into column named its value  \n",
    "`cols`(`List[str]`): specify the name of columns to be tokenized, replace the original columns' data with tokenized one\n",
    "\n",
    "Here, we tokenized \"sentence\" into a new column named \"text_idxs\", the \"sentence\" column still exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n 'label': 1,\n 'idx': 0,\n 'text_idxs': [2256,\n  2814,\n  2180,\n  1005,\n  1056,\n  4965,\n  2023,\n  4106,\n  1010,\n  2292,\n  2894,\n  1996,\n  2279,\n  2028,\n  2057,\n  16599,\n  1012]}"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "cola = nlp.load_dataset('glue', 'cola') \n",
    "# cola is {'train':nlp.Dataset, 'validation':nlp.Dataset, 'test':nlp.Dataset}\n",
    "tokenized_cola = HF_TokenizeTfm(cola, {'sentence':'text_idxs'}, hf_tokenizer).map()\n",
    "tokenized_cola['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom function apply to all splits of dataset at once\n",
    "The `func` of `HF_Transform` is `function` in `nlp.Dataset.map`, but it will be applied to all splits individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'sentence1': 'Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44, according to the Christopher Reeve Foundation.',\n 'sentence2': 'Christopher Reeve had an accident.',\n 'label': 1,\n 'idx': 0,\n 'tok_ids': [101,\n  11271,\n  20726,\n  1010,\n  1996,\n  7794,\n  1997,\n  1996,\n  3364,\n  5696,\n  20726,\n  1010,\n  2038,\n  2351,\n  1997,\n  11192,\n  4456,\n  2012,\n  2287,\n  4008,\n  1010,\n  2429,\n  2000,\n  1996,\n  5696,\n  20726,\n  3192,\n  1012,\n  102,\n  5696,\n  20726,\n  2018,\n  2019,\n  4926,\n  1012,\n  102]}"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "rte = nlp.load_dataset('glue', 'rte')\n",
    "# ax is {'test': nlp.Dataset}\n",
    "def custom_tokenize(example):\n",
    "  example['tok_ids'] = hf_tokenizer.encode(example['sentence1'], example['sentence2'])\n",
    "  return example\n",
    "tokenized_rte = HF_Transform(rte, custom_tokenize).map()\n",
    "tokenized_rte['validation'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create fastai `Dataloaders` and `show_batch`\n",
    "\n",
    "`cols`: **specify columns whose values form a output sample in order**, and the semantic type of each column to encode/decode, with one of the following signature (see doc).\n",
    "\n",
    "Here, `['text_idxs, 'label']` is equal to `{'text_idxs': TensorText, 'label': TensorCategory}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "51%|█████     | 544/1063 [00:00<00:00, 5432.45it/s]"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_idxs</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everybody who has ever, worked in any office which contained any typewriter which had ever been used to type any letters which had to be signed by any administrator who ever worked in any department like mine will know what i mean.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hank plays the guitar and finds arrangements for all the old folk songs which are still sung in these hills, and ernie writes down all the old folk songs which are still sung in these hills.</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "cola_dsets = HF_Datasets(tokenized_cola, cols=['text_idxs', 'label'], hf_toker=hf_tokenizer, neat_show=True)\n",
    "cola_dls = cola_dsets.dataloaders(bs=32, pad_idx=hf_tokenizer.pad_token_id)\n",
    "cola_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either specify `neat_show=False` (which is default), to show real data which is tokenized and  with pad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "54%|█████▍    | 572/1063 [00:00<00:00, 5714.26it/s]"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_idxs</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everybody who has ever , worked in any office which contained any type ##writer which had ever been used to type any letters which had to be signed by any administrator who ever worked in any department like mine will know what i mean .</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>in january 2002 , a dull star in an obscure constellation suddenly became 600 , 000 times more luminous than our sun , temporarily making it the brightest star in our galaxy . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "cola_dsets = HF_Datasets(tokenized_cola, cols={'text_idxs': TensorText, 'label': TensorCategory}, hf_toker=hf_tokenizer)\n",
    "cola_dls = cola_dsets.dataloaders(bs=32, pad_idx=hf_tokenizer.pad_token_id)\n",
    "cola_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_with_label` is `False` by default, so in test set the sample formed by only first `n_inp` columns specified, which is x.\n",
    "\n",
    "This make you able to apply the same to all splits when test set come with no y or fake y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_idxs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cultural commissioner megan smith said that the five ` ` sounds ##cape ' ' pieces would ` ` give a fest ##ive air to park square , they ' re fun and interesting ' ' .</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wendy is eager to sail around the world and bruce is eager to climb ki ##lim ##an ##jar ##o , but neither of them can because money is too tight . [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "cola_dls[2].show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Aggregate Dataset\n",
    "a sample in transformed dataset is aggregated/accumulated from multiple original samples.\n",
    "\n",
    "- Except for `LMTransform`, you can implement your own logic create a class inherits `AggregateTransform` and implements `accumulate` and `create_example` method\n",
    "\n",
    "- Note that you should pass **tokenized** dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make  dataset for (traditional) language model\n",
    "You can always pass dict of `nlp.Dataset` or a `nlp.Dataset` at your will for any transform class, we've test passing a dict, now we test a `nlp.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Original dataset:\nnum of samples: 1043\nsecond to last sentence: John arranged for himself to get the prize.\n          last sentence: John talked to Bill about himself.\nLM dataset:\nnum of sampels: 481\nlast text (x): . john talked to bill about himself\nlast text (y): john talked to bill about himself.\n"
    }
   ],
   "source": [
    "cola_val = tokenized_cola['validation']\n",
    "lm_dataset = LMTransform(cola_val, max_len=20, text_col='text_idxs').map()\n",
    "\n",
    "print('Original dataset:')\n",
    "print('num of samples:', len(cola['validation']))\n",
    "print('second to last sentence:', cola['validation'][-2]['sentence'])\n",
    "print('          last sentence:', cola['validation'][-1]['sentence'])\n",
    "print('LM dataset:')\n",
    "print('num of sampels:', len(lm_dataset))\n",
    "print('last text (x):', hf_tokenizer.decode(lm_dataset[-1]['x_text']))\n",
    "print('last text (y):', hf_tokenizer.decode(lm_dataset[-1]['y_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>y_text</th>\n      <th>y_text_</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey</td>\n      <td>sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey .</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less .</td>\n      <td>mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less . as</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "lm_ds = HF_Dataset(lm_dataset, cols={'x_text':LMTensorText, 'y_text':TensorText},hf_toker=hf_tokenizer)\n",
    "lm_dl = MySortedDL(lm_ds, srtkey_fc=False, pad_idx=hf_tokenizer.pad_token_id)\n",
    "lm_dl.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ELECTRA data creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>inpids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] owners of a pig love to eat tr ##uf ##fles . that whether the world is round is unknown bothered athena . no one expected ag ##ame ##m ##non to to win eu ##cl ##id was interested in plato ' s description of geometry . every reading shakespeare satisfied me can will he do it ? med ##ea poisoned who ? he looked up it [SEP] who guy did you see . we kicked myself who would pose ##idon run away , if the execution ##er murdered ? anson kissed him which city the claim that philip would invade . i haven ' t left yet i am eating a mango and gillian has too . letter is on the table who ate the cake ? [SEP]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[CLS] john had an error in the proof he presented . john had an error in the proof sarah presented . fred had a snake behind the car joe was sitting in . fred had a snake behind the car he was sitting in . there was a yellow collar on the dog which the car injured . [SEP] there was a snake behind the car the time bomb was sitting in . the car had a yellow collar on the dog which it injured . that stone has a hole in the tar ##pa ##ulin which it is holding down . the time bomb had a snake behind the car which it was sitting in . there were several hundred people yelling for me to put [SEP]</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "proc_dsets = ELECTRADataTransform(tokenized_cola, text_col={'text_idxs':'inpids'}, max_length=128, cls_idx=hf_tokenizer.cls_token_id, sep_idx=hf_tokenizer.sep_token_id).map()\n",
    "e_dsets = HF_Datasets(proc_dsets, cols=['inpids'], hf_toker=hf_tokenizer)\n",
    "e_dls = e_dsets.dataloaders(srtkey_fc=False, pad_idx=hf_tokenizer.pad_token_id)\n",
    "e_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test filtering feature\n",
    "Note that filter won't be applied to split other than train, because validation/test set is for fair comparison, and you can't take out samples at your will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'train': 26, 'validation': 2, 'test': 6}\n"
    }
   ],
   "source": [
    "l = 23\n",
    "num = {}\n",
    "for split in tokenized_cola:\n",
    "  num[split] = reduce(lambda sum, sample: sum+(1 if len(sample['text_idxs'])==l else 0), \n",
    "                      tokenized_cola[split], 0)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "51%|█████▏    | 546/1063 [00:00<00:00, 5453.76it/s]Test passed\n"
    }
   ],
   "source": [
    "ccola_dsets = HF_Datasets(tokenized_cola, cols=['text_idxs', 'label'], hf_toker=hf_tokenizer)\n",
    "ccola_dls = ccola_dsets.dataloaders(pad_idx=hf_tokenizer.pad_token_id, \n",
    "                                    filter_fc=lambda text_idxs, label: len(text_idxs)!=l,)\n",
    "\n",
    "for i, split in enumerate(tokenized_cola):\n",
    "  if split == 'train':\n",
    "    assert ccola_dls[i].n == len(tokenized_cola[split])-num[split],f\"{split}: filtered: {ccola_dls[i].n}, unfiltered: {len(tokenized_cola[split])}, should be filtered: {num[split]}\"\n",
    "  else:\n",
    "    assert ccola_dls[i].n == len(tokenized_cola[split]), f\"{split}: accidentally filtered: {ccola_dls[i].n}, unfiltered: {len(tokenized_cola[split])}\"\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cache dataloader\n",
    "If sorting or filtering is applied, dataloader need to create some record inside it, to do it only once, we can cache the records. \n",
    "\n",
    "If `cache_dir` is not specified, it will be the cache_dir of `dsets` passed to `HF_Datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "56%|█████▋    | 598/1063 [00:00<00:00, 5973.88it/s]"
    }
   ],
   "source": [
    "for f in ['/tmp/cached_train.json','/tmp/cached_val.json', '/tmp/cached_test.json']:\n",
    "  if Path(f).exists(): os.remove(f)\n",
    "\n",
    "ccola_dls = ccola_dsets.dataloaders(bs=32, pad_idx=hf_tokenizer.pad_token_id, \n",
    "                                    cache_dir='/tmp', cache_name='cached_{split}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we load the caches, it should be fast and progress bars sholdn't appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccola_dls = ccola_dsets.dataloaders(bs=32, pad_idx=hf_tokenizer.pad_token_id, \n",
    "                                    cache_dir='/tmp', cache_name='cached_{split}.json')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}